---
title: CNN学习随笔
status: Completed
commentable: false
Edit: 2021-3-29
mathjax: true
mermaid: true
tags: Computer Vision , CNN
categories: Computer Vision
Typora-root-url: ../
description: 系统性重温CNN时的随手笔记

---

读前提示：

- 本文为早期重温CNN基础知识时的随笔，重新翻出来上传到博客上

- 笔者能力有限，仅供参考，如有错误，欢迎给我发邮件或者通过友链进行留言。（详情请见About）

# 引入

- CNN到底在干嘛

  提取图像特征，就这么简单

  但很万能，提取到了特征，你可以解决物体检测，风格转换等等众多图像处理问题

- 过滤器（或神经元或核）到底是怎么得来的

  反向传播（说白了就是训练，一会来讲）

  ​	学习来的，比如我有一个图片，打上tag说这是标签，计算机进行学习（训练过程中）就可以学习到在什么位置X对应的过滤器是什么样的，也就是特征（feature）

- 为啥卷积计算得到结果特征可以变成输出（feature map）可以对图像信息进行保留

  ​	过滤器有很多种，比如横向直线过滤器，纵向直线过滤器，右弯曲线过滤器，如果图片某个位置形状和过滤器对应形状非常相似，则输出结果值就会很大，也就是说越大过滤器对应特征越贴近图片对应位置的形状

- feature map是什么

  卷积层的输出

# 结构

典型结构：

输入→卷积→ReLU→卷积→ReLU→池化→ReLU→卷积→ReLU→池化→全连接

## 卷积层

- 数学角度
  - 输入：32x32x3（RGB）的像素值数组
  - Channel：每个卷积层中卷积核的数量（RGB就是3）
  - 过滤器（或神经元或核）：想象手电筒光正从图像的左上角照过，可以覆盖5x5的区域，然后逐一照过所有区域。手电筒==过滤器
    - 同样是一个数组（数字被称作权重或参数）
    - 深度需与输入内容的深度相同，所以大小为5x5x3
  - 感受野：被照过的区域；也就是卷积神经网络每一层输出的feature map上的像素点在输入图片上映射的区域大小
  - 计算点积：筛选值在图像上滑动，过滤器中的值会与图像中的原始像素值相乘，乘积加在一起得到一个数字
  - 单元移动：一直右移一个单元，然后再下一行
  - 激活映射（activation map）/特征（feature）映射：得到28x28x1的数组（28=32-5+1，输出公式见part 3：细节）
  - 小tips：过滤器越多，空间维度保留的越好
- 高层次角度
  - 特征（例如边缘，原色，曲线）标识符：每个过滤器

## 非线性激活层（RELU）

- 干嘛：对之前的结果进行一个非线性的激活效应
  - 卷积层：原图运算多个卷积产生的一组线性激活效应
- 用啥干（公式）：f(x)=max(0,x)
- 实际在干什：保留大于等于0的值，其余所有小于0的数值直接改写为0
- 为啥：越接近1表示特征越关联，越靠近-1表示越不关联，所以为了电脑处理方便（你要知道处理图片任务多浪费时间和GPU资源啊），就会舍弃掉小于0的数据（直接变为0）
- 结果：增加模型模型和整个神经网络的非线性特征，不会影响卷积层的感受野

## 池化层（Pooling）

- 干嘛：减少数据量

- 用啥干
  - 最大池化（Max Pooling）：取最大值，获得最佳匹配结果（越接近1越好嘛），当然CNN并不在乎到底是哪里匹配了，只关心某特征是否匹配上了
  - 平均池化（Average Pooling）：取平均值
  
- 咋干：
  - 池化尺寸：例如2x2，就是在2x2的矩阵中挑选出最大值（或直接取平均）填入新的featrue map
  
  - 和卷积层一样，有步长，例如2x2过滤器+步长为2
  
    ![](/assets/Pics/CNNpics/n3.png)
  
- 结果：

  - 权重参数减少到75%，降低计算成本
  - 控制过拟合

## 全连接层

- 干嘛：处理输入内容后，需要分类成N类
- channel：通道
- 用啥干：N维向量，其中每个数字都代表属于特定类别的概率（当然概率总和为1）
- 咋干：找特征，根据权重，获得概率

## Dropout层

- 一般在训练层中使用
- 干嘛：防止过拟合
- 怎么敢：丢弃该层中一个随机的激活参数集（在前向传导的时候讲这些激活参数集设置为0）

# 训练（反向传播）

## 前向传导

- 用啥干：32x32x3的数组训练图像

- 咋干：

  过滤器：随机初始化


## 损失函数\后向传播\权重更新

- 用啥干(常见损失函数)：MSE（均方误差，老朋友了，不多讲，看公式就懂了）

  $E_{total}=\sum \frac{1}{2}(traget-output)^2$

- 干嘛：想将损失数量最小化

  - 将其视为微积分优化问题：我们想要找出哪部分输入直接导致了网络的损失

  - $w=w_i-\eta \frac{dL}{dW}$，其中$w$代表权重, $w_i$代表初始权重值, $\eta$代表学习率

  - 其中学习率自己决定（越高代表跨越幅度越大，优点耗费时间少，缺点是步长太大可能直接把最优点跨过去了）

    ![](/assets/Pics/CNNpics/n1.png)
  
- 反向传播

  - 原理：链式法则（Chain Rule），神经网络中，上一个Layer的输出就是下一个Layer的输入，所以符合链式法则

    <img src="/assets/Pics/CNNpics/ChainRule.jpeg" style="zoom:50%;" />

  - 步骤：

    - Step 1: 把握关系，掌握链式法则

      - 如$y=f(x),z=g(y)$
      - $\frac{\partial z}{\partial x}=\frac{\partial y}{\partial x}\frac{\partial z}{\partial y}$

    - Step 2: 前向传递与反向传播共同作用，先进行前向传递获得$\frac{\partial y}{\partial x}$并保存

    - Step 3: 不断前向传递，直到到达最后一层，计算图如下图所示

      <img src="/assets/Pics/CNNpics/front.jpg" style="zoom:50%;" />

    - Step 4: 反向传播求得$\frac{\partial z}{\partial x}$，上图根据链式法则得到

      - $\frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\frac{\partial c}{\partial a}$
      - $\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\frac{\partial d}{\partial b}$

# 细节

## 卷积

- 参数
  - 步幅（stride）：过滤器移动的距离

    - 默认值：1
    - 增大步幅的结果：空间维度更小，filter扫过的时候重叠更少

  - 填充（padding）：

    - 为啥：想要更多的保留原始数据，更多提取低层特征

    - 怎么做：在输入内容的周围应用几次零填充

    - 如何保证输入输出内容空间维度一致（其中K代表过滤器大小 $P_0$为0填充）

      $P_0=\frac{(K - 1)}{2}$

      ![](/assets/Pics/CNNpics/n2.png)

- 结果：$O=\frac{(W-K+2P)}{S}+1$（S是步幅）

## 选择超参数

- 如何选择：自己定，找到能创造出图像在何时尺度上抽象的正确组合

  











