---

title: 机器学习笔记5-8章
status: Writing
commentable: false
Edit: 2020-9-28
mathjax: true
mermaid: true
tags: Machine_Learning
categories: Machine_Learning
Typora-root-url: ../
description: 本文为周志华老师《机器学习》（西瓜书）的学习笔记，仅供参考，如有错误，欢迎给我发邮件或者通过友链进行留言。（详情请见About）
---

读前提示：
- 如果想查看各章节的单独笔记以及作业和相关代码、数据请到[这里](https://github.com/WangXiCindy/Machine_Learning_Notes)查看

- 本文为周志华老师《机器学习》（西瓜书）的学习笔记，仅供参考，如有错误，欢迎给我发邮件或者通过友链进行留言。（详情请见About）

- 本文尚在更新中，可通过左上角的按钮切换目录索引到各章或回到专栏查看剩余章节笔记

- 笔者能力有限，本文仅供参考

# 神经网络

## 神经元模型

- **神经网络（neural networks）**是具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应

- 成分

  - **神经元（neuron）**：最基本的，相当于定义中的“简单单元”

- **M-P神经元模型**：

  ​	<img src="/assets/MLpics/T64.png" style="zoom:50%;" />

  - 神经元接收到来自n个其他神经元传递过来的输入信号
  - 这些输入信号通过带权重的连接进行传递
  - 神经元接收到的总输入值将与神经元的阈值进行比较
  - 通过**激活函数（activation function）**处理以产生神经元的输出

- 激活函数

  - 理想：阶跃函数

    - 1对应神经元兴奋
    - 0对应神经元抑制
    - 但具有不连续，不光滑等不佳性质

  - 实际：Sigmoid（别名挤压函数（squashing function））

    <img src="/assets/MLpics/T65.png" style="zoom:50%;" />

## 感知机与多层网络

- **感知机（Perceptron）**

  - 两层神经元组成

    <img src="/assets/MLpics/T66.png" style="zoom:50%;" />

  - 输入层接收外界输入信号传递给输出层

  - 输出层是M-P神经元，亦称**阈值逻辑单元（threshold logic unit）**

    - 感知机只有输出层神经元进行激活函数处理
    - 只有一层**功能神经元（functional neuron）**

  - $y=f(\sum_i w_i x_i-\theta)=f(w^Tx-\theta)$（假设f为阶跃函数）

  - 可进行逻辑与、或、非运算

    - 均属于**线性可分（linearly separable）**问题

    - 与运算（x1^x2）

      - 令$w_1=w_2=1,\theta=2,y=f(1* x _ 1+1* x _ 2-2)$
      - 仅$x_1=x_2=1,y=1$

    - 或运算（x1 v x2）

      - 令$w_1=w_2=1,\theta=0.5,y=f(1* x _1+1 * x _ 2-0.5)$
      - 仅$x_1=1 \  or \  x_2=1,y=1$

    - 非运算<img src="/assets/MLpics/T67.png" style="zoom:50%;" align="left" />

      - 令$w_1=-0.6,w_2=0,\theta=-0.5,y=f(-0.6*x_1+0.5)$
      - 仅$x_1=1,y=0$
      - $x_1=0,y=1$

    - 若两类模式是线性可分的，即存在一个线性超平面能将它们分开

      - n维空间的超平面方程为$w_1x_1+...w_nx_n=w^Tx+b=0$
      - $w^Tx-\theta$可以看作是n维空间中的一个超平面，将n维空间划分为
        - $w^Tx-\theta \ge 0$ 样本模型输出值为1
        - $w^Tx-\theta <0$ 样本模型输出值为0

      <img src="/assets/MLpics/T68.png" style="zoom:50%;" />

    - 感知机的学习过程一定会收敛（converge）而求得适当的权向量w

    - 否则学习过程会发生振荡（fluctuation），w难以稳定下来，如上图中的异或问题

  - 学习

    - 给定训练数据集，权重$w_i(i=1,2,...,n)$以及阈值$\theta$可以通过学习得到
      - 阈值作为一个固定输入为-1.0的**哑结点（dummy node）**所对应的权重$w_{n+1}$
      - 权值和阈值的学习统一为权重的学习
    - 损失函数
      - 易得$(y-\hat{y})(w^T x-\theta) \ge 0$恒成立
      - $L(w,\theta)=\sum_{x \in M}(y-\hat{y})(w^T x-\theta)$
        - M为误分类样本集合
        - 该公式连续可导
          - 没有误分类点，损失函数值为0
          - 误分类点越少，则误分类点离超平面越近，损失函数值越小
      - 求参数
        - 极小化$min_{w,\theta}L(w,\theta)=min_{w,\theta}\sum_{x_i \in M}(\hat{y_i}-{y_i})(w^T x_i-\theta)$
        - 把阈值看为哑结点，简化$w^Tx_i-\theta=w^Tx_i$
          - $min_{w}L(w,\theta)=min_{w}\sum_{x_i \in M}(\hat{y_i}-y_i)w^T x_i$
        - 假设M固定，求梯度
          - $\Delta_wL(w)=\sum_{x_i \in M}(\hat{y_i}-y_i)x_i$
      - 对训练样例（x，y），若当前感知机的输出为$\hat{y}$
        - $w_i \leftarrow w_i+\Delta w_i$
        - 参数更新公式：$\Delta w_i=\eta(y-\hat{y})x_i$
    - **学习率（learning rate）**：$\eta \in (0,1)$
    - 权重调整
      - 若$\hat{y}=y$，感知机不发生变化
      - 根据错误的程度进行权重调整

  - 多层功能神经元

    - 解决非线性可分问题

      <img src="/assets/MLpics/T69.png" style="zoom:50%;" />

    - **隐（含）层（hidden layer）**：也是拥有激活函数的功能神经元

    - **多层前馈神经网络（multi-layer feedforward neural networks）**

      - A别称两层网络（本书称为单隐层网络）

        <img src="/assets/MLpics/T70.png" style="zoom:50%;" />

        - 输入层：接收外界输入
        - 隐藏层，输出层（功能神经元）：对信号进行加工
        - 输出层：输出结果

      - 学习过程就是根据训练数据来调整神经元之间的**“连接权”（connection weight）**以及每个功能神经元的阈值

## 误差逆传播算法

- 误差逆传播（error BackPropagation，简称BP）

  - 训练多层网络的学习算法
  - 虽然可用于其他类型神经网络训练，但通常指的是多层前馈

- 算法思想

  - 给定训练集$D=\{ (x_1,y_1),...(x_m,y_m)\},x_i \in R^d，y_i \in R^l$

    - 输入示例由d个属性描述，输出l维实值向量

  - 下图有d个输入神经元，l个输出，q个隐藏层神经元

    <img src="/assets/MLpics/T71.png" style="zoom:50%;" />

    - $\theta_j$：输出层第j个神经元的阈值
    - $\gamma_h$：隐藏层第h个神经元的阈值
    - $v_{ih}$：输入层第i个神经元与隐藏层第h个神经元之间的连接权
    - $w_{hj}$：隐藏层第h个神经元与输出层第j个神经元之间的连接权
    - $b_h$：隐藏层第h个神经元的输出
    - 使用sigmoid
    - 第h个隐藏层神经元的输入和第j个输出神经元的输入见上图

  - 对训练样例$(x_k,y_k)$，假定$\hat{y_k}=(\hat{y_1}^k,\hat{y_2}^k...\hat{y_l}^k)$

    - $\hat{y_j}^k=f(\beta_j-\theta_j)$

    - 则网络在样例上的均方误差为

      $E_k=\frac{1}{2}\sum_{j=1}^l(\hat{y_j}^k-y_j^k)^2$

  - 上图的网络中有 (d+l+1)q+l个参数需要确定

    - 输入层到隐藏层：dq
    - 隐藏层：q
    - 隐藏层到输出层：ql
    - 输出层：l

  - BP算法公式推导

    - 迭代学习

    - 基于梯度下降，以目标的负梯度方向对参数进行调整

    - 在每一轮中采用广义感知机对参数进行更新估计：$v \leftarrow v+\Delta v$

    - 以上图中$w_{hj}$为例

      - $\Delta w_{hj}=- \eta \frac{\delta E_k}{\delta w_{hj}}$

      - 先影响到第j个输出层神经元的输入值$\beta_j$，再影响到其输出值$\hat{y_j}^k$

        - $\frac{\delta E_k}{\delta w_{hj}}=\frac{\delta E_k}{\delta \hat{y_j}^k}·\frac{\delta  \hat{y_j}^k}{\delta \beta_j}·\frac{\delta \beta_j}{\delta w_{hj}}$

        - $\frac{\delta \beta_j}{\delta w_{hj}}=b_h$

        - sigmoid函数的性质$f'(x)=f(x)(1-f(x))$

          $$\begin{aligned}g_j &=-\frac{\delta E_k}{\delta \hat{y_j}^k}·\frac{\delta  \hat{y_j}^k}{\delta \beta_j} \\ &=-(\hat{y_j}^k-y_j^k)f'(\beta_j-\theta_j) \\ &=\hat{y_j}^k(1-\hat{y_j}^k)(y_j^k-\hat{y_j}^k) \end{aligned}$$

        - $\Delta w_{hj}=- \eta g_j b_h$

        - 同理可得（不懂推导的童鞋可以参考南瓜书）

          - $\Delta \theta_j=-\eta g_j$

            - 个人的简单理解：和上方$w_{hj}$同公式，但$\theta$和输入值无关，所以不需要$b_h$

          - $\Delta v_{ih}=\eta e_h x_i$

            - 非常类似于$w_{hj}$的推导，只是这次为求输入层和隐藏层的连接权

          - $\Delta \gamma_h=-\eta e_h$

            - 同$\theta$

          - 注意$e_h$

            $$\begin{aligned} e_h &=-\frac{\delta E_k}{\delta b_h}·\frac{\delta b_h}{\delta \alpha_h} \\ &=-\sum_{j=1}^l \frac{\delta E_k}{\delta \beta_j}·\frac{\delta \beta_j}{\delta b_h} f'(\alpha_h-\gamma_h) \\ &=\sum_{j=1}^l w_{hj}g_j f'(\alpha_h-\gamma_h) \\ &=b_h(1-b_h)\sum_{j=1}^l w_{hj}g_j\end{aligned}$$

          - 注意$\eta$

            - 学习率控制者每一轮迭代中的更新步长，
              - 太大容易振荡
              - 太小收敛速度会过慢
            - 精细调节
              - $w_{hj},\theta$和$v_{ih},\gamma_h$使用的$\eta$可以不同

  - BP算法的工作流程

    - BP算法的目标：最小化训练集D上的累积误差$E=\frac{1}{m}\sum_{k=1}^m E_k$
    - 标准BP算法：每次仅针对一个训练样例更新连接权和阈值
      - 也就是说下图的更新规则是基于单个$E_k$推导得到
      - 参数更新频繁
      - 对不同样例可能会出现抵消现象
      - 为了达到同样的累积误差最小点，需要更多次的迭代
      - 对应**随机梯度下降（stochastic gradient descent，简称SGD）**

    <img src="/assets/MLpics/T72.png" style="zoom:50%;" />

    结果示例：

    <img src="/assets/MLpics/T73.png" style="zoom:50%;" />

  - **累积误差逆传播（accumulated error backpropagation）**

    - 基于累积误差最小化的更新规则
    - 在读取整个训练集D一遍后才对参数进行更新
    - 更新频率更低
    - 但下降到一定程度之后，进一步下降会非常缓慢，这时标准BP会更快获得较好的解（在D非常大时更明显）
    - 对应**标准梯度下降**

- **试错法（trial-by-error）**

  - **只需要一个包含足够多神经元的隐藏层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数**
  - 如何设置隐藏层神经元的个数

- 过拟合缓解

  - **早停（early stopping）**
    - 将数据氛围训练集和验证集
    - 训练集：计算梯度、更新连接权和阈值
    - 验证集：估计误差
    - 训练集误差降低但验证集误差升高，则停止训练，返回连接权和阈值
  - **正则化（regularization）**
    - 在误差目标函数中增加一个用于描述网络复杂度的部分
    - 例如连接权与阈值的平方和
    - $E=\lambda \frac{1}{m}\sum_{k=1}^mE_k+(1-\lambda)\sum_i w_i^2$
      - $\lambda \in (0,1)$
        - 使用交叉验证进行估计
        - 用于对经验误差与网络复杂度这两项进行折中

## 全局最小与局部极小

- E表示训练集上的误差，训练过程为在参数空间中，寻找一组最优参数使得E最小

  - 若存在$w ^ *,\theta ^ *,\epsilon>0$使得

    $\forall (w;\theta) \in \{ (w;\theta) \vert \ \vert\vert (w;\theta)-(w^ *;\theta^ *) \vert\vert \le \epsilon \}$

- **局部极小（local minimum）**
  - 对上式都有$E(w;\theta) \ge E(w^ *;\theta^ *)$成立，$(w^ *;\theta^ *)$为局部最小解
  - 其邻域点的误差函数值均不小于该点的函数值
  - 对应的$E(w^ *;\theta^ *)$为局部极小值
  - 梯度为0，误差函数值小于邻点
  - 可能存在多个
- **全局最小（global minimum）**
  - 对参数空间的任意$(w;\theta)$都有$E(w;\theta) \ge E(w^ *;\theta^ *),(w^ *;\theta^ *)$为全局最小解
  - 所有点的误差函数值均不小于该点的函数值
  - 对应的$E(w^ *;\theta^ *)$为全局最小值
  - 只能存在一个
  - 一定是局部极小

<img src="/assets/MLpics/T74.png" style="zoom:50%;" />

- 基于梯度的搜索
  - 最广泛的参数寻优
  - 思路
    - 从初始解出发，迭代寻找最优参数
    - 每次迭代中
      - 先计算误差函数在当前点的梯度
      - 根据梯度决定搜索方向
        - 负梯度方向是函数值下降最快的方向
      - 若当前点的梯度为0，则达到局部最小，参数的迭代在这里停止
    - 找到全局最小（大多启发式，理论上缺乏保障）
      - 只有一个局部极小=全局最小
      - 多个局部极小，则试图“跳出”局部极小
        - 方法1：以多组不同参数初始化多个神经网络，取误差最小的解最为最终参数（需要陷入不同的局部极小）
        - 方法2：**模拟退火（simulated annealing）**，每一步都以一定的概率接受比当前解更差的结果
          - 在每步迭代的过程中，接受“次优解”的概率要随着时间的推移而逐渐降低
        - 方法3：随机梯度下降，在计算梯度时加入了随机因素，陷入局部极小点时，它计算出的梯度仍可能不为0
        - 方法4：**遗传算法（genetic algorithms）**

## 其他神经网络

- **RBF（Radial Basis Function，径向基函数）**网络

  - 单隐层前馈神经网络

  - 使用径向基函数作为隐层神经元激活函数

  - 输出层：对隐层神经元的线性组合

  - 假定输入d维向量x

    $\phi(x)=\sum_{i=1}^q w_i \rho(x,c_i)$

    - q为隐层神经元个数

    - $c_i,w_i$分别为第i个隐层神经元对应的中心和权重

    - $\rho(x,c_i)$为径向基函数（某种沿径向对称的标量函数）

      - 样本x到数据中心$c_i$之间欧式距离的单调函数

      - 常用高斯径向基函数

        $\rho(x,c_i)=e^{-\beta_i \vert\vert x-c_i \vert\vert^2}$

  - 步骤

    - 确定神经元中心$c_i$
      - 方式：随机采样，聚类
    - 利用BP算法等来确定参数$w_i,\beta_i$

- **ART（Adaptive Resonance Theory，自适应谐振理论）网络**

  - **竞争型学习（competitve learning）**
    - 无监督学习策略
    - **胜者通吃（winner-take-all）**原则
      - 网络的输出神经元相互竞争
      - 每个时刻仅有一个竞争获胜的神经元被激活，其它的被抑制
    - ART是其中的重要代表
  - 组成
    - 比较层：接收输入样本，并将其传递给识别层神经元
    - 识别层：每个神经元对应一个模式类，神经元数目可在训练过程中动态增长增加新的模式类
      - 模式类可以认为是某个类比的子类
    - 识别阈值
    - 重置模块
  - 过程
    - 识别层神经元收到比较层的输入信号
    - 神经元之间相互竞争产生获胜神经元
      - 计算输入向量与每个识别层神经元所对应的 模式类的 代表向量之间的距离
      - 距离最小者胜
    - 获胜神经元向其他识别层神经元发送信号，抑制激活
    - 输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值
      - 当前输入样本被归为该代表向量所属类别
      - 网络连接权更新
      - 以后再接收到相似输入样本时该模式类会计算出更大的相似度，从而该获胜神经元更大可能获胜
    - 相似度不大于识别阈值
      - 重置模块会在识别层增设一个新的神经元
      - 代表向量为当前输入向量
  - 识别阈值对性能影响
    - 识别阈值较高：输入样本将会分成比较多的、精细的模式类
    - 识别阈值较低：产生比较少、比较粗略的模式类
  - **可塑性-稳定性窘境（stability-plasticity dilemma）**
    - 可塑性：神经网络要有学习新知识的能力
    - 稳定性：神经网络在学习新知识时要保持对旧知识的记忆
    - ART的优点：可以进行**增量学习（incremental learning）**或**在线学习（online learning）**

- **SOM（Self-Organizing （Feature） Map，自组织（特征）映射）**网络

  - 属于竞争学习型的无监督神经网络

  - 功能

    - 将高维输入数据映射到低维空间（通常为二维）

    - 同时保持输入数据在高维空间的拓扑结构

    - 也就是将高维空间中相似的样本点映射到网络输出层中的邻近神经元，从而保持拓扑结构

      <img src="/assets/MLpics/T75.png" style="zoom:50%;" />

  - 过程

    - 接收到一个训练样本
    - 每个输出层神经元会计算该样本与自身携带的权向量之间的距离
    - **最佳匹配单元（best matching unit）**：距离最近的神经元成为竞争获胜者
    - 最佳匹配单元及其邻近神经元的权向量将被调整，使得这些权向量与当前输入样本的距离缩小
    - 不断迭代直至收敛

- **级联相关（Cascade-Correlation）**网络

  - 属于**结构自适应（亦称 构造性（constructive））**神经网络
    
  - 不同于一般的网络结构事先固定，将网络结构也当作学习目标之一
    
  - 训练

    ​	<img src="/assets/MLpics/T76.png" style="zoom:50%;" />

    - 级联：建立层次连接的层级结构
      - 开始时，网络只有输入层和输出层，处于最小拓扑结构
      - 新的隐层神经元逐渐加入
        - 输入端连接权值是冻结固定的
    - 相关：最大化新神经元的输出与网络误差之间的相关性来训练参数

  - 优劣势

    - 无需设置网络层数，隐层神经元网络，训练速度较快
    - 在数据较小时容易陷入过拟合

- Elman网络

  - 属于**递归神经网络（recurrent neural networks）**
    - 允许网络中出现环形结构
    - 可以让一些神经元的输出反馈回来作为输入信号
    - 因此网络在t时刻的输出状态不仅与t时刻的输入状态和t-1时刻的网络状态有关，能够处理与时间有关的动态变化
  - 与前馈区别：隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起作为隐层神经元在下一时刻的输入
  - 隐层神经元：Sigmoid激活函数
  - 网络训练：BP算法

- Boltzmann机（亦称“平衡态（equilibrium）或平稳分布（stationary distribution））

  - **基于能量的模型（energy-based model）**

    - 为网络状态定义一个能量，其最小化时网络达到理想状态，网络训练就是在最小化这个能量函数

  - 神经元

    - 分为两层
      - 显层：表示数据的输入与输出
      - 隐层：数据的内在表达
    - 类型：布尔型的
      - 只能取0和1两种状态
        - 0：抑制
        - 1：激活

  - 能量定义

    - 令向量$s \in \{ 0,1  \}^n$表示n个神经元的状态

    - $w_{ij}$表示神经元i与j之间的连接权

    - $\theta_i$表示神经元i的阈值

    - 状态s对应的Boltzmann机能量的定义为

      $E(s)=-\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{ij} s_i s_j - \sum_{i=1}^n \theta_i s_i$

    - 推导

      - 能量值越大，当前状态越不稳定（物理学上），能量值最小系统处于稳定态

      - Boltzmann机本质上引入了隐变量的无向图模型，能量为

        $E_{graph}=E_{edges}+E_{nodes}$

      - 分别代表图、边和结点的能量

      - 边能量：两连接结点的值及其权重的乘积$E_{edge_{ij}}=-w_{ij}s_is_j$

      - 结点能量：结点的值及其阈值的乘积$E_{nodes_i}=-\theta_is_i$

      - $E_{edges}=\sum_{i=1}^{n-1}\sum_{j=i+1}^n E_{edge_{ij}}=-\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{ij}s_is_j$

      - $E_{nodes}=\sum_{p=1}^n E_{node_i}=-\sum_{p=1}^n \theta_i s_i$

      - $E_{graph}=-\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{ij}s_is_j-\sum_{p=1}^n \theta_i s_i$

  - Boltzmann分布

    - 若网络中的神经元以任意不依赖于输入值的顺序进行更新，最终网络将达到Boltzmann分布

    - 状态s出现的概率将仅由其能量与所有可能状态向量的能量确定

    - 推导

      - 注意相关知识在14.2节

      - 无向图网络联合概率分布

        - k为无向图中的极大团个数
        - $c_i$为极大团的节点集合
        - $x_{c_i}$为极大团所对应的节点变量
        - $\Phi_i$为势函数
        - Z为规范化因子
        - $P(s)=\frac{1}{Z}\prod_{i=1}^k \Phi_i(s_{c_i})$

      -  Boltzmann机的极大团只有一个（为全连接网络），结点集合为$c=\{ s_1,s_2,...,s_n \}$

        - 联合概率分布为

          $P(s)=\frac{1}{Z} \Phi(s_c)$

      - 势函数$\Phi(s_c)$一般定义为指数型函数，所以其一般形式为

        $\Phi(s_c)=e^{-E(s_c)}$

      - 其中$s_c=s$

        - 状态集合T中某个状态s的概率定义：状态s的联合概率分布与所有可能的状态的联合概率分布的比值

        - 则状态s下的联合概率分布为

          $P(s)=\frac{1}{Z} e^{-E(s)}$

        - $P(s)=\frac{e^{-E(s)}}{\sum_{t \in T} e^{-E(t)}}$

      - $P(s)=\frac{e^{-E(s)}}{\sum_t e^{-E(t)}}$            （公式5.21）

  - 训练过程

    - 将每个样本视为一个状态向量，使其出现的概率尽可能大

  - 分类

    ​		<img src="/assets/MLpics/T77.png" style="zoom:50%;" />

    - 标准的Boltzmann机：是一个全连接图，训练网络的复杂度很高，难以解决现实任务
    - **受限Boltzmann机（Restricted Boltzmann Machine，简称RBM）**：仅保留显层与隐层之间的连接，从而将其结构由完全图转为二部图
      - 使用**对比散度（Contrastive Divergence，简称CD）**算法
      - 假定网络中有d个显层神经元和q个隐层神经元
      - 令v和h分别表示显层与隐层的状态向量，则由于同一层内不存在连接
      - $P(v \vert h) = \prod_{i=1}^d P(v_i \vert h)$
      - $P(h \vert v) = \prod_{j=1}^q P(h_j \vert v)$
      - CD算法对每个训练样本v
        - 先根据上式计算出隐层神经元状态的概率分布
        - 根据这个概率分布采样得到h
        - 类似地根据上上式从h产生$v'$，再从$v'$产生$h'$
        - 连接权的更新公式：$\Delta w=\eta(vh^T-v'h'^T)$（推导见本章附录1）

## 深度学习

- 诞生原因（复杂模型的缺陷和当下为何可用）

  - 计算能力大幅提高，缓解训练低效性
  - 训练数据的大幅增加可降低过拟合风险

- 提高容量

  - 容量越大，能完成更复杂的学习任务
  - 增加隐层神经元的数目
  - 增加隐层的数目（更有效）
    - 增加拥有激活函数的神经元数目
    - 增加激活函数嵌套的层数
    - 多隐层（三个以上隐层）神经网络，难以用经典算法（如标准BP）进行训练，因为会**发散（diverge）**而不能收敛到稳定态

- **无监督逐层训练（unsupervised layer-wise training）**

  - 多隐层网络训练的有效手段

  - 思想

    - 预训练+微调：可视为将大量参数分组，对每组先找到局部较优，然后基于这些结果进行全局寻优

      - 目的
        - 利用模型大量参数所提供的自由度时间
        - 节省训练开销
      - **预训练（pre-training）**
        - 每次训练一层隐结点
        - 训练时将上一层隐结点的输出作为输入
        - 本层隐结点的输出作为下一层隐结点的输入
      - **微调（finetuning）**

    - **权共享（weight sharing）**

      - 让一组神经元使用相同的连接权

      - 用于CNN

        - **特征映射（feature map）**：一个由多个神经元构成的平面

        - **RELU**：将sigmoid激活函数替换为修正线性函数
            $$
            f(x)=\left\{\begin{array}{ll}
            0, & \text { if } x<0 \\
            x, & \text { otherwise }
            \end{array}\right.
            $$

        - **采样层（亦称汇合层）（pooling）**：基于局部相关性原理进行亚采样，减少数据量，保留有用信息

        - 可用BP训练

- 理解

  - **特征学习（feature learing）或表示学习（representation learning）**
  - 对输入信号进行逐层加工，把初始输入转化为和输出目标联系更密切的表示，让最后一层输出映射成为可能
    - 逐渐将初始的低层特征转化为高层特征表示后
    - 用简单模型即可完成复杂的分类等学习任务

- **特征工程（feature engineering）**

  - 用于描述样本的特征
  - 现在一般为人工设计
  - 特征学习通过ML技术产生好特征

## 附录1

- 受限Boltzmann机连接权更新公式推导

- 本推导来自南瓜书

  <img src="/assets/MLpics/T78.png" style="zoom:50%;" />

  <img src="/assets/MLpics/T79.png" style="zoom:50%;" />

  <img src="/assets/MLpics/T80.png" style="zoom:50%;" />

  <img src="/assets/MLpics/T81.png" style="zoom:50%;" />


## 习题

- 5.1 试述将线性函数$f(x)=w^Tx$用作神经元激活函数的缺陷

  - 神经网络中必须要有非线性的激活函数，如果全部用线性函数做激活函数，无论就是线性回归而非神经网络。

- 5.2 试述使用图5.2(b)激活函数的神经元与对率回归的联系

  <img src="/assets/MLpics/T65.png" style="zoom:50%;" />

  - 相同点：两者都是将连续值映射到{0,1}上
  - 不同点：激活函数不一定要使用sigmoid，只要是非线性的可导函数都可以使用。

- 5.3 对于图5.7中的$v_{ih}$，试推导出BP算法的更新公式

  见附录1

- 5.4 试述式（5.6）中学习率的取值对神经网络训练的影响

  - 式5.6 $\Delta w_{hj}=- \eta \frac{\delta E_k}{\delta w_{hj}}$
  - 学习率过大，可能会出现扰动现象，在最小值附近来回波动
  - 学习率过小，会导致下降过慢，迭代次数很多

- 5.5 试编程实现标准BP算法和累积BP算法，在西瓜数据集3.0上分别用这两个算法训练一个单隐层网络，并进行比较

  - ```python
    # 标准BP算法
    # hideNum为隐层神经元个数
    def BP(X,Y,hideNum=5,eta=0.01,epoch=1000000):
    
        # 权值及偏置初始化
        V = np.random.rand(X.shape[1],hideNum)
        V_b = np.random.rand(1,hideNum)
        W = np.random.rand(hideNum,Y.shape[1])
        W_b = np.random.rand(1,Y.shape[1])
    
        trainNum=0
        
        while trainNum<epoch:
            # 标准BP每次处理一个样本
            for k in range(X.shape[0]):
                B_h=sigmoid(X[k,:].dot(V)-V_b) # 输入层-隐层 注意是减去阈值
                Y_=sigmoid(B_h.dot(W)-W_b)     # 隐层-输出层 注意是减去阈值
                loss=sum((Y[k]-Y_)**2)*0.5      # 算均方误差
                
                # 计算梯度并更新参数
                g=Y_*(1-Y_)*(Y[k]-Y_)
                e=B_h*(1-B_h)*g.dot(W.T)
                
                # 参数更新
                W+=eta*B_h.T.dot(g)
                W_b-=eta*g
                V+=eta*X[k].reshape(1,X[k].size).T.dot(e)
                V_b-=eta*e
                trainNum+=1
                
        print("标准BP")
        print("总训练次数：",trainNum)
        print("最终损失：",loss)
    ```

  - ```python
    # 累积BP算法
    def BPAcc(X,Y,hideNum=5,eta=0.01,epoch=1000000):
        
        # 权值及偏置初始化
        V = np.random.rand(X.shape[1],hideNum)
        V_b = np.random.rand(1,hideNum)
        W = np.random.rand(hideNum,Y.shape[1])
        W_b = np.random.rand(1,Y.shape[1])
        
        trainNum=0
        
        while trainNum<epoch:
            # 累积BP直接处理所有样本
            B_h=sigmoid(X.dot(V)-V_b)   # 输入层-隐层 注意是减去阈值
            Y_=sigmoid(B_h.dot(W)-W_b)  # 隐层-输出层 注意是减去阈值
            loss=0.5*sum((Y-Y_)**2)/X.shape[0]     # 算均方误差
            
            # 计算梯度并更新参数
            g=Y_*(1-Y_)*(Y-Y_)
            e=B_h*(1-B_h)*g.dot(W.T)
                
            # 参数更新
            W+=eta*B_h.T.dot(g)
            W_b-=eta*g.sum(axis=0)
            V+=eta*X.T.dot(e)
            V_b-=eta*e.sum(axis=0)
            trainNum+=1
            
        print("累积BP")
        print("总训练次数：",trainNum)
        print("最终损失：",loss)
    ```

  - 结果

    <img src="/assets/MLpics/T82.png" style="zoom:50%;" />

    - 累积BP性能更好

- 5.6 试设计一个BP改进算法，能通过动态调整学习率显著提升收敛速度，编程实现该算法，并选择两个UCI数据集与标准BP算法进行实验比较

  - BP优化算法，其中自适应调节学习率相关领域现在比较火热
  - 推荐blog[深度学习 --- BP算法详解（BP算法的优化）](https://blog.csdn.net/weixin_42398658/article/details/83958133)

- 5.7 根据式（5.18）和（5.19），试构造一个能解决疑惑问题的单层RBF神经网络

  - ```python
    class RBF():
        # 权值及偏置初始化
        # 注意是单层RBF
        def __init__(self):
            self.hideNum=4
            self.epoch=10000
        
            self.w = np.random.rand(self.hideNum,1)
            self.beta = np.random.rand(self.hideNum,1)
            self.c=np.random.rand(self.hideNum,2)   #中心
            
        def forward(self,X):
            self.X=X
            self.dist=np.sum((X-self.c)**2,axis=1,keepdims=True)
            # 高斯径向基
            self.rho=np.exp(-self.beta*self.dist)# 注意径向基为激活函数，相当于BP的sigmoid
            self.y=self.w.T.dot(self.rho)
            # w第一位代表w_b,所以y第一位代表预测值
            return self.y[0, 0]
            
            
        # 梯度下降
        # 通过y回退
        def grad(self,y):
            grad_y=self.y-y
            grad_w=grad_y*self.rho
            grad_rho=grad_y*self.w
            grad_beta=-grad_rho*self.rho*self.dist
            grad_c=grad_rho*self.rho*2*self.beta*(self.X-self.c)
            self.grads = [grad_w, grad_beta, grad_c]
            
        # 参数更新
        def update(self,eta=0.01):
            self.w-=eta*self.grads[0]
            self.beta-=eta*self.grads[1]
            self.c-=eta*self.grads[2]
        
        def loss(self,X,y):
            y_=self.forward(X)
            loss=0.5*(y_-y)**2
            return loss
        
        def train(self,X,y):
            losses=[]
            for e in range(self.epoch):
                loss=0
                for i in range(len(X)):
                    self.forward(X[i])
                    self.grad(y[i])
                    self.update()
                    loss+=self.loss(X[i],y[i])
                    
                losses.append(loss)
            return losses
    ```

  - 结果

    <img src="/assets/MLpics/T83.png" style="zoom:50%;" />

# 支持向量机

- **支持向量机（Support Vector Machine，简称SVM）**

## 间隔与支持向量

- 分类学习：找到一个划分超平面，如何找到最合适的一个？

  - 如下图所示，直观看需要找到正中间最粗的那条超平面

    - 该划分超平面对训练样本局部扰动的“容忍性”最好
    - 容忍性：例如由于训练集的局限性或噪声，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，而中间的超平面受影响最小
      - 也就是其分类结果是最鲁棒的，对未见示例的泛化能力最强

    <img src="/assets/MLpics/T84.png" style="zoom:50%;" />

- 划分超平面线性方程$w^Tx+b=0$

  - 其中$w=(w_ 1;w_ 2;...;w_ d)$为法向量，决定了超平面的方向

  - b为位移，决定了超平面与原点之间的距离

  - 样本空间中任意点x到超平面（w,b）的距离可写为$r=\frac{\vert w^Tx+b \vert}{\vert \vert w \vert \vert}$

  - 假设超平面（w,b）能将训练样本正确分类，即对于$(x_ i,y_ i) \in D,若 y_ i=+1，则w^Tx_ i+b <0$

  - 式（6.3）：

    $$ y=\left\{\begin{array}{cl}w^Tx_ i+b \ge +1, & y_ i=+1 \\ w^Tx_ i+b \le -1 & y_ i=-1  \end{array}\right. $$

  - 如下图所示

    - 距离最近的训练样本使上式等号成立，即为**支持向量（supprot vector）**
    - **间隔（margin）**：两个异类支持向量到超平面的距离为$\gamma=\frac{2}{\vert \vert w \vert \vert}$
      - 间隔与w有关也与b有关，因为b通过约束隐式地影响着w的取值，进而对间隔产生影响

    <img src="/assets/MLpics/T85.png" style="zoom:50%;" />

    - **最大间隔（maximum margin）**：找到满足6.3的w和b使得$\gamma$最大

      - （6.5）

        $max_ {w,b} \frac{2}{\vert \vert w \vert \vert}$

        $s.t. \  y_ i(w^Tx_ i+b) \ge 1  \ \ i=1,2,...m$

      - 最大化$\vert \vert w \vert \vert ^{-1}$，等价于最小化$\vert \vert w \vert \vert ^2$

      - 重写得到SVM的基本型（6.6）

        $max_ {w,b} \frac{\vert \vert w \vert \vert ^2}{2}$

        $s.t. \  y_ i(w^Tx_ i+b) \ge 1  \ \ i=1,2,...m$

## 对偶问题

- 求解6.6得到大间隔划分超平面对应的模型$f(x)=w^Tx+b$

- 方法1：

  - 式子6.6本身是一个**凸二次规划（convex quadratic programming）**，可以直接用优化计算包求解

- 方法2:

  - 使用拉格朗日乘子法得到**对偶问题（dual problem）**

  - 对式6.6的每条约束添加拉格朗日乘子$\alpha_ i \ge 0$，则拉格朗日函数可写为

    （6.8）$L(w,b,\alpha)=\frac{1}{2} \vert \vert w \vert \vert ^2+\sum_ {i=1}^m \alpha_ i(1-y_ i(w^Tx_ i+b))$

  - 其中$\alpha=(\alpha_ 1;\alpha_ 2;...;\alpha_ m)$

  - 令$L(w,b,\alpha)$对w和b的偏导为0可得

    （6.9）$w=\sum_ {i=1}^m \alpha_ iy_ ix_ i$

    （6.10）$0=\sum_ {i=1}^m \alpha_ iy_ i$        （对w求导再对b求导得到）

  - 把6.9代入6.8，可以将w和b消除，考虑6.10的约束，就得到对偶问题（6.11）

    $max_ {\alpha} \sum_ {i=1}^m \alpha_ i-\frac{1}{2}\sum_ {i=1}^m \sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j x_ i^T x_ j$

    $s.t. \ \  \sum_ {i=1}^m \alpha_ i y_ i=0$

    $\alpha_ i \ge 0,i=1,2,...,m$

  - 解出$\alpha$（拉格朗日乘子），$\alpha_ i$对应$(x_ i,y_ i)$

    - 求出w和b（6.12）

      $f(x)=w^Tx+b=\sum_ {i=1}^m \alpha_ i y_ i x_ i^Tx+b$

  - 6.6中有不等式约束，因此上述过程需要满足**KKT（Karush-Kuhn-Tucker）**

    - 证明见附录B.1

    $$ \left\{\begin{array}{cl}\alpha_ i \ge 0; \\ 1-y_ if(x_ i) \le 0; \\ \alpha_ i(y_ i f(x_ i)-1)=0  \end{array}\right. $$

    - 若$\alpha_ i=0$，则该样本不会在6.12中出现，不会对f(x)有任何影响
    - 若$\alpha_ i>0$，则必有$y_ i f(x_ i)=1$，对应样本点位于最大间隔边界上
    - SVM性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关

  - 求解6.11:

    - 二次规划问题（见附录B.2）

    - 但该问题的规模正比于训练样本数，会造成很大开销

    - **SMO（Sequential Minimal Optimization）**算法

      - 先固定$\alpha_ i$之外的所有参数，然后求$\alpha_ i$上的极值

      - 由于存在约束$\sum_ {i=1}^m \alpha_ i y_ i=0$

      - 若固定$\alpha_ i$之外的其他变量，则其可由其他变量导出

      - SMO每次选择$\alpha_ i,\alpha_ j$，并固定其他参数，进行初始化

      - SMO不断进行以下步骤进行收敛

        - 选取一对需要更新的变量$\alpha_ i,\alpha_ j$
        - 固定$\alpha_ i,\alpha_ j$以外的参数，求解6.11获得更新后的$\alpha_ i,\alpha_ j$

      - SMO先选取违背KKT条件程度最大的变量

        - 只需选取的$\alpha_ i,\alpha_ j$中有一个不满足KKT，目标函数就会在迭代后减小
        - KKT条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大

      - 再选取一个使目标函数值见效最快的变量

        - 减幅的复杂度过高，因此SMO采用了启发式
          - 选取的两变量所对应样本之间的间隔最大
          - 这样的两个变量会有很大的差别，与对两个相似的变量进行更新对比，对它们进行更新会带给目标函数值更大的变化

      - SMO高效的原因

        - 在固定其他参数后，仅优化两个参数的过程能做到非常高效

        - 仅考虑$\alpha_ i,\alpha_ j$，6.11中的约束可以重写为

          $\alpha_ iy_ i+\alpha_ jy+j=c，\alpha_ i \ge 0，\alpha_ j \ge 0$

        - 其中$c=-\sum_ {k \not=i,j}\alpha_ ky_ k$是使得$\sum_ {i=1}^m \alpha_ i y_ i=0$成立的常数

        - 用$\alpha_ iy_ i+\alpha_ jy_ j=c$消去6.11中的变量$\alpha_ j$，得到一个关于$\alpha_ i$的单变量二次规划问题，仅有约束$\alpha_ i \ge 0$

        - 这样的二次规划问题具有闭式解，于是不必调用数值优化算法即可高效计算出更新后的$\alpha_ i,\alpha_ j$

      - 确定偏移项b，对任意$(x_ s,y_ s)$都有$y_ sf(x_ s)=1$

        - （6.17）：$y_ s(\sum_ {t \in S} \alpha_ i y_ i x_ i^T x_ s+b)=1$
        - $S=\{i \vert \alpha_ i >0,i=1,2,...,m\}$为所有支持向量的下标集
        - 理论上，可选取任意支持向量并通过求解6.17获得b
        - 现实上，使用更鲁棒的做法
          - 使用所有支持向量求解的平均值
          - $b=\frac{1}{\vert S \vert} \sum_ {s \in S}(y_ s-\sum_ {i \in S} \alpha_ i y_ i x_ i^T x_ s)$

## 核函数

- 现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面

  - 例如异或问题

  <img src="/assets/MLpics/T86.png" style="zoom:50%;" />

- 如何解决

  - 将样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间内线性可分，如上图，将二维空间映射到三维
  - 如果原始空间是有限维（属性数有限），一定存在一个高维特征空间使样本可分

- 公式推导

  - $\phi(x)$表示将x映射后的特征向量

  - 超平面所对应的模型$f(x)=w^T\phi(x)+b$

  - w和b是模型参数，类似式6.6，有

    $min_ {w,b}=\frac{1}{2} \vert \vert w \vert \vert ^2$

    $s.t. \ \ y_ i(w^T\phi(x_ i)+b) \ge 1,\ i=1,2,...,m$

  - 对偶问题是（6.21）

    $max_ {\alpha} \sum_ {i=1}^m \alpha_ i-\frac{1}{2}\sum_ {i=1}^m \sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j \phi(x_ i)^T \phi(x_ j)$

    $s.t. \ \  \sum_ {i=1}^m \alpha_ i y_ i=0$

    $\alpha_ i \ge 0,i=1,2,...,m$

  - 求解6.21

    - $\phi(x_ i)^T\phi(x_ j)$是样本$x_ i,x_ j$映射到特征空间之后的内积

    - 为避免由于特征空间维数过高导致的计算困难，设计函数

      $\kappa(x_ i,x_ j) = \ <\phi(x_ i),\phi(x_ j)> \ =\phi(x_ i)^T\phi(x_ j)$

    - 即$x_ i,x_ j$在特征空间的内积等于它们在原始样本空间中通过函数$k(·,·)$计算的结果

    - 因此6.21可以重写为

      $max_ {\alpha} \sum^m_ {i=1}\alpha_ i-\frac{1}{2}\sum_ {i=1}^m\sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j k(x_ i,x_ j)$

      $s.t. \ \ \sum_ {i=1}^m \alpha_ i y_ i=0$

      $\alpha_ i \ge 0,i=1,2,...,m$

    - 求解得到式6.24

      $\begin{aligned} f(x)&=w^T\phi(x)+b \\ &= \sum_ {i=1}^m \alpha_ i y_ i \phi(x_ i)^T \phi(x)+b \\ &=\sum_ {i=1}^m \alpha_ i y_ i k(x,x_ i)+b\end{aligned}$

    - 这里的函数$\kappa(·,·)$就是**“核函数（kernel function）”**

    - 6.24显示出模型最优解可通过训练样本的核函数展开，这一展式亦称**“支持向量展式”（support vector expansion）**

- **核函数**

  - 当$\chi$为输入空间，$\kappa(·,·)$是定义在$\chi \times \chi$上的对称函数

  - 则$\kappa$是核函数当且仅当对于任意数据$D= \{x_ 1,x_ 2,...,x_ m\}$

  - **“核矩阵”（kernel matrix）K**总是半正定的

    $$\mathbf{K}=\left[\begin{array}{ccccc}\kappa\left(\boldsymbol{x}_ {1}, \boldsymbol{x}_ {1}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {1}, \boldsymbol{x}_ {j}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {1}, \boldsymbol{x}_ {m}\right) \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ \kappa\left(\boldsymbol{x}_ {i}, \boldsymbol{x}_ {1}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {i}, \boldsymbol{x}_ {j}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {i}, \boldsymbol{x}_ {m}\right) \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ \kappa\left(\boldsymbol{x}_ {m}, \boldsymbol{x}_ {1}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {m}, \boldsymbol{x}_ {j}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {m}, \boldsymbol{x}_ {m}\right)\end{array}\right]$$

  - 只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用

  - 对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$

  - 换言之，任何一个核函数都隐式定义了一个**“再生核希尔伯特空间”（Reproducing Kernel Hilbert Space，简称RKHS）**的特征空间

  - 我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要

- 核函数选择

  - SVM的最大变数

  - 常用核函数

    <img src="/assets/MLpics/T87.png" style="zoom:50%;" />

  - 函数组合

    - 若$\kappa_ 1，\kappa_ 2$为核函数，则对于任意正数$\gamma_ 1，\gamma_ 2$，其线性组合$\gamma_ 1 \kappa_ 1+\gamma_ 2 \kappa_ 2$也是核函数
    - 若$\kappa_ 1，\kappa_ 2$为核函数，则核函数的直积$\kappa_ 1 \otimes \kappa_ (x,z)=\kappa_ 1(x,z)\kappa_ 2(x,z)$也是核函数
    - 若$\kappa_ 1$为核函数，则对于任意函数$g(x)$，$\kappa(x,z)=g(x)\kappa_ 1(x,z)g(z)$也是核函数

## 软间隔与正则化

- 现实中很难确定合适的核函数使得训练样本在特征空间中线性可分

- 即使找到也很难判断是否是由于过拟合造成的

- **硬间隔（hard margin）**：要求所有样本均满足约束（6.3），前面提到的SVM都是如此

- **软间隔（soft margin）**：允许SVM在一些样本上出错

  - 式6.28：$y_ i(w^Tx_ i+b) \ge 1$

  <img src="/assets/MLpics/T88.png" style="zoom:50%;" />

- **软间隔支持向量机**

  - 最大化间隔的同时，不满足约束的样本应该尽可能少，因此优化目标为

  - 式（6.29）：$min_ {w,b} \frac{1}{2} \vert \vert w \vert \vert ^2+C\sum_ {i=1}^m l_ {0/1}(y_ i(w^Tx_ i+b)-1)$ 

  - 其中$C >0$是一个常数$l_ {0/1}$是0/1损失函数

    $$ l_ {0/1}(z)=\left\{\begin{array}{cl}1, &if \  z<0; \\ 0, &otherwise  \end{array}\right. $$

  - 当C为无穷大时，6.29迫使所有样本均满足约束（6.28），于是式6.29等价于6.6

  - 当C取有限值时，6.29允许一些样本不满足约束

  - **替代损失（surrogate loss）**

    - 由于$l_ {0/1}$非凸、非连续，数学性质不好，使得式6.29不易直接求解

    - 需要用其他一些函数替代$l_ {0/1}$

    - 常用的替代损失函数

      - hinge损失：$l_ {hinge}(z)=max(0,1-z)$
      - 指数损失（exponential loss）：$l_ {exp}(z)=exp(-z)$
      - 对率损失（logistic loss）：$l_ {log}(z)=log(1+exp(-z))$
        - 对率损失函数通常表示为$l_ {log}(·)$而非$ln(·)$

      <img src="/assets/MLpics/T89.png" style="zoom:50%;" />

    - 若采用hinge损失，则6.29变成

      ​    $min_ {w,b} \frac{1}{2} \vert \vert w \vert \vert ^2+C\sum_ {i=1}^m max(0,1-y_ i(w^Tx_ i+b))$ 

      - $max(0,1-y_ i(w^Tx_ i+b))=\xi_ i$

      - 引入**松弛变量（slack variables）**$\xi_ i \ge 0$，可将上式重写

        - 当$1-y_ i(w^Tx_ i+b)>0$，$1-y_ i(w^Tx_ i+b)=\xi_ i$
        - 当$1-y_ i(w^Tx_ i+b) \le 0$，$\xi_ i=0$

      - 得到软间隔SVM（6.35）

        $min_ {w,b,\xi_ i} \frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m \xi_ i$

        $s.t. \ \ y_ i(w^Tx_ i+b) \ge 1- \xi_ i$

        $\xi_ i \ge 0,i=1,2,...,m$

      - 每一个样本对应松弛变量，表征该样本不满足约束6.28的程度

      - 仍是一个二次规划问题，通过拉格朗日乘子法

        $$\begin{aligned}L(w,b,\alpha,\xi,\mu) =&\frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m \xi_ i \\ &+\sum_ {i=1}^m \alpha_ i(1-\xi_ i-y_ i(w^Tx_ i+b))-\sum_ {i=1}^m \mu_ i \xi_ i \end{aligned}$$

        - $\alpha_ i \ge 0,\mu_ i \ge 0$是拉格朗日乘子

        - 令$L(w,b,\alpha,\xi,\mu)$对$w,b,\xi_ i$的偏导为0可得

          式6.37：$w=\sum_ {i=1}^m \alpha_ i y_ i x_ i$

          式6.38：$0=\sum_ {i=1}^m \alpha_ i y_ i$

          式6.39：$C=\alpha_ i+\mu_ i$

        - 将式6.37-6.39代入式6.36即可得到式6.35的对偶问题（6.40）

          $max_ {\alpha} \sum_ {i=1}^m \alpha_ i -\frac{1}{2}\sum_ {i=1}^m \sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j x_ i^T x_ j$

          $s.t. \ \ \sum_ {i=1}^m \alpha_ i y_ i=0$

          $0 \le \alpha_ i \le C,i=1,2,...,m$

        - 将上式与软间隔下的对偶问题6.11只有对偶变量的约束不同，所以采用同样的算法求解，因此KKT

          $$ \left\{\begin{array}{cl}\alpha_ i \ge 0, & \mu_ i \ge 0, \\ 1-\xi_ i-y_ if(x_ i) \le 0; \\ \alpha_ i(y_ i f(x_ i)-1+\xi_ i)=0 \\ \xi_ i \ge 0, &\mu_ i \xi_ i=0 \end{array}\right. $$

          - 对任意训练样本$(x_ i,y_ i)$,总有$\alpha_ i=0$或$y_ if(x_ i)=1-\xi_ i$
          - 若$\alpha_ i=0$，则该样本不会对$f(x)$有任何影响
          - 若$\alpha_ i>0$则必有$y_ i f(x_ i)=1-\xi_ i$，即该样本是支持向量
          - 由6.39可知，
            - 若$\alpha_ i<C$，则$\mu_ i > 0$，进而$\xi_ i = 0$，则该样本恰在最大间隔边界上
            - 若$\alpha_ i=C$，则$\mu_ i = 0$，此时
              - 若$\xi_ i \le 1$则该样本落在最大间隔内部
              - 若$\xi_ i > 1$则该样本被错误分类

      - 软间隔SVM的最终模型仅与支持向量有关

        - hinge函数有一处“平坦”的0区域，使得解具有稀疏性

    - 使用对率损失函数，几乎就得到了对率回归模型

      - 和对率回归的异同
        - SVM与对率回归的优化目标相近，一般性能相当
        - 对率回归的优势在于其输出具有自然的概率意义；SVM输出不具有概率意义，欲得到概率输出需进行特殊处理
        - 对率回归能直接用于多分类任务；SVM为此则需要进行推广
      - 光滑的单调递减函数，不能导出类似支持向量的概念，依赖于更多的训练样本，预测开销大

      

- **正则化（regularization）**

  - 可理解为对不希望得到的结果进行惩罚，从而使得优化过程趋向于希望目标
    - 从贝叶斯估计角度看，正则化项是提供了模型的先验概率
  - 损失函数的共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项用于表示训练集上的误差，可以写为（6.42）$min_ {f} \Omega(f)+C\sum_ {i=1}^m l(f(x_ i),y_ i)$
    - $\Omega(f)$称为**结构风险（structural risk）**，描述模型f的某些性质
    - $\sum_ {i=1}^m l(f(x_ i),y_ i)$称为**经验风险（empirical risk）**，描述模型与训练数据的契合程度
    - C用于对二者进行折中
    - 某种意义上6.42可以称为正则化问题
      - $\Omega(f)$称为**正则化项**
      - C称为**正则化常数**
        - $L_ p$范数是常用的正则化项
          - $L_ 2（\vert \vert w \vert \vert_ 2）$范数倾向于w的分量取值尽量均衡（非0分量个数尽量稠密
          - $L_ 0$范数$（\vert \vert w \vert \vert_ 0）$和$L_ 1（\vert \vert w \vert \vert_ 1）$$范数则倾向于w的分量尽量稀疏，即非0分量个数尽量少

## 支持向量回归

- 与传统模型不同，**支持向量回归（Support Vector Regression，简称SVR）**假设能容忍$f(x),y$之间最多有$\epsilon$的偏差

  - 如下图所示，以$f(x)$为中心，构建了一个宽度为$2\epsilon$的间隔带，若落入间隔带，则认为预测正确

  <img src="/assets/MLpics/T90.png" style="zoom:50%;" />

- SVR问题形式化（6.43）

  $min_ {w,b} \frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m l_ \epsilon(f(x_ i)-y_ i)$    

  - C为正则化常数

  - $l_ \epsilon$是**$\epsilon$-不敏感损失（$\epsilon$-insensitive loss）**函数

    $$l_ \epsilon(z)=\left\{\begin{array}{cl}0, & if \vert z \vert \le \epsilon; \\ \vert z \vert - \epsilon & otherwise.  \end{array}\right. $$

    <img src="/assets/MLpics/T91.png" style="zoom:50%;" />

  - 引入松弛变量$\xi_ i，\hat{\xi_ i}$，可以将6.43重写为

    $min_ {w,b,\xi_ i,\hat{\xi_ i}} \frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m (\xi_ i+\hat{\xi_ i})$

    $s.t. \ \ f(x_ i)-y_ i \le \epsilon+\xi_ i$

    $y_ i-f(x_ i) \le \epsilon+\hat{\xi_ i}$

    $\xi_ i \ge 0,\hat{\xi_ i} \ge 0,i=1,2,...,m$    

  - 引入拉格朗日乘子$\mu_ i \ge 0,\hat{\mu_ i} \ge 9,\alpha_ i \ge 0,\hat{\alpha_ i} \ge 0$，得到式（6.46）

    $L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}}, \boldsymbol{\mu}, \hat{\boldsymbol{\mu}})$
  
    $=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_ {i=1}^{m}\left(\xi_ {i}+\hat{\xi}_ {i}\right)-\sum_ {i=1}^{m} \mu_ {i} \xi_ {i}-\sum_ {i=1}^{m} \hat{\mu}_ {i} \hat{\xi}_ {i}$
  $+\sum_ {i=1}^{m} \alpha_ {i}\left(f\left(\boldsymbol{x}_ {i}\right)-y_ {i}-\epsilon-\xi_ {i}\right)+\sum_ {i=1}^{m} \hat{\alpha}_ {i}\left(y_ {i}-f\left(\boldsymbol{x}_ {i}\right)-\epsilon-\hat{\xi}_ {i}\right)$
  
- 将式$f(x)=w^Tx+b$代入，对w，b，$\xi_ i，\hat{\xi_ i}$偏导为0可得
  
  $$\begin{aligned} \boldsymbol{w} &=\sum_ {i=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right) \boldsymbol{x}_ {i} \\ 0 &=\sum_ {i=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right) \\ C &=\alpha_ {i}+\mu_ {i} \\ C &=\hat{\alpha}_ {i}+\hat{\mu}_ {i} \end{aligned}$$
  
- 上面几式代入6.46，得到SVR的对偶问题（6.51）
  
  $$\begin{aligned} \max _ {\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}} & \sum_ {i=1}^{m} y_ {i}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right)-\epsilon\left(\hat{\alpha}_ {i}+\alpha_ {i}\right) \\ &-\frac{1}{2} \sum_ {i=1}^{m} \sum_ {j=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right)\left(\hat{\alpha}_ {j}-\alpha_ {j}\right) \boldsymbol{x}_ {i}^{\mathrm{T}} \boldsymbol{x}_ {j} \\ \text { s.t. } & \sum_ {i=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right)=0 \\ & 0 \leqslant \alpha_ {i}, \hat{\alpha}_ {i} \leqslant C \end{aligned}$$
  
- 上述过程需要满足KKT条件
  
    $$\left\{\begin{array}{l}
    \alpha_ {i}\left(f\left(\boldsymbol{x}_ {i}\right)-y_ {i}-\epsilon-\xi_ {i}\right)=0 \\
    \hat{\alpha}_ {i}\left(y_ {i}-f\left(\boldsymbol{x}_ {i}\right)-\epsilon-\hat{\xi}_ {i}\right)=0 \\
    \alpha_ {i} \hat{\alpha}_ {i}=0, \xi_ {i} \hat{\xi}_ {i}=0 \\
    \left(C-\alpha_ {i}\right) \xi_ {i}=0,\\ \left(C-\hat{\alpha}_ {i}\right) \hat{\xi}_ {i}=0
  \end{array}\right.$$
  
    - 当且仅当$f(\boldsymbol{x}_ {i})-y_ {i}-\epsilon-\xi_ {i}=0$（不落入间隔带·中）时$\alpha_ i$和$\hat{\alpha_ i}$能取非0值
  - $f(\boldsymbol{x}_ {i})-y_ {i}-\epsilon-\xi_ {i}=0$和$f(\boldsymbol{x}_ {i})-y_ {i}-\epsilon-\hat{\xi_ {i}=0}$不能同时成立，$\alpha_ i,\hat{\alpha_ i}$至少有一个为0
  
- 将$w =\sum_ {i=1}^{m}(\hat{\alpha}_ {i}-\alpha_ {i}) x_ {i}$代入$f(x)=w^Tx+b$，则SVR的解形如
  
  $f(x)=\sum_ {i=1}^m(\hat{\alpha_ i}-\alpha_ i)x_ i^Tx+b$
  
  - 能使上式中的$(\hat{\alpha_ i}-\alpha_ i) \not=0$的样本即为SVR的支持向量，必落在间隔带之外
  
  - 间隔带中的样本均满足$\alpha_ i$和$\hat{\alpha_ i}$均为0
  
    - 其解仍然具有稀疏性
  
  - 由KKT条件，若$0 < \alpha_ i <C$必有$\xi_ i=0$，进而
  
    $b=y_ i+\epsilon-\sum_ {i=1}^m(\hat{\alpha_ i}-\alpha_ i)x_ i^Tx$
  
      - 在求解6.51得到$\alpha_ i$后，理论上可以任意选取满足$0 < \alpha_ i <C$的样本求得b
    - 只见中采用更鲁棒的方法，选取多个满足条件的样本求b后求平均值
  
- 若考虑特征映射形式（6.19），则
  
  $w =\sum_ {i=1}^{m}(\hat{\alpha}_ {i}-\alpha_ {i}) \phi(x_ {i})$
  
  - SVR可以表示为
  
    $f(x)=\sum_ {i=1}^{m}(\hat{\alpha}_ {i}-\alpha_ {i}) \kappa(x,x_ i)+b$
  
      $\kappa(x,x_ i)=\phi(x_ i)^T\phi(x_ j)$为核函数

## 核方法

- **表示定理（representer theorem）**

  - 令$\Bbb{H}$为核函数$\kappa$对应的再生核希尔伯特空间，$\vert \vert h \vert \vert_ {\Bbb{H}}$表示$\Bbb{H}$空间中关于h的范数，对于任意单调递增函数$ \Omega : [0, \infty ] \mapsto \Bbb{R}$和任意非负损失函数$l:\Bbb{R}^m \mapsto  [0, \infty ]$，优化问题（式6.57）

    $\min _ {h \in \mathbb{H}} F(h)=\Omega\left(\|h\|_ {\mathbb{H}}\right)+\ell\left(h\left(\boldsymbol{x}_ {1}\right), h\left(\boldsymbol{x}_ {2}\right), \ldots, h\left(\boldsymbol{x}_ {m}\right)\right)$

    的解可总写为

    $h^{*}(\boldsymbol{x})=\sum_ {i=1}^{m} \alpha_ {i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_ {i}\right)$

  - 表示定理对$\Omega$要求单调递增，不需要是凸函数

    - 对于一般的损失函数和正则化项，优化问题6.57的最优解$h^*(x)$都可以表示为核函数$\kappa(x,x_ i)$的线性组合

- **核方法（kernel methods）**

  - 基于核函数的学习方法

- **线性判别分析（Kernelized Linear Discriminant Analysis，简称KLDA）**

  - 通过**核化**（引入核函数）来为线性学习器拓展为非线性

  - 过程

    - 假设通过$\phi:\chi \mapsto \Bbb{F}$将样本映射到一个特征空间$\Bbb{F}$，执行线性判别分析，得到

      $h(x)=w^T \phi(x)$

    - KLDA的学习目标

      $max_ w J(w)=\frac{w^T S_ b^ \phi w}{w^T S_ w^ \phi w}$

      - $S_ b ^\phi,S_ w^\phi$分别为训练样本在特征空间中的类间散度矩阵和类内散度矩阵

    - $X_ i$表示第$i \in \{0,1\}$在类样本的集合，其样本数为$m_ i$，总样本数为$m=m_ 0+m_ 1$，第i类样本在特征空间的均值为

      $\mu_ i ^ \phi=\frac{1}{m_ i}\sum_ {x \in X_ i} \phi(x)$

    - 两个散度矩阵分别为

      $\mathbf{S}_ {b}^{\phi} =\left(\boldsymbol{\mu}_ {1}^{\phi}-\boldsymbol{\mu}_ {0}^{\phi}\right)\left(\boldsymbol{\mu}_ {1}^{\phi}-\boldsymbol{\mu}_ {0}^{\phi}\right)^{\mathrm{T}}  $

      $\mathbf{S}_ {w}^{\phi} =\sum_ {i=0}^{1} \sum_ {\boldsymbol{x} \in X_ {i}}\left(\phi(\boldsymbol{x})-\boldsymbol{\mu}_ {i}^{\phi}\right)\left(\phi(\boldsymbol{x})-\boldsymbol{\mu}_ {i}^{\phi}\right)^{\mathrm{T}} $

    - 一般难以知道映射的具体形式，使用核函数来隐式表达映射和特征空间

    - $J(w)$作为6.57的损失函数可得到式（6.64）

      $h(\boldsymbol{x})=\sum_ {i=1}^{m} \alpha_ {i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_ {i}\right)$

      $w=\sum_ {i=1}^m \alpha_ i \phi(x_ i)$

    - 令$K \in \Bbb{R}^{m \times m}$为核函数对应的核矩阵，$(K)_ {ij}=\kappa(x_ i,x_ j)$

    - $1_ i \in \{1,0\}^{m \times 1}$为第i类样本的指示向量

      - $1_ i$的第j个分量为1当且仅当$x_ j \in X_ i$，否则为0

        $\hat{\mu_ 0}=\frac{1}{m_ 0}K1_ 0$

        $\hat{\mu_ 1}=\frac{1}{m_ 1}K1_ 1$

        $M=(\hat{\mu_ 0}-\hat{\mu_ 1})(\hat{\mu_ 0}-\hat{\mu_ 1})^T$

        $N=KK^T-\sum_ {i=0}^1 m_ i\hat{\mu_ i}\hat{\mu_ i}^T$

      - 于是KLDA的学习目标等价为

        $max_ \alpha J(\alpha)=\frac{\alpha^T M \alpha}{\alpha^T N \alpha}$

      - 求得$\alpha$之后可以由6.64得到投影函数$h(x)$

## 习题

- 6.1：试证明样本空间中任意点x到超平面（w,b）的距离为$r=\frac{\vert w^Tx+b \vert}{\vert \vert w \vert \vert}$

  <img src="/assets/MLpics/T92.png" style="zoom:50%;" />

  - 如上图所示，$x=x_0+r$

    $r=r_0 \frac{w}{\vert \vert w \vert \vert}$

    $x=x_0+r_0 \frac{w}{\vert \vert w \vert \vert}$

    $w^Tx_0+b=0，x_0=x-r_0 \frac{w}{\vert \vert w \vert \vert}$

    $w^Tx-w^Tr_0 \frac{w}{\vert \vert w \vert \vert}+b=0$

    $r_0=\frac{\vert w^Tx+b \vert}{\vert \vert w \vert \vert}$

- 6.2：试使用LIBSVM，在西瓜数据集3.0上分别用线性核和高斯核训练一个SVM，并比较其支持向量的差别

  ```python
  def SVM():
      X,y=ReadExcel('./alpha_data.xlsx')
      X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)
      y_train=y_train.T.tolist()[0]
      y_test=y_test.T.tolist()[0]
      prob=svm_problem(y_train,X_train)
      # t=2高斯核，t=0线性核
      param=svm_parameter('-t 2 -c 35')
      model=svm_train(prob,param)
      p_label,p_acc,p_val=svm_predict(y_test,X_test,model)
      print(p_acc)
      return p_label
  ```

  - 高斯核和线性核训练出来的支持向量一致，同时线性核以及高斯核训练的SVM准确率最高均在0.67。

- 6.3 选择两个UCI数据集，分别用线性核核高斯核训练一个SVM，并与BP神经网络和C4.5决策树进行实验比较

  ```python
  # SVC
  #线性核，核函数系数，软间隔分类器
  svc=SVC(kernel='linear',gamma="auto",C=1)
  svc.fit(X_train,y_train)
  print("The accuracy of svc on training set under linear",format(svc.score(X_train,y_train)))
  print("The accuracy of svc on test set under linear",format(svc.score(X_test,y_test)))
  
  #高斯核
  svc=SVC(kernel='rbf',gamma="auto",C=1)
  svc.fit(X_train,y_train)
  print("The accuracy of svc on training set under rbf",format(svc.score(X_train,y_train)))
  print("The accuracy of svc on test set under rbf",format(svc.score(X_test,y_test)))
  
  # BP神经网络
  mlp=MLPClassifier(random_state=0,max_iter=1000,alpha=1).fit(X_train,y_train)
  print("The accuracy of BP neural network on training set",format(mlp.score(X_train,y_train)))
  print("The accuracy of BP neural network on test set",format(mlp.score(X_test,y_test)))
  
  # C4.5决策树
  tree=DecisionTreeClassifier(random_state=0,max_depth=4).fit(X_train,y_train)
  print("The accuracy of C4.5 tree on training set",format(tree.score(X_train,y_train)))
  print("The accuracy of C4.5 tree on test set",format(tree.score(X_test,y_test)))
  ```

  结果

  ```python
  The accuracy of svc on training set under linear 0.9910714285714286
  The accuracy of svc on test set under linear 1.0
  The accuracy of svc on training set under rbf 0.9642857142857143
  The accuracy of svc on test set under rbf 1.0
  
  The accuracy of BP neural network on training set 0.9732142857142857
  The accuracy of BP neural network on test set 1.0
  
  The accuracy of C4.5 tree on training set 1.0
  The accuracy of C4.5 tree on test set 0.9736842105263158
  ```

  - 对比，在训练集上准确率相差不大，而在测试集上决策树表现相对不算优异

- 6.4：试讨论线性判别分析与线性核支持向量机在何种条件下等价

  - 线性判别分析能够解决 n 分类问题
  -  而 SVM只能解决二分类问题，如果要解决 n 分类问题要通过 OvR来迂回解决.
  - 线性判别分析能将数据以同类样例间低方差和不同样例中心之间大间隔来投射到一条直线上, 但是如果样本线性不可分, 那么线性判别分析就不能有效进行, 支持向量机也是
  - 综上, 等价的条件是
    - 数据有且仅有 2 种, 也就是说问题是二分类问题
    - 数据是线性可分的

- 6.5：试述高斯核SVM与RBF神经网络之间的联系

  - 若将隐层神经元数设置为训练样本数，且每个训练样本对应一个神经元中心，则以高斯径向基函数为激活函数的RBF网络恰与高斯核SVM的预测函数相同。
  - RBF的径向基函数和高斯核SVM均采用高斯核
  - 神经网络是最小化累计误差，将参数 w 作为惩罚项；而SVM相反，主要是最小化参数，将误差作为惩罚项。

- 6.6：试析SVM对噪声敏感的原因

  - SVM的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束（函数间隔要大于1）
  - 若数据集中存在噪声点并且满足线性可分，那么SVM为了将噪声点划分为正确，远离该点的类靠近，使得划分超平面的几何间距变小，泛化性能差
  - 若数据集由于噪声点无法满足线性可分，就需要使用核方法，会得到更加复杂的模型，泛化能力差（过拟合）
  - 因此需要软间隔SVM来解决这些问题

- 6.7：试给出式（6.52）的完整KKT条件

  <img src="/assets/MLpics/T93.png" style="zoom:50%;" />

- 6.8：以西瓜数据集3.0的“密度”为输入，“含糖率”为输出，试使用LIBSVM训练一个SVR\

  ```python
  def SVR():
      X,y=ReadExcel('./alpha_data.xlsx')
      X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)
      y_train=y_train.tolist()
      # t=2高斯核，t=0线性核
      # s=3 epsilon-SVR
      model=svm_train(y_train,X_train,'-t 0 -s 3')
      p_label,p_acc,p_val=svm_predict(y_test,X_test,model)
      print(p_acc)
  ```

  结果

  ```python
  Mean squared error = 0.00833156 (regression)
  Squared correlation coefficient = 0.00832679 (regression)
  (0.0, 0.008331563045225086, 0.008326794756839036)
  ```

  

- 6.9：试使用核技巧推广对率回归，产生“核对率回归”

  <img src="/assets/MLpics/T94.png" style="zoom:50%;" />

  

  
