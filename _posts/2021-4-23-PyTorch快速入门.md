---
title: PyTorch快速入门
status: Completed
commentable: false
Edit: 2021-4-23
mathjax: true
mermaid: true
tags: Computer_Vision 
categories: Computer_Vision
Typora-root-url: ../
description: PyTorch快速入门笔记
---

读前提示：


- 之前在做项目和论文复现的时候接触过pytorch，但已经忘了很多了，来重温一下
- 主要学习来源
  - [PyTorch Tutorials](https://pytorch.org/tutorials/)
  - [pytorch-beginner](https://github.com/L1aoXingyu/pytorch-beginner)

# PyTroch介绍

- 基于python的科学计算库
  - 通过使用GPU和其他计算器算力取代numpy
  - 自动实现神经网络的库

# Quickstart

Quickstart中阐述的内容在后文不再解释

## data

- torch.utils.data

  ```python
  from torch.utils.data import DataLoader#数据读取接口
  from torch.utils.data import Dataset#保存pytorch样本以及对应标签
  ```

  - DataLoader

    ```python
    train_dataloader=DataLoader(training_data,batch_size=batch_size)#准备训练数据，还有一个参数shuffle=True代表随机打乱数据集
    ```

- 提供数据集

  - TorchText

  - [TorchVision](https://pytorch.org/vision/stable/datasets.html)

    ```python
    from torchvision import datasets#使用python自带TorchVision数据集，包括常用的COCO等
    ```

  - TorchAudio

## creating models

- 获取GPU或CPU资源

  ```python
  device = "cuda" if torch.cuda.is_available() else "cpu"
  ```

- 定义layers

  ```python
  from torch import nn#引入nn模块用于建立训练神经网络
  ```

  ```python
  #神经网络类
  class NeuralNetwork(nn.Module):
    #在init下定义layers
    def __init__(self):
      #继承父类属性初始化
      super(NeuralNetwork, self).__init__()
      #将连续范围内的维度展平为张量(tensor)
      self.flatten=nn.Flatten()
      #Sequential容器,模块按照顺序添加到模块中
      self.linear_relu_stack=nn.Sequential( 
      	nn.Linear(28*28,512),#线性转换层 y=Ax+b(y=xA^T+b)
        nn.ReLU(),#RELU层，激活函数
        ...
      )
    #前向传播函数，对输入进行处理
    def forward(self,x):   
      x=self.flatten(x)
      logits=self.linear_relu_stack(x)#调用init中声明的container
    return logits
  
  ```

#调用神经网络生成模型
  model=NeuralNetwork().to(device)
  print(model)

- flatten详解

  ```python
  >>>t = torch.tensor([[[1, 2, 2, 1],
                            [3, 4, 4, 3],
                            [1, 2, 3, 4]],
                           
                            [[5, 6, 6, 5],
                             [7, 8, 8, 7],
                             [5, 6, 7, 8]]])
      
  #end_dim默认值为-1(start_dim为0)也就是说把1到最后一个维度全部推平
  >>>x = torch.flatten(t, start_dim=1)
  tensor([[1, 2, 2, 1, 3, 4, 4, 3, 1, 2, 3, 4],
  				[5, 6, 6, 5, 7, 8, 8, 7, 5, 6, 7, 8]])
      
  #把0到1维度全部推平
  >>>y=torch.flatten(t,start_dim=0,end_dim=1)
  tensor([[1, 2, 2, 1],
          [3, 4, 4, 3],
          [1, 2, 3, 4],
          [5, 6, 6, 5],
          [7, 8, 8, 7],
          [5, 6, 7, 8]])
  ```

## Optimizing the Model Parameters

定义loss、优化器、训练、测试函数，再根据epoch值进行循环训练，得到结果

## Loss and Optimizer

```python
#定义loss和优化器
loss_fn = nn.CrossEntropyLoss()#使用交叉熵作为损失函数（-log(softmax)）	
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#SGD（随机梯度下降）作为optimzer
```

## Training

```python
def train(dataloader, model, loss_fn, optimizer):
  size = len(dataloader.dataset)
  for batch, (X, y) in enumerate(dataloader):
    X, y = X.to(device), y.to(device)#读取数据copy到device上去

    pred = model(X)#计算loss，用于后续反向传播
    loss = loss_fn(pred, y)

    optimizer.zero_grad()#清空过去梯度，避免多次运算
    loss.backward()#对loss做反向传播，计算梯度，如果使用多次反向传播，请在除了最后一个backward以外都加入 retain_graph=True 参数，保留前一次backward的梯度值
    optimizer.step()#梯度下降，更新参数
    
    #batch每是100的倍数就进行loss的输出
    if batch % 100 == 0:
      loss, current = loss.item(), batch * len(X)
      print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
```

## Testing

```python
def test(dataloader,model):
  size=len(dataloader.dataset)
  #关掉Dropouts，BatchNorm层
  model.eval()
  test_loss,correct=0,0
  #关闭自动求导引擎，能节省显存并加速
  with torch.no_grad():
    for X,y in dataloader:
      X,y=X.to(device),y.to(device)
      pred = model(X)
      #体现模型性能（通过loss平均值和accuracy）
      test_loss+=loss_fn(pred,y).item()
      correct+=(pred.argmax(1)==y).type(torch.float).sum().item()
    
    test_loss/=size
    correct/=size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

# Tensors

类似于数组和矩阵的特殊数据结构，在PyTorch中用于编码输入输出和模型参数

## 数据格式转换

```python
#数据生成tensor
data=[[1,2],[3,4]]
x_data=torch.tensor(data)

#np转tnesor，此时对np进行操作t的值也会改变
np = np.array(data)                           
t = torch.from_numpy(np)

#Tensor到numpy，此时对t进行操作n的值也会改变
t = torch.ones(5)
n = t.numpy()
```

## Initializing

```python
#全1 tensor
x_ones = torch.ones_like(x_data)
#随机 tensor
x_rand = torch.rand_like(x_data, dtype=torch.float)
shape = (2,3,)
rand_tensor = torch.rand(shape)
#torch.ones(shape)、torch.zeros(shape)
```

## Attributes

- shape：大小
- dtype：数据类型
- device：存储地址

## Operations

- torch.cat 拼接tensor

  ```python
  tensor([[1., 0., 1., 1.],
          [1., 0., 1., 1.],
          [1., 0., 1., 1.],
          [1., 0., 1., 1.]])
  
  >>>t1 = torch.cat([tensor, tensor, tensor], dim=1)
  
  tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
          [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
          [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
          [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])
  ```

- 众多操作如Arithmetic、indexing类似于numpy，建议参考[官方文档](https://pytorch.org/docs/stable/tensors.html)

# Datasets and DataLoaders

部分见QuickStart

## Iterating and Visualizing the Dataset

```python
img, label = training_data[index]										#定位到dataset对应index的数据和label
```

## Creating a Custom Dataset for your files

```python
#创造自己的数据集
from torchvision.io import read_iamge
class CustomImageDataset(Dataset):
  #__init__初始数据集object
  def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
    self.img_labels = pd.read_csv(annotations_file)
    self.img_dir = img_dir
    self.transform = transform
    self.target_transform = target_transform
  def __len__(self):
    return len(self.img_labels)
  
  #通过index返回数据sample
  def __getitem__(self, idx):
    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
    image = read_image(img_path)#读取index对应数据和label
    label = self.img_labels.iloc[idx, 1]
    #对数据做变换处理
    if self.transform:
      image = self.transform(image)
    #对标签做变换处理
    if self.target_transform:
      label = self.target_transform(label)
		sample = {"image": image, "label": label}
    return sample
```

# Iterate through the DataLoader

```python
train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
#iter()返回迭代器，使用next访问
train_features, train_labels = next(iter(train_dataloader))

#以下两种访问形式等同，注意images和labels的顺序
for images, labels in iter(train_dataloader):
for labels, images in enumerate(train_dataloader):
```

# Transforms

## ToTensor

```python
from torchvision.transforms import ToTensor 
#将PIL或者ndarray转换为FloatTensor
#例如打开的图片img是w,h,c形状的三维numpy数组, ToTensor转换成形状为：c,h,w形状的三维tensor
#且归一化到[0,1.0]之间
```

# Build Model

- 见QuickStart Creating Models部分

- 模型使用案例

  ```python
  X = torch.rand(1, 28, 28, device=device)#准备数据集
  logits = model(X)#将X传入模型得到logits（未经softmax向量）
  pred_probab = nn.Softmax(dim=1)(logits)#使用softmax，映射到(0,1)区间
  y_pred = pred_probab.argmax(1)#得出预测结果
  print(f"Predicted class: {y_pred}")
  ```

# Automatic Differentiation

- 用于计算gradients（梯度）
- 自动化计算任何计算图的梯度（计算图请见CNN笔记中反向传播部分）

## Tensors, Functions and Computational graph

- requires_grad：说明当前量是否需要在计算中保留对应的梯度信息
  - 在创造tensor时加入requires_grad
  - 或者使用x.requires_grad_(True)
- grad_fn：反向传播function
  - z.grad_fn
  - loss.grad_fn

## Computing Gradients

```python
loss.backward()#进行反向传播
print(w.grad)#也可以计算b.grad
```

## Disabling Gradient Tracking

- 理由：某些情况并不需要追踪历史记录或支持梯度运算
  - 用于标记某些参数，常用于基于预训练模型上的训练过程
  - 只需要前向传播，加速运算

- 方法（注意输出的z.requires_grad=False）

  - 使用torch.no_grad()

  ```python
  with torch.no_grad():#关闭自动求导引擎，能节省显存并加速
  z=torch.matmul(x,w)+b
  ```

  - 使用detach()

  ```python
  z = torch.matmul(x, w)+b
  z_det = z.detach()
  ```

# Optimization Loop

过程已在QuickSort中进行说明，不再赘述

## Hyperparameters

用于控制模型优化进程

- Learing Rate：学习率，每个batch/epoch模型参数的变化量，过小会导致训练过慢，过大易导致陷入局部最小
- Batch Size：每个epoch模型训练的数据样本数
- Epochs：训练循环次数

## Optimization Loop

- Train Loop：训练循环
- Validation/Test Loop：验证循环

## Loss Function

- nn.MSELoss：MSE均方误差
- nn.NLLLoss：负对数似然，常用于多分类任务，先softmax，再-log，再做NLLLoss
- nn.CrossEntropyLoss：交叉熵（和sigmoid/softmax一起出现），本质上就是把Softmax–Log–NLLLoss合成为一步

# Save, Load and Use Model

## Saving and Loading Model Weights

```python
model = models.vgg16(pretrained=True)#根据权重读取VGG16预训练模型
torch.save(model.state_dict(), 'model_weights.pth')#以pth文件保存模型
```

```python
model = models.vgg16()#使用VGG16模型结构，但不读取预训练参数
model.load_state_dict(torch.load('model_weights.pth'))#使用自定义参数
```

## Saving and Loading Model Shapes

```python
torch.save(model, 'model.pth')#同时保存模型结构和参数
model = torch.load('model.pth')
```