---
title: 机器学习笔记
status: Writing
commentable: false
Edit: 2020-9-28
mathjax: true
mermaid: true
tags: Machine_Learning
categories: Machine_Learning
Typora-root-url: ..
description: 本文为周志华老师《机器学习》（西瓜书）的学习笔记，仅供参考，如有错误，欢迎给我发邮件或者通过友链进行留言。（详情请见About）
---

# 绪论

## 基本术语

- 数据

  - 数据集：记录的集合

  - 示例：数据集中每条记录--关于一个事件/对象的描述

  - 属性/特征：反映事件或对象在某方面的表现或性质的事项

  - 属性/样本/输入空间：属性张成的空间

  - 特征向量：属性空间中的每一个点对应一个坐标向量---一个示例

    公式说明$$ D=\left\{ x _ {1},x _ {2},...,x _ {m} \right\} $$

    其中D代表数据集，xi代表示例$$x _ {i}=(x _ {i1};x _ {i2};...x _ {in})$$

- 学习/训练：从数据中学得模型的过程

  - 训练数据

  - 训练样本

  - 训练集

  - 假设：学得模型对应了关于数据的某种潜在的规律

  - 真相：潜在规律

  - 标签：一个示例的具体结果说明

  - 标记：结果信息（当然也存在标记空间）

  - 样例：拥有了标记信息的示例

    样例：$(x _ {i},y _ {i})$ $y _ {i}$是标记

  - 预测（监督学习）

    - 分类：预测的是离散值
      - 二分类(样本空间为$\{-1,+1\}$或$\{0,1\}$)
      - 多分类（样本空间的模>2）
    - 回归：预测的是连续值

  - 测试：学得模型后进行预测的过程

    - 测试样本：被预测的样本

  - 聚类（非监督学习）：有助于帮助我们发掘数据内在规律

  - 泛化：从样本很小空间得到的模型可以很好的适用于整个样本空间

    - 归纳：特殊到一般的泛化
    - 演绎：一般到特殊的特化

- 假设空间

  - 归纳学习

    - 狭义：从训练数据中学得概念，亦称概念学习/概念形成

      - 布尔概念学习：0/1

      案例：假设判断是否是好瓜有三种属性：

      色泽（3）	根蒂（2）	敲声（2）

      假设空间规模大小就是4x3x3+1=37

      （之所以为4x3x3是要考虑可能啥都行的情况，+1是因为要考虑可能根本没有好瓜的情况）

    - 广义：从样例中学习

  - 版本空间：可能有多个假设和训练集一致，这个假设集合就被称为这个训练集对应的版本空间

- 归纳偏好：在面临多个假设的时候无法判断哪一个更好，这时算法偏好就会起到作用

  - 引导算法

    - **奥卡姆剃刀**（Occam's razor）：若有多个假设与观察一致，则选最简单的那个（but简单的诠释也往往不同）

  - $\mathfrak{L} _ {a}$代表一种学习算法，对于不同的学习算法$\mathfrak{L} _ {a}$公和$\mathfrak{L} _ {b}$它们在不同的情况下会展现出不同的优势和缺陷

  - $\mathfrak{L} _ {a}$在训练集之外的所有样本上的误差为：$$E _ {\text {ote}}\left(\mathfrak{L} _ {a} \mid X, f\right)=\sum _ {h} \sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h \mid X, \mathfrak{L} _ {a}\right)$$	（1.1）

    - $\mathbb{I}(·)$是指示函数，内部为真取1，否则取0
    - X为训练数据
    - $\mathcal{X}$为样本空间（离散）
    - $\mathcal{H}$为假设空间（离散）
    - $P\left(h \mid X, \mathfrak{L} _ {a}\right)$代表算法基于训练数据X产生假设h的概率
    - $f$我们希望学习的真实目标函数
    - 个人公式理解：
      - $P(x)$表示在训练数据以外（样本空间之内）该x出现的概率。
    - 在某一种假设和训练数据条件下，$\mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h \mid X, \mathfrak{L} _ {a}\right)$为基于该训练数据关于x的预测结果和实际结果相同的概率
  
  - 考虑到二分类问题，且真实目标函数可以是任何函数，对所有可能的f按均匀分布对误差求和 
        $$\begin{aligned}
    \sum _ {f} E _ {\text {ote}}\left(\mathfrak{L} _ {a} \mid X, f\right) &=\sum _ {f} \sum _ {h} \sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h \mid X, \mathfrak{L} _ {a}\right)  \\ 
    &=\sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum _ {h} P\left(h \mid X, \mathfrak{L} _ {a}\right) \sum _ {f} \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) \qquad   \qquad   \qquad          (1.2) \\ 
    &=\sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum _ {h} P\left(h \mid X, \mathfrak{L} _ {a}\right) \frac{1}{2} 2^{ \vert \mathcal{X} \vert }  \\ 
    &=\frac{1}{2} 2^{ \vert \mathcal{X} \vert } \sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum _ {h} P\left(h \mid X, \mathfrak{L} _ {a}\right)
  \end{aligned}$$
  
  南瓜书具体说明：
  
    - 第1步到第2步
      - $\sum _ {i}^{m} \sum _ {j}^{n} \sum _ {k}^{o} a _ {i} b _ {j} c _ {k}=\sum _ {i}^{m} a _ {i} \cdot \sum _ {j}^{n} b _ {j} \cdot \sum _ {k}^{o} c _ {k}$
    - 第2步到第3步
      - 对于f的假设是任何能将样本映射到0，1的函数并且服从均匀分布。不止一个f且f出现的概率相等
      - 举个栗子：样本空间只有两个样本时($ \vert \mathcal{X} \vert =2$)，f的可能性$2^{ \vert \mathcal{X} \vert }$有四种，比如：$f _ {1}(x _ {1})=0,f _ {1}(x _ {2})=0;$
      - 所以通过$\mathfrak{L}  _ {a}$学习出的模型h(x)对 *每个样本* 无论是预测值为0还是1必然有一半的f与其预测值相等，例如$h(x _ {1})=1$则必有两个$f _ {n}(x _ {1})=1$
      - 所以说$\sum _ {f} \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x}))=\frac{1}{2}2^{ \vert \mathcal{X} \vert }$
  - 最终结果：总误差和学习算法无关（NFL**“没有免费的午餐”** 定理） 
  
- NFL定理的bug前提：所有问题出现的机会相同/所有问题同等重要，但现实并非如此。
  
    - 一般来说我们只需要在某个具体的应用任务上找到一个解决方案
    - 瓜瓜🍉栗子：我们对好瓜会有一种评判标准，比如卖瓜的时候会喜欢买色泽青绿，根蒂蜷缩，敲声浊响的瓜，那么这种好瓜会更为常见，而根蒂硬挺，敲声清脆的好瓜罕见甚至不存在

## 部分习题解释

- 表格

  ![T1](/assets/MLpics/T1.png)

- 1.1	版本空间

  - 好瓜{色泽=青绿，根蒂=蜷缩，敲声=浊响}	坏瓜{色泽=乌黑，根蒂=稍蜷，敲声=沉闷}
  - 好瓜{色泽=青绿，根蒂=\*，敲声=\*}	坏瓜{色泽=乌黑，根蒂=\*，敲声=\*}
  - 好瓜{色泽=\*，根蒂=蜷缩，敲声=\*}	坏瓜{色泽=\*，根蒂=稍蜷，敲声=\*}
  - 好瓜{色泽=\*，根蒂=\*，敲声=浊响}	坏瓜{色泽=\*，根蒂=\*，敲声=沉闷}
  - 好瓜{色泽=\*，根蒂=蜷缩，敲声=浊响}	坏瓜{色泽=\*，根蒂=稍蜷，敲声=沉闷}
  - 好瓜{色泽=青绿，根蒂=\*，敲声=浊响}	坏瓜{色泽=乌黑，根蒂=\*，敲声=沉闷}
  - 好瓜{色泽=青绿，根蒂=蜷缩，敲声=\*}	坏瓜{色泽=乌黑，根蒂=稍蜷，敲声=\*}

- 1.2

  - 包含3种属性，假设空间大小为3x4x4+1=49
  - 考虑冗余（A=a与A=*等价
    - 具体的假设：2x3x3=18
    - 1属性泛化：2x3+3x3+2x3=21
    - 2属性泛化：2+3+3=8
    - 3属性泛化：1
    - 不考虑冗余/空集：kmax=48
    - 考虑冗余：kmax=18

- 1.3

  - 归纳偏好选择：一致性比例越高越好
    - 两个数据属性越接近，则分为同一类
    - 若相同属性出现了两种不同的分类
      - 则认为它属于与他最邻近的几个数据的属性
      - 同时去掉所有具有相同属性而不同分类的数据，可能会丢失部分信息

# 模型评估与选择

## 经验误差与过拟合

- 概念
  - 错误率：分类错误的样本数占样本总数的比例
  - 精度：1-错误率
  - 误差：学习器的实际预测输出与样本的真实输出之间的差异
    - **训练/经验误差**：学习器的实际预测输出与样本的真实输出
    - **泛化误差**：在新样本上的误差
      - 如何获得更好的泛化误差：尽可能在训练样本中学出适用于所有潜在样本的“普遍规律”
      - 过拟合：把训练样本学习“太好”，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质。原因：学习能力过于强大。无法避免，只能缓解。
      - 欠拟合：对训练样本的一般性质尚未学好。原因：学习能力低下。方法：决策树中扩展分支，神经网络中增加训练轮数等。

## 评估方法

- 测试集：测试学习器对新样本的判别能力
  - 概念
    - **测试误差**：泛化误差的近似
    - 尽可能与训练集互斥（测试样本尽量不在训练集中出现）
  - 常见得到测试集做法
    - **留出法**：将数据集D划分为两个互斥的集合，一个集合作为训练集S，另一个作为测试集T
      - 训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程中引入额外的偏差而对最终结果产生影响
      - 分层采样：保留类别比例的采样方式
      - 在给定样本比例后，仍然存在多种划分方式对数据集进行分割，比如是前350个放入训练集还是后350个
      - 所以使用留出法时，一般要采用若干次随机划分，重复进行实验评估，最后取平均值
    - **交叉验证法**（k折交叉验证）
      - 将数据集D划分为k个大小相似的互斥子集，也存在多种划分方式，一般要随机使用不同的划分重复p次，最终评估结果是这p次k折交叉验证结果的均值
      - 每次用k-1个子集的并集作为训练集，余下的子集作为测试集
      - 可以进行k次训练和测试，最后返回这k个测试结果的均值
      - k最常用的取值是10/5/20
      - 特例：**留一法**，D中包含m个样本，k=m
        - m个样本只有唯一的方式划分为m个子集
        - 优点：与用D训练出来的模型很相似，往往被认为较为准确
        - 缺点：
          - 在数据集比较大时，训练m个模型的计算开销可能是难以忍受的
          - 没有免费的午餐仍然适用
      - **自助法**：为了减少训练样本规模不同造成的影响
        - 以自助采样法为基础
        - 每次从D中随机挑选一个样本，将其拷贝放入D‘，然后再将该样本放回D，使得下一次采样时仍有可能被采到
        - 样本在m次采样中始终不被采到的概率取极限$\lim  _ {m \mapsto \infty}\left(1-\frac{1}{m}\right)^{m} \mapsto \frac{1}{e} \approx 0.368$
        - 我们说D中约有36.8%的样本未出现在D‘中，D\D‘即可作为测试集
        - 这样的测试结果，成为：包外估计“
        - 优点：
          - 在数据集较小，难以有效划分训练/测试集时很有用
          - 能从初始数据集中产生多个不同的训练集，对集成学习等有较大好处
        - 缺点：改变了初始数据集分布，会引入估计偏差，所以数据量足够时，留出法和交叉验证更为常用
      - 调参与最终模型
        - 学习算法的很多参数是在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的
        - 常用做法：选定范围和变化步长
          - 非最佳，但是在计算开销和性能估计之间折中的结果
        - 最终模型！！！：一开始只适用一部分数据训练模型，所以在模型选择完成之后，需要将D重新训练模型

## 性能度量

- 概念：衡量模型泛化能力的评价标准

  - **均方误差**：回归任务最常用

    - $E(f ; D)=\frac{1}{m} \sum _ {i=1}^{m}\left(f\left(\boldsymbol{x} _ {i}\right)-y _ {i}\right)^{2}$

    - 对于数据分布$\mathcal{D}$和概率密度函数$p(·)$，均方误差

      $E(f ; \mathcal{D})=\int _ {\boldsymbol{x} \sim \mathcal{D}}(f(\boldsymbol{x})-y)^{2} p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$

    - 几何意义：对应了欧式距离

      - 基于其最小化来进行模型求解的方法为最小二乘法
      - 最小二乘法就是找到一条直线，使所有样本到直线上的欧式距离和最小

- 错误率与精度

  - 错误率：

    - $E(f ; D)=\frac{1}{m} \sum _ {i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right)$

    - 对于数据分布$\mathcal{D}$和概率密度函数$p(·)$，

      $E(f ; \mathcal{D})=\int _ {\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$

  - 精度
    
      $ \begin{aligned} \operatorname{acc}(f ; D) =\frac{1}{m} \sum _ {i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right)=y _ {i}\right) =1-E(f ; D) \end{aligned} $
      
  - 对于数据分布$\mathcal{D}$和概率密度函数$p(·)$，
    
      $\begin{aligned}
      \operatorname{acc}(f ; \mathcal{D})=\int _ {\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x})=y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
      =1-E(f ; \mathcal{D})
    \end{aligned}$
    
  - 查准率，查全率（两者互相矛盾）与F1

    - **查准率**$P=\frac{T P}{T P+F P}$（在预测结果为正例的情况下预测正确的概率）

    - **查全率**$R=\frac{T P}{T P+F N}$(在真实情况为正例的情况下预测正确的概率)

    - T/F判断是否正确

      P/N预测结果是否是正例

      以下为**混淆矩阵**：

    - | 真实情况\预测结果 | 正例         | 反例         |
      | ----------------- | ------------ | ------------ |
      | 正例              | TP（真正例） | FN（假反例） |
      | 反例              | FP（假正例） | TN（真反例） |

      T/F判断是否正确

      P/N预测结果是否是正例

    - 矛盾度量

      - 查准率高，查全率低
      - 比如：为了高查全率，将所有西瓜都选上，所有的好瓜也都必然被选上了，但这样查准率就低
      - 查准率高，只挑选最有把握的瓜，但可能就会漏掉不少好瓜

    - P-R曲线与**平衡点**

      - 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则后者的性能优与前者（比如A优于C）
      - 如果两个学习器的P-R曲线发生交叉，则具体情况具体分析。此时，可以比较P-R曲线下面积的大小，这表征了学习器在查准率和查全率上取得相对较好的比例

    <img src="/assets/MLpics/T2.png" alt="图片2" style="zoom:60%;" />

    - ​	平衡点（BEP）

      - 综合考虑查准率，查全率的性能度量
      - 查准率=查全率的取值
      - 平衡点越高，学习器性能越好

    - **F1**度量（调和平均）

      - BEP较为简化

      - 与算术平均和几何平均相比，调和平均更重视较小值

      - $F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}$

      - 表达出对查准率/查全率的不同偏好：F1度量的一般形式（加权调和平均）

        $F _ {\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}$

        - $\beta>0$度量了查全率对查准率的相对重要性
          - $\beta>1$对查全率有更大影响
          - $\beta=1$退化为标准的F1
          - $\beta<1$对查准率有更大影响

    - 多个二分类混淆矩阵综合考察

      - 方法1:先在各混淆矩阵上分别计算出查准率和查全率，记为(P1,R1),(P2,R2),...,(Pn,Rn)，再计算平均值

        - 宏查准率

          $\operatorname{macro}-P=\frac{1}{n} \sum _ {i=1}^{n} P _ {i}$

          - 宏查全率

          $\operatorname{macro}-R=\frac{1}{n} \sum _ {i=1}^{n} R _ {i}$

          - 宏F1

          $\operatorname{macro}-F 1=\frac{2 \times \operatorname{macro}-P \times \operatorname{macro}-R}{\operatorname{macro}-P+\operatorname{macro}-R}$

      - 对各混淆矩阵的对应元素进行平均，得到TP，FP，
        TN，FN的平均值，再计算出

        - 微查准率

        $\operatorname{micro}-P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}$

        - 微查全率

        $\operatorname{micro}-R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}$

        - 微F1

        $\operatorname{micro}-F 1=\frac{2 \times \operatorname{micro}-P \times \operatorname{micro}-R}{\operatorname{micro}-P+\operatorname{micro}-R}$

- **ROC与AUC**

  - 很多学习器是为样本产生一个实值或概率预测，然后将这个预测值与一个分类**阈值（threshold）**比较，大于阈值为正类，否则为反。

  - 这个实值预测结果的好坏，直接决定了学习器的泛化能力。根据这个实值或概率预测结果，将测试样本进行排序。

  - 分类过程=以某个截断点将样本分为两部分，前一部分判定为正例，后部分为反例

    - 更重视查准率：选择排序靠前的位置进行截断
    - 更重视查全率：选择靠后的位置进行截断

  - ROC：研究学习器泛化性能的工具

    - 根据预测结果对样例进行排序，按此排序逐个把样本作为正例进行预测，每次计算出两个量的值，分别以他们为横纵坐标作图

      - 真正确率（纵轴）

      $\mathrm{TPR}=\frac{T P}{T P+F N}$

      - 假正例率（横轴）

      $\mathrm{FPR}=\frac{F P}{T N+F P}$

      - <img src="/assets/MLpics/T3.png" style="zoom:50%;" />
      - 曲线说明
        - 有限个坐标对时，无法产生光滑曲线
        - 先把分类阈值设为最大，所有样例均为反例，此时坐标为（0，0）
        - 依次将每个样例划分为正例
        - 在统计预测结果时，预测值=分类阈值的样本也算作预测为正例

    - AUC：如果两个ROC曲线发生交叉，则难以一般性断言两者谁优谁劣，此时比较ROC曲线的面积=AUC

      $\mathrm{AUC}=\frac{1}{2} \sum _ {i=1}^{m-1}\left(x _ {i+1}-x _ {i}\right) \cdot\left(y _ {i}+y _ {i+1}\right)$

      - 🎃书案例：

        假设有7个预测结果（不做详细说明），画出ROC曲线，将增加面积看成梯形来计算，（梯形和长方形的面积公式相同）

        **梯形面积**$=\frac{1}{2} \left(x _ {i+1}-x _ {i}\right) \cdot\left(y _ {i}+y _ {i+1}\right)$

        ![](/assets/MLpics/T4.png)

      - 给$m^{+}$个正例和$m^{-}$个反例，令$D^{+}$和$D^{-}$分别表示正、反例集合，则loss为

        $\ell _ {r a n k}=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)$

        做变形：$$\begin{aligned} \ell _ {rank} &=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)  \\  &=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}}\left[\sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \cdot \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right]  \\ &=\sum _ {x^{+} \in D^{+}}\left[\frac{1}{m^{+}} \cdot \frac{1}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \cdot \frac{1}{m^{+}} \cdot \frac{1}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right]  \\ &=\sum _ {x^{+} \in D^{+}} \frac{1}{2} \cdot \frac{1}{m^{+}} \cdot\left[\frac{2}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right]
        \end{aligned}$$

        - 因为新增正例就新增一条蓝色/绿色线段，所以$\sum _ {x^{+} \in D^{+}}$是在遍历所有蓝色和绿色线段

        - 后面那一项是在求绿色线段/蓝色线段与y轴围成的面积

          $\frac{1}{2} \cdot \frac{1}{m^{+}} \cdot\left[\frac{2}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)<f\left(\boldsymbol{x}^{-}\right)\right)+\frac{1}{m^{-}} \sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)=f\left(\boldsymbol{x}^{-}\right)\right)\right]$

          - $\frac{1}{m^{+}} $为梯形的高

          - 梯形的上底，每增加一个假正例时x坐标就新增一个单位，实则为目前预测值$x^{+}$大的假正例个数乘$\frac{1}{m^{-}}$

            $\frac{1}{m^{-}} \sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)<f\left(\boldsymbol{x}^{-}\right)\right)$

          - 梯形的下底，每增加一个假正例时x坐标就新增一个单位，实则为目前预测值$x^{+}$大/等的假正例个数乘$\frac{1}{m^{-}}$

            $\frac{1}{m^{-}}\left(\sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)<f\left(\boldsymbol{x}^{-}\right)\right)+\sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)=f\left(\boldsymbol{x}^{-}\right)\right)\right)$

          - $\mathrm{AUC}\approx1-\ell _ {\text {rank}}$

  - **代价敏感错误率**与**代价曲线**

    - 情况：错误的把患者检测为健康（必须避免发生）/把健康的人检测为患者（增加了一次进一步检查的麻烦）

    - **代价矩阵**$cost _ {ij}$代表将第i类样本预测为第j类样本的代价

      | 真实类别\预测类别 | 0           | 1           |
      | ----------------- | ----------- | ----------- |
      | 0                 | 0           | $cost _ {01}$ |
      | 1                 | $cost _ {10}$ | 0           |

    - 之前的性能度量大多隐式的假设了均等代价，并没有考虑不同错误会造成不同的后果

    - 在非均等代价下，我们希望最小化“总体代价”而非“错误次数”

    - 将第0类作为正类，第1类作为反类，$D^{+}\text{与}D^{-}$分别代表D的正例子集和反例子集，则代价敏感错误率 $$\begin{aligned}
      E(f ; D ; \cos t)=& \frac{1}{m}\left(\sum _ {\boldsymbol{x} _ {i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right) \times \operatorname{cost} _ {01}
      +\sum _ {\boldsymbol{x} _ {i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right) \times \operatorname{cost} _ {10}\right)
      \end{aligned}$$

    - 在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，但“代价曲线”可达到该目的

      - 横轴：取值为[0,1]的正例概率代价

        $P(+) \cos t=\frac{p \times \cos t _ {01}}{p \times \cos t _ {01}+(1-p) \times \cos t _ {10}}$

      - 纵轴：取值为[0,1]的归一化代价

        $\operatorname{cost} _ {n o r m}=\frac{\operatorname{FNR} \times p \times \cos t _ {01}+\mathrm{FPR} \times(1-p) \times \operatorname{cost} _ {10}}{p \times \operatorname{cost} _ {01}+(1-p) \times \operatorname{cost} _ {10}}$

        FPR是假正例率，FNR=1-TPR是假反例率

      - ROC曲线上的每一点对应了代价平面上的一条线段，设ROC曲线上点的坐标为（TPR，FPR），则可相应计算出FNR，然后绘制一条从（0，FPR）到（1，FNR）的线段

      - 线段下的面积代表该条件下的期望总体代价

## 比较检验

- **假设检验**

  - 假设：对学习器泛化错误率分布的某种判断或猜想

    - 现实生活中我们并不知道学习器的泛化错误率，只能知道其测试错误率
    - 泛化错误率与测试错误率不一定相同，但二者接近的可能性较大

  - 泛化错误率为$\epsilon$的学习器在一个样本上犯错的概率是$\epsilon$

  - 测试错误率$\hat{\epsilon}$意味着在m个测试样本中恰有$\hat{\epsilon}\times m$被错误分类

    - 假设测试样本是从样本分布中独立采样获得，泛化错误率为$\epsilon$的学习器恰好将其中$m'$个样本误分类的概率$\epsilon^{m^{\prime}}(1-\epsilon)^{m-m^{\prime}}$

    - 恰好将$\hat{\epsilon}\times m$个样本误分类的概率/泛化错误率为$\epsilon$的学习器被测试得测试错误率为$\hat{\epsilon}$的概率$$ P(\hat{\epsilon} ; \epsilon)=\left(\begin{array}{c} m  \\ \hat{\epsilon} \times m  \end{array}\right) \epsilon^{\hat{\epsilon} \times m}(1-\epsilon)^{m-\hat{\epsilon} \times m}$$

    - 给定$\hat{\epsilon}$，则

      ​	<img src="/assets/MLpics/T5.png" style="zoom:50%;" />

      - $\partial P(\hat{\epsilon} ; \epsilon) / \partial \epsilon=0$
      - $P(\hat{\epsilon} ; \epsilon)$在$\hat{\epsilon}=\epsilon$时最大，如图，若$\epsilon=0.3$则10个样本中测得3个被误分类的概率最大
      - $ \vert \hat{\epsilon}-\epsilon \vert $增大时减小
      - 符合二项分布，注意$$\left(\begin{array}{c}
        m  \\ 
        \hat{\epsilon} \times m
        \end{array}\right) $$

    - **二项检验**

      - 对$\epsilon \leq \epsilon _ {0}$进行假设检验（单侧）

      - 求解最大错误率$\bar{\epsilon}$

      - $ 1-\alpha$为概率学习中的置信度，如0.95，0.90

      - 计算事件最小发生频率到最大发生频率的概率和现$\epsilon _ {0}$的情况做对比，从而得到检验结果

      - $\frac{\bar{C}}{m}$为事件最大发生频率$$\bar{C}=\min C \quad \text { s.t. } \sum _ {i=C+1}^{m}\left(\begin{array}{c}
        m  \\ 
        i
        \end{array}\right) p _ {0}^{i}\left(1-p _ {0}\right)^{m-i}<\alpha$$
        $$\frac{\bar{C}}{m}=\min \frac{C}{m} \quad \text { s.t. } \quad \sum _ {i=C+1}^{m}\left(\begin{array}{c}
        m  \\ 
        i
        \end{array}\right) p _ {0}^{i}\left(1-p _ {0}\right)^{m-i}<\alpha$$

        将$\frac{\bar{C}}{m}, \frac{C}{m}, p _ {0}$等价替换为$\bar{\epsilon}, \epsilon, \epsilon _ {0}$
        $$\bar{\epsilon}=\min \epsilon \quad \text { s.t. } \quad \sum _ {i=\epsilon \times m+1}^{m}\left(\begin{array}{c}
        m  \\ 
        i
        \end{array}\right) \epsilon _ {0}^{i}\left(1-\epsilon _ {0}\right)^{m-i}<\alpha$$

      - 若在$\alpha$的显著度下，$\epsilon \leq \epsilon _ {0}$不能被拒绝，那么就能以$1-\alpha$的置信度认为，学习器的泛化错误率不大于$\epsilon _ {0}$

    - **t检验**（大学概率学习内容）

      - 经过多次重复留出法/交叉验证法进行多次训练/测试，会得到多个测试错误率

      - 注意误分类样本数服从二项分布，大量样本之后，根据**中心极限定理**，所以大量二项分布（极限）服从正态

        <img src="/assets/MLpics/T13.png" style="zoom:50%;" />

      - 假设我们得到了k个测试错误率$\hat\epsilon _ {1}, \hat{\epsilon} _ {2}, \ldots, \hat{\epsilon} _ {k}$，则

        - $\mu=\frac{1}{k} \sum _ {i=1}^{k} \hat{\epsilon} _ {i}$
        - $\sigma^{2}=\frac{1}{k-1} \sum _ {i=1}^{k}\left(\hat{\epsilon} _ {i}-\mu\right)^{2}$
        - 则$\tau _ {t}=\frac{\sqrt{k}\left(\mu-\epsilon _ {0}\right)}{\sigma}$

      - 大学概率补充知识：

        - <img src="/assets/MLpics/T6.png" style="zoom:50%;" />

        - <img src="/assets/MLpics/T8.png" style="zoom:50%;" />

        - t假设检验<img src="/assets/MLpics/T7.png" style="zoom:50%;" />

        - 得到服从自由度为k-1的t分布

          <img src="/assets/MLpics/T9.png" style="zoom:50%;" />

        - 对假设$\mu=\epsilon$和显著度$\alpha$，我们可以计算出当测试错误率均值为$\epsilon _ {0}$时，在$1-\alpha$概率内能观测到的最大错误率

        - 假设检验（两侧）

        - $ \vert \mu-\epsilon _ {0} \vert $在$[t _ {-\alpha/2},t _ {\alpha/2}]$内，为接受域

- **交叉验证t检验**

  - 对两个学习器A和B，使用k折交叉验证法得到的测试错误率分别为$\epsilon _ {1}^{A},\epsilon _ {2}^{A}...\epsilon _ {k}^{A}$和$\epsilon _ {1}^{B},\epsilon _ {2}^{B}...\epsilon _ {k}^{B}$，其中$\epsilon _ {i}^{A}$和$\epsilon _ {i}^{B}$是在相同的第i折训练/测试集上得到的结果

  - 如果两个学习器的性能相同，则它们使用相同的训练/测试集得到的测试错误率应相同

  - 先对每对结果求差$\Delta i=\epsilon _ {i}^{A}-\epsilon _ {i}^{B}$

    - 若两个学习器性能相同，则差值均值应该为0

    - 应该根据差值$\Delta 1,\Delta 2,...\Delta k$来对学习器A与B性能相同这个假设做t检验

    - 计算差值的均值$\mu$和方差$\sigma^{2}$

    - $\tau _ {t}= \vert \frac{\sqrt{k}\mu}{\sigma} \vert $

    - 小于$t _ {\alpha/2}(k-1)$，为接受域，两个学习器的性能没有显著差别

    - 大学概率补充知识

      <img src="/assets/MLpics/T10.png" style="zoom:50%;" />

  - 假设检验的重要前提：测试错误率均为泛化错误率的独立采样

    - 但通常情况下由于样本有限，所以在使用交叉验证时，不同轮次的训练集会有一定程度的重叠
    - 过高估计假设成立的概率
    - 采用5x2交叉验证
      - 做5次2折交叉验证
      - 在每次2折交叉验证之前随机将数据打乱，使得5次交叉验证中的数据划分不重复
      - 对两个学习器A，B，第i次2折交叉验证将产生两对错误测试率，对其分别求差，得到第1折上的差值$\Delta _ {i}^{1}$和第2折上的差值$\Delta _ {i}^{2}$
      - 为缓解测试错误率的非独立性，仅计算第1次2折交叉验证的两个结果的平均值$\mu=0.5(\Delta _ {1}^{1}+\Delta _ {1}^{2})$
      - 对每次2折实验的结果都计算其方差$\sigma _ {i}^{2}=(\Delta _ {i}^{1}-\frac{\Delta _ {1}^{1}+\Delta _ {1}^{2}}{2})^{2}+(\Delta _ {i}^{2}-\frac{\Delta _ {1}^{1}+\Delta _ {1}^{2}}{2})^{2}$
      - $\tau _ {t}=\frac{\mu}{\sqrt{0.2}\sum _ {i=1}^{5}\sigma _ {i}^{2}}$服从自由度为5的t分布

- **McNemar检验**

  - **列联表（contingency table）**是观测数据按两个或更多属性（定性变量）分类时所列出的频数表。它是由两个以上的变量进行交叉分类的频数分布表。

  - <img src="/assets/MLpics/T11.png" style="zoom:50%;" />

  - 若假设为两学习器性能相同，则应有$e _ {01}=e _ {10}$（实则为概率$p _ {e _ {01}}=p _ {e _ {10}}$）

  - 根据ALLEN L. EDWARDS的论文

    “NOTE ON THE "CORRECTION FOR CONTINUITY" IN TESTING THE SIGNIFICANCE OF THE DIFFERENCE BETWEEN CORRELATED PROPORTIONS ”

    <img src="/assets/MLpics/T15.png" style="zoom:40%;" />

  - 通过卡方分布进行评估，可将上表的$e _ {01}$与$e _ {10}$两个频率中较小的一个加上0.5、较大的一个减去0.5来进行**连续性校正**。

  - 那么变量$ \vert e _ {01}-e _ {10} \vert $应该服从正态分布，均值为1，方差为$e _ {01}+e _ {10}$

  - 变量$\tau _ {\chi}^{2}=\frac{( \vert e _ {01}-e _ {10} \vert -1)^{2}}{e _ {01}+e _ {10}}$

  - 服从自由度为1的**$\chi^{2}$分布**

  - 大学概率补充：

    <img src="/assets/MLpics/T12.png" style="zoom:50%;" />

- **Friedman检验**与Nemenyi后续检验

  - 当有多个算法参与比较时

    - 在每个数据集上分别列出两两比较的结果

      - 假定我们用$D _ {1},D _ {2},D _ {3},D _ {4}$四个数据集对ABC进行比较

      - 使用留出法/交叉验证法得到每个算法在每个数据集上的测试结果

      - 在每个数据集上根据测试性能由好到坏排序，并赋予序值

      - 若算法测试性能相同，则平分序值

        <img src="/assets/MLpics/T14.png" style="zoom:50%;" />

    - 使用基于算法排序的Friedman检验

      - 判断这些算法是否性能都相同（根据平均序值）

      - 我们在N个数据集上比较k个算法

        - $r _ {i}$表示第i个算法的平均序值

        - 暂时不考虑平分序值的情况

        - $r _ {i}$服从正态分布

          - 均值：$sum=k(k+1)/2$ 所以$\mu=sum/k=(k+1)/2$
          - 方差  $$ \begin{aligned} \delta^2 &=[(1-(k+1)/2)^{2}+(2-(k+1)/2)^{2}+...+(k-(k+1)/2)^{2}]/k \\ &=[(2-k-1)^{2}/4+(4-k-1)^{2}/4+...+(2k-k-1)^{2}/4]/k  \\ &= (4-2\times2(k+1)+(k+1)^{2}+...+(4k^{2}-2k\times2(k+1)+(k+1)^{2})/4k  \\ &=[(4+16+...+4k^{2})-2(k+1)(2+4+...+2k)+k(k+1)^{2}]/4k  \\ &=[4(1+2^{2}+...+k^{2})-4(k+1)(1+2+...+k)+k(k+1)^{2}]/4k \end{aligned} $$
            - 根据平方和公式（注意该图中n对应上式中k）<img src="/assets/MLpics/T16.png" style="zoom:50%;" />
            $$ \begin{aligned} \delta^2 &=[4k(k+1)(2k+1)/6-4(k+1)(1+2+...+k)+k(k+1)^{2}]/4k  \\ &=(k+1)[8k^{2}/6+4k/6-(k^{2}+k)]/4  \\ &=(k+1)[k^{2}/3-k/3]/4k \\ &=(k^{2}-1)/12 \end{aligned} $$
          - 所以其均值和方差分别为$(k+1)/2$和$(k^{2}-1)/12$，则$$ \begin{aligned} \tau _ {\chi}^{2}&=\frac{k-1}{k}\frac{12N}{k^{2}-1}\sum _ {i=1}^{k}(r _ {i}-\frac{k+1}{2})^{2}  \\ &=\frac{12N}{k(k+1)}(\sum _ {i=1}^{k}r _ {i}^{2}-\sum _ {i=1}^{k}r _ {i}(k+1)+\frac{k(k+1)^{2}}{4}) \\ &=\frac{12N}{k(k+1)}(\sum _ {i=1}^{k}r _ {i}^{2}-\frac{k(k+1)^{2}}{4}) \end{aligned}$$

        - 在k和N都较大的情况下，其服从自由度为k-1的$\chi^{2}$分布

        - 现在通常使用**F检验**

          - $\tau _ {F}=\frac{(N-1)\tau _ {\chi^{2}}}{N(k-1)-\tau _ {\chi^{2}}}$

          - 补充知识，**F分布**

            <img src="/assets/MLpics/T17.png" style="zoom:50%;" />

          - $\tau _ {F}$服从自由度为$k-1$和$(k-1)(N-1)$的F分布

    - Nemenyi后续检验算法

      - 若所有算法的性能相同的假设被拒绝，需要进行“后续检验”来进一步区分各算法

      - Nemenyi检验计算出平均序值差别的临界值域

      - $CD=q _ {\alpha}\sqrt{\frac{k(k+1)}{6N}}$

        <img src="/assets/MLpics/T18.png" style="zoom:50%;" />

      - 若两个算法的平均序值之差超出了临界值域CD，则拒绝两个算法性能相同的假设

    - 举个例子

      <img src="/assets/MLpics/T14.png" style="zoom:50%;" />

      - 先计算出$\tau _ {F}=24.429$，它大于$\alpha=0.05$时F的检验临界值，因此它拒绝“所有算法性能相同”的假设

      - 使用Nemenyi后续检验，$k=3$时$q _ {0.05}=2.344$，临界值域$CD=1.657$，所以算法A与B的差距，以及算法B与C的差距均未超过临界值域，而算法A和C的差距超过临界值域，所以认为A与C的性能显著不同

      - Friedman检验图如下图所示

        <img src="/assets/MLpics/T19.png" style="zoom:50%;" />

        - 两个横线段（临界值域）有交叠，则说明这两个算法没有显著差别

## 偏差与方差

- 偏差-方差分解

  - 是解释学习算法泛化性能的一种重要工具

  - 对学习算法的期望泛化错误率进行拆解

    - 算法在不同训练集（可能来自同一分布）上学得的结果可能不同

    - 对测试样本$x$，令$y _ {D}$为$x$在数据集中的标记，$y$为$x$的真实标记，$f(x;D)$为训练集D上学得模型$f$在$x$上的预测输出

    - 以回归任务为例

      - 学习算法期望

      <img src="/assets/MLpics/T20.png" style="zoom:40%;" />

      - **偏差**
        - 期望预测与真实结果的偏离程度
        - 学习算法本身的拟合能力
      - **方差**
        - 同样大小的训练集变动所导致的
        - 学习性能的变化数据扰动所造成的影响

    - 噪声

      - 在当前任务上任何学习算法所能达到的期望泛化误差的下界
      - 计算数据集中标记和真实标记的差别，属于学习该问题本身存在的差距
      - 学习问题本身的难度
      - 假定噪声期望为0，也就是<img src="/assets/MLpics/T21.png" style="zoom:40%;" />

    - 通过推导，可对算法的期望泛化误差进行分解

      ![](/assets/MLpics/T22.png)

    - 公式推导详细说明
      - 3-4: $$ \begin{aligned} \mathbb{E} _ {D}[2(f(x;D)-\mathop{f}\limits_{}^-(x))\mathop{f}\limits_{}^-(x)] &= \mathbb{E} _ {D}[2(f(x;D)\mathop{f}\limits_{}^-(x)-\mathop{f}\limits_{}^-(x)^2)] \\ &= 2\mathop{f}\limits_{}^-(x)\mathbb{E} _ {D}(f(x;D))-2\mathop{f}\limits _ {}^-(x)^2 \\ &= 2\mathop{f}\limits _ {}^-(x)^2-2\mathop{f}\limits _ {}^-(x)^2=0\end{aligned} $$
        - $ \mathop{f}\limits _ {}^-(x)$为常量，并且$\mathbb{E} _ {D}(f(x;D))=\mathop{f}\limits _ {}^-(x) $，所以$$ \begin{aligned} \mathbb{E} _ {D}[2(f(x;D)-\mathop{f}\limits_{}^-(x))y_{D}] &= \mathbb{E} _ {D}[2(f(x;D)\mathop{f}y _ {D}-\mathop{f}\limits _ {}^-(x)y_{D})] \\ &= 2\mathbb{E} _ {D}(y _ {D})\mathbb{E} _ {D}(f(x;D))-2\mathop{f}\limits_{}^-(x)\mathbb{E} _ {D}(y _ {D}) \\ &= 2\mathop{f}\limits_{}^-(x)\mathbb{E} _ {D}(y _ {D})-2\mathop{f}\limits _ {}^-(x)\mathbb{E} _ {D}(y _ {D})=0 \end{aligned} $$
      - 6-7: $ \mathbb{E} _ {D}(2(\mathop{f}\limits _ {}^-(x)-y)(y-y _ {D}))=2(\mathop{f}\limits _ {}^-(x)-y)\mathbb{E} _ {D}(y-y _ {D}) $
        - $\mathbb{E} _ {D}(y-y _ {D})=0$
        - 原式=0

    - 最终结果$E(f;D)=var(x)+bias^2(x)+\epsilon^2$

      - 泛化误差可以分解为偏差、方差与噪声之和
    - 泛化性能是由学习算法的能力、数据的充分性和学习任务本身的难度所决定的

  - 偏差-方差窘境（bias-variance dilemma)

    - <img src="/assets/MLpics/T23.png" style="zoom:50%;" />
  - 在训练不足时，学习器的拟合能力不强，偏差大，方差小，此时偏差主导泛化错误率
    - 随着训练逐渐加深，拟合能力逐渐增强，方差逐渐主导泛化错误率
    - 当训练充足时，学习器拟合能力超强，方差大，偏差小，训练数据的微小变化都会导致学习器发生显著变化
      - 导致过拟合：训练数据自身的非全局的特性被学习到了，并且学习器使用这些无用的“特性”进行后续预测

## 部分习题解释

- 2.1

  - 需要1000个样本，700个训练集，300个测试集
  - 500个正例和500个反例平均分配
  - 700训练集：350正/350反，300测试集：150正/150反
  - $C _ {500}^{350}C _ {500}^{350}$

- 2.2 

  - 10折交叉验证
    - 分成10个大小相似的互斥子集（5个一组）
    - 9个子集为训练集，1个测试集
    - 有10种选择求均值
    - 正反例数目相同
    - 预测判断为正反例的概率相同
    - 错误率50%
  - 留一法
    - 每一个都是一个子集
    - 99个子集为训练集，1个测试集
    - 有100种选择求均值
    - 可能结果：正/反
    - 正：预测错误，反：预测错误 错误率100%

- 2.3

  - $F 1=\frac{2 \times P \times R}{P+R}$
  - 事实上在P同的情况下，R越大$F1$越大，反之亦然
  - 而P越大，R越小
  - P-R图通常是非单调，不平滑的，在很多局部有上下波动
  - A和B的F1值相同和BEP值二者没有绝对相关。
  - BEP点前后很可能同时出现更大的P和更大的R，因此现实中BEP并不实用。

- 2.4

  - 公式
    - $\mathrm{TPR}=\frac{T P}{T P+F N}$
    - $\mathrm{FPR}=\frac{F P}{T N+F P}$
    - $P=\frac{T P}{T P+F P}$
    - $R=\frac{T P}{T P+F N}$
    - $TPR=R$    查全率=真正例率

- 2.5

  - 证明$\mathrm{AUC}\approx1-\ell _ {\text {rank}}$

  - ROC曲线如下（已经乘$m^+m^-$）

    <img src="/assets/MLpics/T4.png" style="zoom:50%;" />

  - 以上图为例，一共有4个正例4个反例$m^+=m^-=4$

  - 理论向证明

    - 暂时不考虑$f(x^{+})=f(x^{-})$，$AUC=\sum _ {i=1}^{m-1}\left(x _ {i+1}-x _ {i}\right)y _ {i}$
      - 乘$m^+m^-$（如上图），可发现$x _ {i+1}-x _ {i}=\{0,1\}$
      - 当$x _ {i+1}-x _ {i}=1$时，$(x _ {i+1},y _ {i+1})$是一个反例样本，排在它前面的有$y _ {i}$个正例
      - 也就是说$AUC=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}(\mathbb{I}(f(x^{+})>f(x^{-}))$
      - 此时$\ell _ {r a n k}=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)\right)$
      - $\mathrm{AUC}=1-\ell _ {\text {rank}}$
    - 考虑$f(x^{+})=f(x^{-})$
      - 正例预测值和反例预测值都相同，如图中的（x2，y3）和（x3，y4）
      - 在AUC面对这种情况时，会多余的加上一个三角形，需要减去
      - 对于$\ell _ {r a n k}$来说就是加上这个三角（1/2）

  - 数据向证明

    - $\ell _ {r a n k}=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)$
      - $f(x^{+})<f(x^{-})$
        - （0，y2）点前有1个反例样本
        - （x2，y3）点前有2.5个反例样本
        - （x4，y5）点前有3个反例样本
      - $f(x^{+})=f(x^{-})$
        - （x2，y3）和（x3，y4）预测值相等，记0.5
    - 由上式可得$\ell _ {r a n k}=$5.5/16
    - $\mathrm{AUC}=\frac{1}{2} \sum _ {i=1}^{m-1}\left(x _ {i+1}-x _ {i}\right) \cdot\left(y _ {i}+y _ {i+1}\right)$（总面积）
    - 由上式可得$AUC=(1+2+1/2+3+4)/16=10.5/16$
    - 所以$\mathrm{AUC}=1-\ell _ {\text {rank}}$

- 2.6

  - $\mathrm{TPR}=\frac{T P}{T P+F N}$，针对$D^+$情况
  - $\mathrm{FPR}=\frac{F P}{T N+F P}$，针对$D^-$情况
  - 可得$$ \begin{aligned}
    E(f ; D ; \cos t)&= \frac{1}{m}(\sum _ {\boldsymbol{x} _ {i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}) \times \operatorname{cost} _ {01}
    +\sum _ {\boldsymbol{x} _ {i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right) \times \operatorname{cost} _ {10}\right)
     \\ &= \frac{1}{m}( \vert D^+ \vert (1-TPR) cost _ {01}+FPR \vert D^- \vert cost _ {10}) \\ \end{aligned} $$

- 2.10

  - 一个为卡方检验：主要针对具有相同频数的数据组的一致性（在2.34中用于比较不同算法之间的差异）
  - 一个为基于卡方检验的F检验：主要针对两组数值差异性（在2.35中基于卡方检验进行数据集之间的差异性）
  - F检验比起卡方检验更好的是考虑到了不同数据集所带来的影响（卡方检验直接xN）

# 线性模型

## 基本形式

- $f(x)=w _ {1}x _ 1+w _ 2x _ 2+...+w _ dx _ d+b$
- 向量形式$f(x)=w^Tx+b$
- 许多非线性模型可以在线性模型的基础上通过引入层级结构或高维影射得到
- 由于w直观表达了各属性在预测中的重要性，因此线性模型有很好的**可解释性**

## 线性回归

- 数据集$D={(x _ 1,y _ 1),(x _ 2,y _ 2)...,(x _ m,y _ m)}$

- $x _ i=(x _ i1;x _ i2;...;x _ id)$

- 线性回归试图学得一个线性模型以尽可能的预测实值并输出标记

- 最简单的情形：输入属性的数目只有一个，也就是说第二条的$x _ i=x _ {i1}$

  - $D=\{(x _ i,y _ i)\} _ {i=1}^m$

  - 对于离散属性

    - 若属性值之间存在序关系
      - 可通过连续化将其转化为序列值
      - 如高、矮可转化为$\{1.0,0.0\}$
    - 若属性值之间不存在序关系
      - 如果有k个属性值，则通常转化为k维向量
      - 如西瓜、南瓜可转化为$(1,0),(0,1)$

  - 线性回归试图学得$f(x _ i)=wx _ i+b,\text{使得}f\left(x _ {i}\right) \simeq y _ {i}$

    - 如何求得w和b

      - 在性能度量一节中，均方误差是回归任务最常用的，为凸函数（说明详见西瓜书P54注解）

      - 试图让均方误差最小化$$ \begin{aligned}
        \left(w^{ * }, b^{ * }\right) &=\underset{(w, b)}{\arg \min } \sum _ {i=1}^{m}\left(f\left(x _ {i}\right)-y _ {i}\right)^{2}  \\ 
        &=\underset{(w, b)}{\arg \min } \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}-b\right)^{2}
        \end{aligned} $$

      - 在2.3节中提到，均方误差的几何意义对应欧式距离，基于均方误差最小化来进行模型求解的方法为**“最小二乘法”**

      - 线性回归模型的最小二乘“参数估计”：求解$w,b$使得均方误差最小化的过程

      - 我们分别对$w,b$求导

        - $\frac{\partial E _ {(w, b)}}{\partial w}=2\left(w \sum _ {i=1}^{m} x _ {i}^{2}-\sum _ {i=1}^{m}\left(y _ {i}-b\right) x _ {i}\right)$

        - $\frac{\partial E _ {(w, b)}}{\partial b}=2\left(m b-\sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)\right)$

        - 令上两式为0得到$w,b$的最优解的闭式解

          - $b=\frac{1}{m} \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)$

          - $w=\frac{\sum _ {i=1}^{m} y _ {i}\left(x _ {i}-\bar{x}\right)}{\sum _ {i=1}^{m} x _ {i}^{2}-\frac{1}{m}\left(\sum _ {i=1}^{m} x _ {i}\right)^{2}}$

            - 求解上方求导式（注意带入b）：

            - $w \sum _ {i=1}^{m} x _ {i}^{2} =\sum _ {i=1}^{m} y _ {i} x _ {i}-\sum _ {i=1}^{m}b x _ {i}$

            - $b=\frac{1}{m} \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)$

              - $\frac{1}{m} \sum _ {i=1}^{m}y _ {i}=\bar{y}$
              - $\frac{1}{m} \sum _ {i=1}^{m}w x _ {i}=w\bar{x}$
              - $b=\bar{y}-w\bar{x}$
              - 所以$$ \begin{aligned} w \sum _ {i=1}^{m} x _ {i}^{2} &=\sum _ {i=1}^{m} y _ {i} x _ {i}-\sum _ {i=1}^{m}(\bar{y}-w \bar{x}) x _ {i}  \\ 
              w \sum _ {i=1}^{m} x _ {i}^{2} &=\sum _ {i=1}^{m} y _ {i} x _ {i}-\bar{y} \sum _ {i=1}^{m} x _ {i}+w \bar{x} \sum _ {i=1}^{m} x _ {i} \end{aligned}$$
- 可得$$\begin{aligned}w\left(\sum _ {i=1}^{m} x _ {i}^{2}-\bar{x} \sum _ {i=1}^{m} x _ {i}\right) &=\sum _ {i=1}^{m} y _ {i} x _ {i}-\bar{y} \sum _ {i=1}^{m} x _ {i} \\ w=&\frac{\sum _ {i=1}^{m} y _ {i}\left(x _ {i}-\bar{x}\right)}{\sum _ {i=1}^{m} x _ {i}^{2}-\frac{1}{m}\left(\sum _ {i=1}^{m} x _ {i}\right)^{2}}\end{aligned}$$
              

          - **闭式解**
            
            - 又名解析解，针对一些严格的公式，给出任意的自变量就可以求出其因变量,也就是问题的解, 他人可以利用这些公式计算各自的问题。
            - 除此之外，**数值解**是采用某种计算方法,如数值逼近,插值的方法 得到的解。别人只能利用数值计算的结果, 而不能随意给出自变量并求出计算值。

- 更一般的情形是$f(x _ i)=wx _ i^T+b,\text{使得}f\left(x _ {i}\right) \simeq y _ {i}$，称之为多元线性回归

  - 同样适用最小二乘法进行估计

  - 把$w,b$放入向量形式

  - 把数据集D表示为一个$m\times(d+1)$大小的矩阵X

    - 其中每行对应一个示例

    - 该行前d个元素对应于示例的d个属性值，最后一个元素恒为1$$\mathbf{X}=\left(\begin{array}{ccccc}
  x _ {11} & x _ {12} & \ldots & x _ {1 d} & 1  \\ 
      x _ {21} & x _ {22} & \ldots & x _ {2 d} & 1  \\ 
      \vdots & \vdots & \ddots & \vdots & \vdots  \\ 
      x _ {m 1} & x _ {m 2} & \ldots & x _ {m d} & 1
      \end{array}\right)=\left(\begin{array}{cc}
      \boldsymbol{x} _ {1}^{\mathrm{T}} & 1  \\ 
      \boldsymbol{x} _ {2}^{\mathrm{T}} & 1  \\ 
      \vdots & \vdots  \\ 
      \boldsymbol{x} _ {m}^{\mathrm{T}} & 1
      \end{array}\right) $$
    
    - 把标记写成向量形式：$\hat{\boldsymbol{w}}^{*}=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$

    - 令$E _ {\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})=\boldsymbol{y}^{\mathrm{T}} \boldsymbol{y}-\boldsymbol{y}^{\mathrm{T}} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{\mathrm{T}} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}+\hat{\boldsymbol{w}}^{\mathrm{T}} \mathbf{X}^{\mathrm{T}} \mathbf{X} \hat{\boldsymbol{w}}$

      - 对$E _ {\hat{\boldsymbol{w}}}$求导
  - 矩阵微分公式$\frac{\partial \boldsymbol{a}^{\mathrm{T}} \boldsymbol{x}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{x}^{\mathrm{T}} \boldsymbol{a}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{\mathrm{T}} \mathbf{A} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{A}+\mathbf{A}^{\mathrm{T}}\right) \boldsymbol{x}$
      - $\frac{\partial E _ {\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=0-X^Ty-X^Yy+(X^TX+X^TX)w \\  \frac{\partial E _ {\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})$
      
    - 令上式为0得到$\hat{\boldsymbol{w}}$的最优解的闭式解

      - 当$\mathbf{X} ^T\mathbf{X} $为**满秩矩阵/正定矩阵**时

        - 补充：

          - 满秩矩阵：设A是n阶矩阵, 若r（A） = n, 则称A为满秩矩阵。但满秩不局限于n阶矩阵。

            - 在线性代数中，一个矩阵A的列秩是A的线性独立的纵列的极大数目。类似地，行秩是A的线性无关的横行的极大数目。

            - 如下图

              <img src="/assets/MLpics/T24.png" style="zoom:50%;" />

          - 正定矩阵

            - 设M是n阶方阵，如果对任何非零向量z，都有$z^TMz>0$，就称M为正定矩阵。

        - 令求导式为0得到

          - $\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$
      - 令$\hat{\boldsymbol{x _ i}}=(x _ i,1)$，最终学得的多元线性回归模型为
        
          - $f(\hat{\boldsymbol{x _ i}})= \hat{\boldsymbol{x _ i}}^T\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$
    
  - 现实生活中$\mathbf{X} ^T\mathbf{X} $往往不是满秩矩阵，此时可以解出多个$\hat{\boldsymbol{w}}$，他们都能使均方误差最小化，选择哪一个解作为输出将由学习算法的归纳偏好决定
    
      - 常见做法为引入**正则化(regularization)**

- **广义线性模型**

  - 对数线性回归
    - 我们得到了线性回归模型之后，可否令模型预测值逼近$y$的衍生物
    - 譬如我们认为示例所对应的输出标记是在指数尺度上变化
      - $lny=w^Tx+b$
      - 虽然形式上仍然是线性回归，但实质上已经是在求取空间到输出空间的非线性函数映射
      - <img src="/assets/MLpics/T25.png" style="zoom:50%;" />
      - 这里对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用
  - 广义线性模型
    - 考虑单调可微函数$g(·)$（称之为联系函数）
    - $y=g^{-1}(w^Tx+b)$
    - 对数线性回归是广义线性模型在$g(·)=ln(·)$时的特例

## 对数几率回归

- 先决条件

  - 二分类任务
  - 输出标记为$y\in\{0,1\}$
  - 线性回归模型产生的预测值$z=w^Tx+b$是实际值
  - 我们需要将z转换为0/1值

- $y=\frac{1}{1+e^{-(w^Tx+b)}}$公式推导

  - 单位阶跃函数（unit-step function）$$ y=\left\{\begin{array}{cl}
  0, & z<0  \\ 
    0.5, & z=0  \\ 
    1, & z>0    \end{array}\right. $$
  
  - 若预测值z大于0就判为正例，小于0则判为反例，预测值为临界值0则可以任意判别

    <img src="/assets/MLpics/T26.png" style="zoom:50%;" />

  - 从图中可以看出，单位阶跃函数不连续，因此不能直接用作广义线性模型中的$g^-(·)$

    - 找出一个替代函数，并且单调可微---对数几率函数

      - $y=\frac{1}{1+e^{-z}}$

      - 对数几率函数是一个**Sigmoid函数**，将z转化成一个接近0或1的y值

      - 将其作为$g^-(·)$得到

        $y=\frac{1}{1+e^{-(w^Tx+b)}}$

      - 上式可变化为$ln{\frac{y}{1-y}}=w^Tx+b$

      - 将y视为样本x作为正例的可能性，1-y为其反例可能性，$\frac{y}{a-y}$称之为**几率（odds）**，将其取对数称之为**对数几率（log odds/logit）**

  - 虽然$y=\frac{1}{1+e^{-(w^Tx+b)}}$叫对数几率回归，但实际上却是一种分类学习方法
  - 优点
      - 对分类可能性进行建模，无需假设数据分布，避免了假设分布不准确所带来的问题
      - 不是仅预测出“类别”，而是可得到近似概率预测，适合许多需要利用概率辅助决策的任务
      - 对率函数是任意阶可导的凸函数，现有的许多数值优化算法可以直接用于求取最优解
  
- 确定$w,b$

  - 若将y视作**类后验概率估计**p(y=1 \vert x)，则$$\begin{aligned} ln{\frac{y}{1-y}}=&w^Tx+b  \\  ln\frac{p(y=1 \vert x)}{p(y=0 \vert x)}=&w^Tx+b  \\  p(y=1 \vert x)=&\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}  \\  p(y=0 \vert x)=&\frac{1}{1+e^{w^Tx+b}}\end{aligned}$$

    （最后两公式为3.23和3.24）

    - 通过极大似然法

      - 补充知识

        <img src="/assets/MLpics/T27.png" style="zoom:30%;" />

      - 给定数据集$\{(x _ i,y _ i)\}^m _ {i=1}$，对率回归模型最大化“对数似然”

        $\ell(\boldsymbol{w}, b)=\sum _ {i=1}^{m} \ln p\left(y _ {i} \mid \boldsymbol{x} _ {i} ; \boldsymbol{w}, b\right)$								(公式3.25)

      - 令每个样本属于其真实标记的概率越大越好

        - 令$\beta=(w;b),\hat{x}=(x;1)$，则$w^Tx+b=\beta^T\hat{x}$

        - $p _ 1(\hat{x};\beta)=p(y=1 \vert \hat{x};\beta)$

        - $p _ 0(\hat{x};\beta)=p(y=0 \vert \hat{x};\beta)=1-p(y=1 \vert \hat{x};\beta)$

        - 公式3.25的似然项为$$\begin{aligned}p(y _ i \vert x _ i;w,b)=&p(y=1 \vert \hat{x};\beta)+p(y=0 \vert \hat{x};\beta)  \\  p(y _ i \vert x _ i;w,b)=&y _ ip _ 1(\hat{x _ i};\beta)+(1-y _ i)p _ 0(\hat{x _ i};\beta)\end{aligned}$$

        - 将上式带入3.25

          - 注意$$\begin{aligned}p _ 1(\hat{x _ i};\beta)=&\frac{e^{\beta^T\hat{x _ i}}}{1+e^{\beta^T\hat{x _ i}}}  \\  p _ 0 (\hat{x _ i};\beta)=&\frac{1}{1+e^{\beta^T\hat{x _ i}}}\end{aligned}$$

          - 根据公式3.23和3.24：最大化似然函数等价于最小化似然函数的相反数（下式中倒数第二个推导出的公式为极大似然估计的似然函数）$$\begin{aligned}\ell(\beta)=& \sum _ {i=1}^mln(y _ ip _ i(\hat{x _ i};\beta)+(1-y _ i)p _ 0(\hat{x _ i};\beta))  \\  =&\sum _ {i=1}^mln(\frac{y _ ie^{\beta^T\hat{x _ i}}+1-y _ i}{1+e^{\beta^T\hat{x _ i}}})  \\  =&\sum _ {i=1}^{m}(ln(y _ ie^{\beta^T\hat{x _ i}}+1-y _ i)-ln(1+e^{\beta^T\hat{x _ i}}))  \\  \ell(\beta)=&\left\{\begin{array}{cl}
            \sum _ {i=1}^{m}(-ln(1+e^{\beta^T\hat{x _ i}})), & y _ i=0  \\ 
            \sum _ {i=1}^{m}(\beta^T\hat{x _ i}-ln(1+e^{\beta^T\hat{x _ i}})), & y _ i=1
            \end{array}\right.  \\  \ell(\beta)=&\sum _ {i=1}^{m}\left(y _ {i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}-\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}}\right)\right)  \\  \ell(\beta)=&\sum _ {i=1}^{m}\left(-y _ {i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}}\right)\right)\end{aligned} $$

          - 上式是关于$\beta$的高阶可导连续凸函数，根据凸优化，通过**梯度下降、牛顿法**都可求其最优解

            $\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}}{\arg \min } \ell(\boldsymbol{\beta})$

## 线性判别分析

- 线性判别分析（Linear Discriminant Analysis，简称LDA，亦称Fisher判别分析）

- 思想

  - 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离
  - 在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别
  - LDA与Fisher判别分析稍有胡同，前者假设了各类样本的协方差矩阵相同并且满秩

- 二维示意图

  <img src="/assets/MLpics/T28.png" style="zoom:50%;" />

- 公式推导

  - 给定数据集$D=\{(x _ i,y _ i)\}^m _ {i=1}$

  - 令$X _ i,\mu _ i,\sum _ i$分别表示第$i\in\{0,1\}$类示例的集合、均值向量、协方差矩阵

  - 若将数据投影到直线$w$上，则两类样本的中心在直线上的投影分别为$w^T\mu _ 0$和$w^T\mu _ 1$

  - 若将所有样本点都投影到直线上，则两类样本的协方差分别为$w^T\sum _ 0w$和$w^T\sum _ 1w$

    - 知识补充：

      - **协方差**：协方差是对两个随机变量联合分布线性相关程度的一种度量。两个随机变量越线性相关，协方差越大，完全线性无关，协方差为零。

      <img src="/assets/MLpics/T29.png" style="zoom:30%;" />

      - **样本协方差**：对于多维随机变量$Q(x1,x2,x3,…,xn)$，样本集合为$$x _ {ij}=[x _ {1j},x _ {2j},…,x _ {nj}] (j=1,2,…,m) $$，m为样本数量，在$ a,b（a,b=1,2…n）$两个维度内:

        $\operatorname{cov}\left(\mathrm{x} _ {\mathrm{a}}, \mathrm{x} _ {\mathrm{b}}\right)=\frac{\sum _ {j=1}^{m}\left(x _ {a j}-\bar{x} _ {a}\right)\left(x _ {b j}-\bar{x} _ {b}\right)}{m-1}$

      - **协方差矩阵**：对于多维随机变量$Q(x1,x2,x3,…,xn)$我们需要对任意两个变量$(xi,xj)$求线性关系，即需要对任意两个变量求协方差矩阵$$ \operatorname{cov}\left(x _ {i}, x _ {j}\right)=\left[\begin{array}{ccccc}
        \operatorname{cov}\left(x _ {1}, x _ {1}\right) & \operatorname{cov}\left(x _ {1}, x _ {2}\right) & \operatorname{cov}\left(x _ {1}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {1}, x _ {n}\right)  \\ 
        \operatorname{cov}\left(x _ {2}, x _ {1}\right) & \operatorname{cov}\left(x _ {2}, x _ {2}\right) & \operatorname{cov}\left(x _ {2}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {2}, x _ {n}\right)  \\ 
        \operatorname{cov}\left(x _ {3}, x _ {1}\right) & \operatorname{cov}\left(x _ {3}, x _ {2}\right) & \operatorname{cov}\left(x _ {3}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {3}, x _ {n}\right)  \\ 
        \vdots & \vdots & \vdots & \ddots & \vdots  \\ 
        \operatorname{cov}\left(x _ {m}, x _ {1}\right) & \operatorname{cov}\left(x _ {m}, x _ {2}\right) & \operatorname{cov}\left(x _ {m}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {m}, x _ {n}\right)
        \end{array}\right] $$

  - 欲最大化

    - 欲使同类样例的投影点尽可能接近，可以让同类样例点的协方差$w^T\sum _ 0w+w^T\sum _ 1w$尽可能小

    - 欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$\|w^T\mu _ 0-w^T\mu _ 1\| _ 2^2$

    - 同时考虑二者，则可以得到欲最大化的目标$$ \begin{aligned}
      J &=\frac{\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu}_{1}\right\|_{2}^{2}}{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{0} \boldsymbol{w}+\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma}_{1} \boldsymbol{w}} \\
      &=\frac{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)\left(\boldsymbol{\mu}_{0}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma}_{0}+\boldsymbol{\Sigma}_{1}\right) \boldsymbol{w}}
      \end{aligned} $$

    - 定义**类内散度矩阵**$$ S _ w=\sum _ 0+\sum _ 1 \\ \begin{aligned}=\sum _ {\boldsymbol{x} \in X _ {0}}\left(\boldsymbol{x}-\boldsymbol{\mu} _ {0}\right)\left(\boldsymbol{x}-\boldsymbol{\mu} _ {0}\right)^{\mathrm{T}}+\sum _ {\boldsymbol{x} \in X _ {1}}\left(\boldsymbol{x}-\boldsymbol{\mu} _ {1}\right)\left(\boldsymbol{x}-\boldsymbol{\mu} _ {1}\right)^{\mathrm{T}}\end{aligned}$$

    - 定义**类间散度矩阵**

      $S _ b=(\mu _ 0-\mu _ 1)(\mu _ 0-\mu _ 1)^T$

    - 则J可重写为

      $J=\frac{w^TS _ bw}{w^TS _ ww}$

    - LDA欲最大化的目标：$S _ b$和$S _ w$的**广义瑞利商（generalized Rayleigh quotient）**

  - 确定w

    - 由于J的解与w的长度无关，只与其方向有关，不失一般性，令$w^TS _ ww=1$，则J的重写式等价于$$\begin{array}{ll}
      \min  _ {\boldsymbol{w}} & -\boldsymbol{w}^{\mathrm{T}} \mathbf{S} _ {b} \boldsymbol{w}  \\ 
      \text { s.t. } & \boldsymbol{w}^{\mathrm{T}} \mathbf{S} _ {u} \boldsymbol{w}=1
      \end{array}$$

    - 由拉格朗日乘子法

      - 补充说明：拉格朗日乘数法是一种寻找变量受一个或多个条件所限制的多元函数的极值的方法。这种方法将一个有n 个变量与k 个约束条件的最优化问题转换为一个有n + k个变量的方程组的极值问题，其变量不受任何约束。这种方法引入了一种新的标量未知数，即拉格朗日乘数：约束方程的梯度的线性组合里每个向量的系数，从而找到能让设出的隐函数的微分为零的未知数的值。

        - 设给定二元函数z=ƒ(x,y)和附加条件φ(x,y)=0，为寻找z=ƒ(x,y)在附加条件下的极值点，先做拉格朗日函数 

          ![](/assets/MLpics/T30.png) 

          (其中λ为参数)

      - 推导：

        $L(w,\lambda)=-w^TS _ bw+\lambda(w^TS _ ww-1)$

        对$w$求偏导

        $\frac{\partial L(w,\lambda)}{\partial w}=-(S _ b+S _ b^T)w+\lambda (S _ w+S _ w^T)w$

        因为$S _ b=S _ b^T,S _ w=S _ w^T$

        $\frac{\partial L(w,\lambda)}{\partial w}=-2S _ bw+2\lambda S _ ww=0$

        $S _ bw=\lambda S _ ww$

        由于拉格朗乘子具体取值多少无所谓，我们像求解的只有$w$，所以我们可以任意设定$\lambda$来配合我们求解$w$

        如果我们令$\lambda=(\mu _ 0-\mu _ 1)^Tw$，那么$S _ bw=\lambda(\mu _ 0-\mu _ 1)$

        则$w=S _ w^{-1}(\mu _ 0-\mu _ 1)$

      - 考虑到数值解的稳定性，实际上是对$S _ w$做奇异值分解

        - 补充知识：**奇异值分解**
          - 假设M是一个m×n阶矩阵，其中的元素全部属于域 K，也就是实数域或复数域。如此则存在一个分解使得$M=U\sum V^T$
          - 其中U是m×m阶酉矩阵（$U^TU=UU^T=I _ A/U^T=U^{-1}$）；$\sum$是半正定m×n阶对角矩阵；$V^*$，(V的共轭转置)，是n×n阶酉矩阵。这样的分解就称作M的奇异值分解。Σ对角线上的元素Σi，其中Σi即为M的奇异值。
        - $S _ w=U\sum V^T$
        - $\sum$是一个实对角矩阵，其对角线上的元素是$S _ w$的奇异值
        - $S _ w^{-1}=V\sum^{-1}U^T$

  - LDA贝叶斯决策理论阐释

    - 可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类

  - LDA推广到多分类任务重

    - 假定存在N个类，且第i类示例数为$m _ i$

    - **全局散度矩阵**$$\begin{aligned}S _ t=&S _ b+S _ w  \\ =&\sum^{m} _ {i=1}(x _ i-\mu)(x _ i-\mu)^T\end{aligned}$$

      - 其中$\mu$是所有示例的均值向量，将类内散度矩阵$S _ w$重定义为每个类别的散度矩阵之和$$ \begin{aligned}S _ w=\sum^N _ {i=1}S _ {w _ i}\end{aligned}$$
      $$ \begin{aligned}S _ {w _ i}=\sum _ {\boldsymbol{x} \in X _ {i}}\left(\boldsymbol{x}-\boldsymbol{\mu} _ {i}\right)\left(\boldsymbol{x}-\boldsymbol{\mu} _ {i}\right)^T\end{aligned}$$
      $$ \begin{aligned}S _ b=&S _ t-S _ w  \\  =&\sum^{m} _ {i=1}(x _ i-\mu)(x _ i-\mu)^T-\sum^N _ {i=1}\sum _ {x \in X _ {i}}(x-\mu _ {i})(x-\mu _ {i})^T               \\ =&\sum^N _ {i=1}(\sum _ {x \in X _ {i}}((x-\mu)(x-\mu)^T-(x-\mu _ {i})(x-\mu _ {i})^T))  \\ =&\sum^N _ {i=1}(\sum _ {x \in X _ {i}}((x-\mu)(x^T-\mu^T)-(x-\mu _ {i})(x^T-\mu _ {i}^T)))                           \\ =& \sum^N _ {i=1}(\sum _ {x \in X _ {i}}(-\mu x^T-x\mu^T +\mu\mu^T+\mu _ i^Tx+\mu _ ix^T-\mu _ i\mu _ i^T))                          \\ =& \sum^N _ {i=1}(-m _ i\mu \mu _ i^T-m _ i\mu _ i\mu^T +m _ i\mu\mu^T+m _ i\mu _ i^T\mu _ i)      \\ =& \sum^N _ {i=1}m _ i(\mu _ i\mu _ i^T-\mu \mu _ i^T-\mu _ i\mu^T +\mu\mu^T) \\ =&\sum^N _ {i=1}m _ i(\mu _ i-\mu)(\mu _ i-\mu)^T\end{aligned}$$

    - 多分类LDA可以有多种实现方法

      - 采用优化目标$$\max  _ {\mathbf{W}} \frac{\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {b} \mathbf{W}\right)}{\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {w} \mathbf{W}\right)}$$

        - $\mathbf{W}=(w _ 1,w _ 2...w _ i,...w _ {N-1}) \in \mathbb{R}^{d \times(N-1)}$
        - $tr(·)$表示矩阵的迹，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和。$$ \left\{ \begin{aligned}
          \operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {b} \mathbf{W}\right) &=\sum_{i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {b} \boldsymbol{w} _ {i}  \\ 
          \operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {w} \mathbf{W}\right) &=\sum _ {i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {w} \boldsymbol{w}_{i}
          \end{aligned} \right. $$
        - 可以变形为$\max  _ {\mathbf{W}} \frac{\sum _ {i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {b} \boldsymbol{w} _ {i}}{\sum _ {i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {w} \boldsymbol{w} _ {i}}$
        - 为$J=\frac{w^TS _ bw}{w^TS _ ww}$的推广式
        - 求解：$S _ bW=\lambda S _ w W$
  
- 若将W视为一个投影矩阵
      
  - 则多分类$LDA$将样本投影到N-1维空间，N-1通常远小于数据原有的属性数
        - 可以通过这个投影来减小样本点的维数，且投影过程中使用了类别信息
        - LDA通常也被视为一种经典的监督降维技术

## 多分类学习

- 有些二分类学习方法可直接推广到多分类

- 二分类学习器

  - 考虑N个类别$C _ 1,C _ 2,...,C _ N$
  - 多分类学习器的基本思路是**拆解法**
    - 将多分类任务拆为若干个二分类任务求解
    - 先对问题进行拆分
    - 为拆出的每个二分类任务训练一个分类器
    - 测试时：对这些分类器的预测结果进行集成
    - 重点为**拆分和集成**

- 拆分策略

  - **“一对一”（OvO）**

    - 给定数据集$D=\{(x _ 1,y _ 1),(x _ 2,y _ 2),...,(x _ m,y _ m)\},y _ i \in \{C _ 1,C _ 2,...,C _ N\}$
    - 将这N个类别两两配对，从而产生N(N-1)/2个二分类任务
    - 例如OvO为区别$C _ i,C _ j$训练一个分类器
      - 该分类器把D中的$C _ i$类样例作为正例
      - 把D中的$C _ j$类样例作为反例
    - 测试阶段，新样本将同时提交给所有分类器，所以获得N(N-1)/2个分类结果
    - 被预测得最多的类别作为最终分类结果

  - **“一对其余”（OvR）**

    - 只训练N个分类器

    - 训练

      - 每次将一个类的样例作为正例
      - 所有其他类的样例作为反例

    - 测试时若仅有一个分类器预测为正类，对应的类别标记为最终分类结果

      <img src="/assets/MLpics/T31.png" style="zoom:33%;" />

    - 比起OvO的优缺点

      - 优点：存储开销和测试时间开销更小
      - 缺点：OvR的每个分类器均适用全部训练样例，而OvO仅需要用到两个类的样例
        - 在类别很多时，OvO的训练时间开销通常比OvR更小

  - **“多对多”（MvM）**

    - 每次将若干个类作为正类，若干个其他类作为反类

    - OvO和OvR是MvM的特例

    - MvM的正反类构造需要有特殊设计

      - 最常用的MvM技术**纠错输出码 （Error Correcting Output Codes，简称ECOC）**

        - 编码

          - 对N个类做M次划分，每次划分将一部分类别划为正类，一部分类划为反类，从而形成一个二分类训练集
          - 一共产生M个训练集，可以训练出M个分类器

        - 解码

          - M个分类器分别对测试样本进行预测，这些预测标记组成一个编码
          - 将这个预测编码与每个类别各自的编码进行比较
          - 最终预测结果：距离最小的类别

        - 类别划分：**”编码矩阵（coding matrix）**

          - 二元码

            - 将每个类别分别指定为正类和反类
            - 如下图，分类器$f _ 2$将$C _ 1$类和$C _ 3$类的样例作为正例，$C _ 2$类和$C _ 4$类的样例作为反例

          - 三元码

            - 在正、反类之外，还可以指定“停用类”
            - 如下图，分类器$f _ 6$将$C _ 1$类和$C _ 3$类的样例作为正例，$C _ 2$类的样例作为反例

            <img src="/assets/MLpics/T32.png" style="zoom:50%;" />

          - 在解码阶段

            - 各分类器的预测结果联合起来形成了测试示例的编码
            - 将距离最小的编码所对应的类别作为预测结果，如上图二元码结果为$C _ 3$

        - 修正

          - 在测试阶段，EOOC编码对分类器的错误有一定的容忍和修正能力
          - 如上图a中，如果$f _ 2$出错导致错误编码（-1，-1，+1，-1，+1），但结果仍然正确
          - 同一个学习任务，ECOC编码越长
            - 纠错能力越强
            - 所需训练的分类器越多，计算存储开销增大
            - 对有限类别数，可能的组合数目是有限的，码长超过一定范围后就失去了意义
          - 同等长度的编码
            - 任意两个类别之间的编码距离越远，纠错能力越强
            - NP难问题
              - 码长较小时可以根据上一条的原则计算出理论最优编码
              - 码长稍大就难以有效确定最优编码（NP难问题）
              - 但通常我们不需要获得理论最优编码

## 习题







