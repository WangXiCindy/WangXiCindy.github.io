---

title: 机器学习笔记
status: Writing
commentable: false
Edit: 2020-9-28
mathjax: true
mermaid: true
tags: Machine_Learning
categories: Machine_Learning
Typora-root-url: ../
description: 本文为周志华老师《机器学习》（西瓜书）的学习笔记，仅供参考，如有错误，欢迎给我发邮件或者通过友链进行留言。（详情请见About）
---

读前提示：
- 如果想查看各章节的单独笔记以及作业和相关代码、数据请到[这里](https://github.com/WangXiCindy/Machine_Learning_Notes)查看

- 本文为周志华老师《机器学习》（西瓜书）的学习笔记，仅供参考，如有错误，欢迎给我发邮件或者通过友链进行留言。（详情请见About）

- 本文尚在更新中，可通过左上角的目录索引到各章或回到主页

- 笔者能力有限，本文仅供参考

  

# 绪论

## 基本术语

- 数据

  - 数据集：记录的集合

  - 示例：数据集中每条记录--关于一个事件/对象的描述

  - 属性/特征：反映事件或对象在某方面的表现或性质的事项

  - 属性/样本/输入空间：属性张成的空间

  - 特征向量：属性空间中的每一个点对应一个坐标向量---一个示例

    公式说明$$ D=\left\{ x _ {1},x _ {2},...,x _ {m} \right\} $$

    其中D代表数据集，xi代表示例$$x _ {i}=(x _ {i1};x _ {i2};...x _ {in})$$

- 学习/训练：从数据中学得模型的过程

  - 训练数据

  - 训练样本

  - 训练集

  - 假设：学得模型对应了关于数据的某种潜在的规律

  - 真相：潜在规律

  - 标签：一个示例的具体结果说明

  - 标记：结果信息（当然也存在标记空间）

  - 样例：拥有了标记信息的示例

    样例：$(x _ {i},y _ {i})$ $y _ {i}$是标记

  - 预测（监督学习）

    - 分类：预测的是离散值
      - 二分类(样本空间为$\{-1,+1\}$或$\{0,1\}$)
      - 多分类（样本空间的模>2）
    - 回归：预测的是连续值

  - 测试：学得模型后进行预测的过程

    - 测试样本：被预测的样本

  - 聚类（非监督学习）：有助于帮助我们发掘数据内在规律

  - 泛化：从样本很小空间得到的模型可以很好的适用于整个样本空间

    - 归纳：特殊到一般的泛化
    - 演绎：一般到特殊的特化

- 假设空间

  - 归纳学习

    - 狭义：从训练数据中学得概念，亦称概念学习/概念形成

      - 布尔概念学习：0/1

      案例：假设判断是否是好瓜有三种属性：

      色泽（3）	根蒂（2）	敲声（2）

      假设空间规模大小就是4x3x3+1=37

      （之所以为4x3x3是要考虑可能啥都行的情况，+1是因为要考虑可能根本没有好瓜的情况）

    - 广义：从样例中学习

  - 版本空间：可能有多个假设和训练集一致，这个假设集合就被称为这个训练集对应的版本空间

- 归纳偏好：在面临多个假设的时候无法判断哪一个更好，这时算法偏好就会起到作用

  - 引导算法

    - **奥卡姆剃刀**（Occam's razor）：若有多个假设与观察一致，则选最简单的那个（but简单的诠释也往往不同）

  - $\mathfrak{L} _ {a}$代表一种学习算法，对于不同的学习算法$\mathfrak{L} _ {a}$公和$\mathfrak{L} _ {b}$它们在不同的情况下会展现出不同的优势和缺陷

  - $\mathfrak{L} _ {a}$在训练集之外的所有样本上的误差为：

    $E _ {\text {ote}}\left(\mathfrak{L} _ {a} \mid X, f\right)=\sum _ {h} \sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h \mid X, \mathfrak{L} _ {a}\right)$	（1.1）
  
    - $\mathbb{I}(·)$是指示函数，内部为真取1，否则取0
    - X为训练数据
    - $\mathcal{X}$为样本空间（离散）
    - $\mathcal{H}$为假设空间（离散）
    - $P\left(h \mid X, \mathfrak{L} _ {a}\right)$代表算法基于训练数据X产生假设h的概率
    - $f$我们希望学习的真实目标函数
    - 个人公式理解：
      - $P(x)$表示在训练数据以外（样本空间之内）该x出现的概率。
    - 在某一种假设和训练数据条件下，$\mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h \mid X, \mathfrak{L} _ {a}\right)$为基于该训练数据关于x的预测结果和实际结果相同的概率
  
  - 考虑到二分类问题，且真实目标函数可以是任何函数，对所有可能的f按均匀分布对误差求和
    
    $$\begin{aligned}
    \sum _ {f} E _ {\text {ote}}\left(\mathfrak{L} _ {a} \mid X, f\right) &=\sum _ {f} \sum _ {h} \sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h \mid X, \mathfrak{L} _ {a}\right)  \\ 
    &=\sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum _ {h} P\left(h \mid X, \mathfrak{L} _ {a}\right) \sum _ {f} \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) \qquad   \qquad   \qquad          (1.2) \\ 
    &=\sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum _ {h} P\left(h \mid X, \mathfrak{L} _ {a}\right) \frac{1}{2} 2^{ \vert \mathcal{X} \vert }  \\ 
  &=\frac{1}{2} 2^{ \vert \mathcal{X} \vert } \sum _ {\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \sum _ {h} P\left(h \mid X, \mathfrak{L} _ {a}\right)
    \end{aligned}$$
  
  南瓜书具体说明：
  
    - 第1步到第2步
      - $\sum _ {i}^{m} \sum _ {j}^{n} \sum _ {k}^{o} a _ {i} b _ {j} c _ {k}=\sum _ {i}^{m} a _ {i} \cdot \sum _ {j}^{n} b _ {j} \cdot \sum _ {k}^{o} c _ {k}$
    - 第2步到第3步
      - 对于f的假设是任何能将样本映射到0，1的函数并且服从均匀分布。不止一个f且f出现的概率相等
      - 举个栗子：样本空间只有两个样本时($ \vert \mathcal{X} \vert =2$)，f的可能性$2^{ \vert \mathcal{X} \vert }$有四种，比如：$f _ {1}(x _ {1})=0,f _ {1}(x _ {2})=0;$
      - 所以通过$\mathfrak{L}  _ {a}$学习出的模型h(x)对 *每个样本* 无论是预测值为0还是1必然有一半的f与其预测值相等，例如$h(x _ {1})=1$则必有两个$f _ {n}(x _ {1})=1$
      - 所以说$\sum _ {f} \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x}))=\frac{1}{2}2^{ \vert \mathcal{X} \vert }$
  - 最终结果：总误差和学习算法无关（NFL**“没有免费的午餐”** 定理） 
  
- NFL定理的bug前提：所有问题出现的机会相同/所有问题同等重要，但现实并非如此。
  
    - 一般来说我们只需要在某个具体的应用任务上找到一个解决方案
    - 瓜瓜🍉栗子：我们对好瓜会有一种评判标准，比如卖瓜的时候会喜欢买色泽青绿，根蒂蜷缩，敲声浊响的瓜，那么这种好瓜会更为常见，而根蒂硬挺，敲声清脆的好瓜罕见甚至不存在

## 部分习题解释

- 表格

  ![T1](/assets/MLpics/T1.png)

- 1.1	版本空间

  - 好瓜{色泽=青绿，根蒂=蜷缩，敲声=浊响}	坏瓜{色泽=乌黑，根蒂=稍蜷，敲声=沉闷}
  - 好瓜{色泽=青绿，根蒂=\*，敲声=\*}	坏瓜{色泽=乌黑，根蒂=\*，敲声=\*}
  - 好瓜{色泽=\*，根蒂=蜷缩，敲声=\*}	坏瓜{色泽=\*，根蒂=稍蜷，敲声=\*}
  - 好瓜{色泽=\*，根蒂=\*，敲声=浊响}	坏瓜{色泽=\*，根蒂=\*，敲声=沉闷}
  - 好瓜{色泽=\*，根蒂=蜷缩，敲声=浊响}	坏瓜{色泽=\*，根蒂=稍蜷，敲声=沉闷}
  - 好瓜{色泽=青绿，根蒂=\*，敲声=浊响}	坏瓜{色泽=乌黑，根蒂=\*，敲声=沉闷}
  - 好瓜{色泽=青绿，根蒂=蜷缩，敲声=\*}	坏瓜{色泽=乌黑，根蒂=稍蜷，敲声=\*}

- 1.2

  - 包含3种属性，假设空间大小为3x4x4+1=49
  - 考虑冗余（A=a与A=*等价
    - 具体的假设：2x3x3=18
    - 1属性泛化：2x3+3x3+2x3=21
    - 2属性泛化：2+3+3=8
    - 3属性泛化：1
    - 不考虑冗余/空集：kmax=48
    - 考虑冗余：kmax=18

- 1.3

  - 归纳偏好选择：一致性比例越高越好
    - 两个数据属性越接近，则分为同一类
    - 若相同属性出现了两种不同的分类
      - 则认为它属于与他最邻近的几个数据的属性
      - 同时去掉所有具有相同属性而不同分类的数据，可能会丢失部分信息

# 模型评估与选择

## 经验误差与过拟合

- 概念
  - 错误率：分类错误的样本数占样本总数的比例
  - 精度：1-错误率
  - 误差：学习器的实际预测输出与样本的真实输出之间的差异
    - **训练/经验误差**：学习器的实际预测输出与样本的真实输出
    - **泛化误差**：在新样本上的误差
      - 如何获得更好的泛化误差：尽可能在训练样本中学出适用于所有潜在样本的“普遍规律”
      - 过拟合：把训练样本学习“太好”，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质。原因：学习能力过于强大。无法避免，只能缓解。
      - 欠拟合：对训练样本的一般性质尚未学好。原因：学习能力低下。方法：决策树中扩展分支，神经网络中增加训练轮数等。

## 评估方法

- 测试集：测试学习器对新样本的判别能力
  - 概念
    - **测试误差**：泛化误差的近似
    - 尽可能与训练集互斥（测试样本尽量不在训练集中出现）
  - 常见得到测试集做法
    - **留出法**：将数据集D划分为两个互斥的集合，一个集合作为训练集S，另一个作为测试集T
      - 训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程中引入额外的偏差而对最终结果产生影响
      - 分层采样：保留类别比例的采样方式
      - 在给定样本比例后，仍然存在多种划分方式对数据集进行分割，比如是前350个放入训练集还是后350个
      - 所以使用留出法时，一般要采用若干次随机划分，重复进行实验评估，最后取平均值
    - **交叉验证法**（k折交叉验证）
      - 将数据集D划分为k个大小相似的互斥子集，也存在多种划分方式，一般要随机使用不同的划分重复p次，最终评估结果是这p次k折交叉验证结果的均值
      - 每次用k-1个子集的并集作为训练集，余下的子集作为测试集
      - 可以进行k次训练和测试，最后返回这k个测试结果的均值
      - k最常用的取值是10/5/20
      - 特例：**留一法**，D中包含m个样本，k=m
        - m个样本只有唯一的方式划分为m个子集
        - 优点：与用D训练出来的模型很相似，往往被认为较为准确
        - 缺点：
          - 在数据集比较大时，训练m个模型的计算开销可能是难以忍受的
          - 没有免费的午餐仍然适用
      - **自助法**：为了减少训练样本规模不同造成的影响
        - 以自助采样法为基础
        - 每次从D中随机挑选一个样本，将其拷贝放入D‘，然后再将该样本放回D，使得下一次采样时仍有可能被采到
        - 样本在m次采样中始终不被采到的概率取极限$\lim  _ {m \mapsto \infty}\left(1-\frac{1}{m}\right)^{m} \mapsto \frac{1}{e} \approx 0.368$
        - 我们说D中约有36.8%的样本未出现在D‘中，D\D‘即可作为测试集
        - 这样的测试结果，成为：包外估计“
        - 优点：
          - 在数据集较小，难以有效划分训练/测试集时很有用
          - 能从初始数据集中产生多个不同的训练集，对集成学习等有较大好处
        - 缺点：改变了初始数据集分布，会引入估计偏差，所以数据量足够时，留出法和交叉验证更为常用
      - 调参与最终模型
        - 学习算法的很多参数是在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的
        - 常用做法：选定范围和变化步长
          - 非最佳，但是在计算开销和性能估计之间折中的结果
        - 最终模型！！！：一开始只适用一部分数据训练模型，所以在模型选择完成之后，需要将D重新训练模型

## 性能度量

- 概念：衡量模型泛化能力的评价标准

  - **均方误差**：回归任务最常用

    - $E(f ; D)=\frac{1}{m} \sum _ {i=1}^{m}\left(f\left(\boldsymbol{x} _ {i}\right)-y _ {i}\right)^{2}$

    - 对于数据分布$\mathcal{D}$和概率密度函数$p(·)$，均方误差

      $E(f ; \mathcal{D})=\int _ {\boldsymbol{x} \sim \mathcal{D}}(f(\boldsymbol{x})-y)^{2} p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$

    - 几何意义：对应了欧式距离

      - 基于其最小化来进行模型求解的方法为最小二乘法
      - 最小二乘法就是找到一条直线，使所有样本到直线上的欧式距离和最小

- 错误率与精度

  - 错误率：

    - $E(f ; D)=\frac{1}{m} \sum _ {i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right)$

    - 对于数据分布$\mathcal{D}$和概率密度函数$p(·)$，

      $E(f ; \mathcal{D})=\int _ {\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$

  - 精度
    
      $ \begin{aligned} \operatorname{acc}(f ; D) =\frac{1}{m} \sum _ {i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right)=y _ {i}\right) =1-E(f ; D) \end{aligned} $
      
  - 对于数据分布$\mathcal{D}$和概率密度函数$p(·)$，
    
      $\begin{aligned}
      \operatorname{acc}(f ; \mathcal{D})=\int _ {\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x})=y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
      =1-E(f ; \mathcal{D})
    \end{aligned}$
    
  - 查准率，查全率（两者互相矛盾）与F1

    - **查准率**$P=\frac{T P}{T P+F P}$（在预测结果为正例的情况下预测正确的概率）

    - **查全率**$R=\frac{T P}{T P+F N}$(在真实情况为正例的情况下预测正确的概率)

    - T/F判断是否正确

      P/N预测结果是否是正例

      以下为**混淆矩阵**：

    - | 真实情况\预测结果 | 正例         | 反例         |
      | ----------------- | ------------ | ------------ |
      | 正例              | TP（真正例） | FN（假反例） |
      | 反例              | FP（假正例） | TN（真反例） |

      T/F判断是否正确

      P/N预测结果是否是正例

    - 矛盾度量

      - 查准率高，查全率低
      - 比如：为了高查全率，将所有西瓜都选上，所有的好瓜也都必然被选上了，但这样查准率就低
      - 查准率高，只挑选最有把握的瓜，但可能就会漏掉不少好瓜

    - P-R曲线与**平衡点**

      - 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则后者的性能优与前者（比如A优于C）
      - 如果两个学习器的P-R曲线发生交叉，则具体情况具体分析。此时，可以比较P-R曲线下面积的大小，这表征了学习器在查准率和查全率上取得相对较好的比例

    <img src="/assets/MLpics/T2.png" alt="图片2" style="zoom:60%;" />

    - ​	平衡点（BEP）

      - 综合考虑查准率，查全率的性能度量
      - 查准率=查全率的取值
      - 平衡点越高，学习器性能越好

    - **F1**度量（调和平均）

      - BEP较为简化

      - 与算术平均和几何平均相比，调和平均更重视较小值

      - $F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}$

      - 表达出对查准率/查全率的不同偏好：F1度量的一般形式（加权调和平均）

        $F _ {\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}$

        - $\beta>0$度量了查全率对查准率的相对重要性
          - $\beta>1$对查全率有更大影响
          - $\beta=1$退化为标准的F1
          - $\beta<1$对查准率有更大影响

    - 多个二分类混淆矩阵综合考察

      - 方法1:先在各混淆矩阵上分别计算出查准率和查全率，记为(P1,R1),(P2,R2),...,(Pn,Rn)，再计算平均值

        - 宏查准率

          $\operatorname{macro}-P=\frac{1}{n} \sum _ {i=1}^{n} P _ {i}$

          - 宏查全率

          $\operatorname{macro}-R=\frac{1}{n} \sum _ {i=1}^{n} R _ {i}$

          - 宏F1

          $\operatorname{macro}-F 1=\frac{2 \times \operatorname{macro}-P \times \operatorname{macro}-R}{\operatorname{macro}-P+\operatorname{macro}-R}$

      - 对各混淆矩阵的对应元素进行平均，得到TP，FP，
        TN，FN的平均值，再计算出

        - 微查准率

        $\operatorname{micro}-P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}$

        - 微查全率

        $\operatorname{micro}-R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}$

        - 微F1

        $\operatorname{micro}-F 1=\frac{2 \times \operatorname{micro}-P \times \operatorname{micro}-R}{\operatorname{micro}-P+\operatorname{micro}-R}$

- **ROC与AUC**

  - 很多学习器是为样本产生一个实值或概率预测，然后将这个预测值与一个分类**阈值（threshold）**比较，大于阈值为正类，否则为反。

  - 这个实值预测结果的好坏，直接决定了学习器的泛化能力。根据这个实值或概率预测结果，将测试样本进行排序。

  - 分类过程=以某个截断点将样本分为两部分，前一部分判定为正例，后部分为反例

    - 更重视查准率：选择排序靠前的位置进行截断
    - 更重视查全率：选择靠后的位置进行截断

  - ROC：研究学习器泛化性能的工具

    - 根据预测结果对样例进行排序，按此排序逐个把样本作为正例进行预测，每次计算出两个量的值，分别以他们为横纵坐标作图

      - 真正确率（纵轴）

      $\mathrm{TPR}=\frac{T P}{T P+F N}$

      - 假正例率（横轴）

      $\mathrm{FPR}=\frac{F P}{T N+F P}$

      - <img src="/assets/MLpics/T3.png" style="zoom:50%;" />
      - 曲线说明
        - 有限个坐标对时，无法产生光滑曲线
        - 先把分类阈值设为最大，所有样例均为反例，此时坐标为（0，0）
        - 依次将每个样例划分为正例
        - 在统计预测结果时，预测值=分类阈值的样本也算作预测为正例

    - AUC：如果两个ROC曲线发生交叉，则难以一般性断言两者谁优谁劣，此时比较ROC曲线的面积=AUC

      $\mathrm{AUC}=\frac{1}{2} \sum _ {i=1}^{m-1}\left(x _ {i+1}-x _ {i}\right) \cdot\left(y _ {i}+y _ {i+1}\right)$

      - 🎃书案例：

        假设有7个预测结果（不做详细说明），画出ROC曲线，将增加面积看成梯形来计算，（梯形和长方形的面积公式相同）

        **梯形面积**$=\frac{1}{2} \left(x _ {i+1}-x _ {i}\right) \cdot\left(y _ {i}+y _ {i+1}\right)$

        <img src="/assets/MLpics/T4.png" style="zoom:50%;" />

      - 给$m^{+}$个正例和$m^{-}$个反例，令$D^{+}$和$D^{-}$分别表示正、反例集合，则loss为

        $\ell _ {r a n k}=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)$

      - 如果我们对上面的式子继续做变形，可以得到：
      
      $$\begin{aligned} \ell _ {rank} &=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)  \\  &=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}}\left[\sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \cdot \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right]  \\ &=\sum _ {x^{+} \in D^{+}}\left[\frac{1}{m^{+}} \cdot \frac{1}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \cdot \frac{1}{m^{+}} \cdot \frac{1}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right]  \\ &=\sum _ {x^{+} \in D^{+}} \frac{1}{2} \cdot \frac{1}{m^{+}} \cdot\left[\frac{2}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right]
        \end{aligned}$$
  
      - 因为新增正例就新增一条蓝色/绿色线段，所以$\sum _ {x^{+} \in D^{+}}$是在遍历所有蓝色和绿色线段

        - 后面那一项是在求绿色线段/蓝色线段与y轴围成的面积

          $\frac{1}{2} \cdot \frac{1}{m^{+}} \cdot\left[\frac{2}{m^{-}} \sum _ {x^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)<f\left(\boldsymbol{x}^{-}\right)\right)+\frac{1}{m^{-}} \sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)=f\left(\boldsymbol{x}^{-}\right)\right)\right]$

          - $\frac{1}{m^{+}} $为梯形的高

          - 梯形的上底，每增加一个假正例时x坐标就新增一个单位，实则为目前预测值$x^{+}$大的假正例个数乘$\frac{1}{m^{-}}$

            $\frac{1}{m^{-}} \sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)<f\left(\boldsymbol{x}^{-}\right)\right)$

          - 梯形的下底，每增加一个假正例时x坐标就新增一个单位，实则为目前预测值$x^{+}$大/等的假正例个数乘$\frac{1}{m^{-}}$

            $\frac{1}{m^{-}}\left(\sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)<f\left(\boldsymbol{x}^{-}\right)\right)+\sum _ {\boldsymbol{x}^{-} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x}^{+}\right)=f\left(\boldsymbol{x}^{-}\right)\right)\right)$
    
          - $\mathrm{AUC}\approx1-\ell _ {\text {rank}}$

  - **代价敏感错误率**与**代价曲线**

    - 情况：错误的把患者检测为健康（必须避免发生）/把健康的人检测为患者（增加了一次进一步检查的麻烦）

    - **代价矩阵**$cost _ {ij}$代表将第i类样本预测为第j类样本的代价
  
      | 真实类别\预测类别 | 0           | 1           |
      | ----------------- | ----------- | ----------- |
    | 0                 | 0           | $cost _ {01}$ |
      | 1                 | $cost _ {10}$ | 0           |

    - 之前的性能度量大多隐式的假设了均等代价，并没有考虑不同错误会造成不同的后果

    - 在非均等代价下，我们希望最小化“总体代价”而非“错误次数”
  
    - 将第0类作为正类，第1类作为反类，$D^{+}\text{与}D^{-}$分别代表D的正例子集和反例子集，则代价敏感错误率
      
    
    $$\begin{aligned}
  E(f ; D ; \cos t)=& \frac{1}{m}\left(\sum _ {\boldsymbol{x} _ {i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right) \times \operatorname{cost} _ {01}
    +\sum _ {\boldsymbol{x} _ {i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right) \times \operatorname{cost} _ {10}\right)
    \end{aligned}$$
  
- 在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，但“代价曲线”可达到该目的
  
  - 横轴：取值为[0,1]的正例概率代价
    
    $P(+) \cos t=\frac{p \times \cos t _ {01}}{p \times \cos t _ {01}+(1-p) \times \cos t _ {10}}$
    
  - 纵轴：取值为[0,1]的归一化代价
    
    $\operatorname{cost} _ {n o r m}=\frac{\operatorname{FNR} \times p \times \cos t _ {01}+\mathrm{FPR} \times(1-p) \times \operatorname{cost} _ {10}}{p \times \operatorname{cost} _ {01}+(1-p) \times \operatorname{cost} _ {10}}$
    
      FPR是假正例率，FNR=1-TPR是假反例率
    
      - ROC曲线上的每一点对应了代价平面上的一条线段，设ROC曲线上点的坐标为（TPR，FPR），则可相应计算出FNR，然后绘制一条从（0，FPR）到（1，FNR）的线段
    
      - 线段下的面积代表该条件下的期望总体代价

## 比较检验

- **假设检验**

  - 假设：对学习器泛化错误率分布的某种判断或猜想

    - 现实生活中我们并不知道学习器的泛化错误率，只能知道其测试错误率
    - 泛化错误率与测试错误率不一定相同，但二者接近的可能性较大

  - 泛化错误率为$\epsilon$的学习器在一个样本上犯错的概率是$\epsilon$

  - 测试错误率$\hat{\epsilon}$意味着在m个测试样本中恰有$\hat{\epsilon}\times m$被错误分类

    - 假设测试样本是从样本分布中独立采样获得，泛化错误率为$\epsilon$的学习器恰好将其中$m'$个样本误分类的概率$\epsilon^{m^{\prime}}(1-\epsilon)^{m-m^{\prime}}$

    - 恰好将$\hat{\epsilon}\times m$个样本误分类的概率/泛化错误率为$\epsilon$的学习器被测试得测试错误率为$\hat{\epsilon}$的概率$$ P(\hat{\epsilon} ; \epsilon)=\left(\begin{array}{c} m  \\ \hat{\epsilon} \times m  \end{array}\right) \epsilon^{\hat{\epsilon} \times m}(1-\epsilon)^{m-\hat{\epsilon} \times m}$$

    - 给定$\hat{\epsilon}$，则

      ​	<img src="/assets/MLpics/T5.png" style="zoom:50%;" />

      - $\partial P(\hat{\epsilon} ; \epsilon) / \partial \epsilon=0$
      - $P(\hat{\epsilon} ; \epsilon)$在$\hat{\epsilon}=\epsilon$时最大，如图，若$\epsilon=0.3$则10个样本中测得3个被误分类的概率最大
      - $ \vert \hat{\epsilon}-\epsilon \vert $增大时减小
      - 符合二项分布，注意$$\left(\begin{array}{c}
        m  \\ 
        \hat{\epsilon} \times m
        \end{array}\right) $$

    - **二项检验**

      - 对$\epsilon \leq \epsilon _ {0}$进行假设检验（单侧）

      - 求解最大错误率$\bar{\epsilon}$

      - $ 1-\alpha$为概率学习中的置信度，如0.95，0.90

      - 计算事件最小发生频率到最大发生频率的概率和现$\epsilon _ {0}$的情况做对比，从而得到检验结果

      - $\frac{\bar{C}}{m}$为事件最大发生频率$$\bar{C}=\min C \quad \text { s.t. } \sum _ {i=C+1}^{m}\left(\begin{array}{c}
        m  \\ 
        i
        \end{array}\right) p _ {0}^{i}\left(1-p _ {0}\right)^{m-i}<\alpha$$
        $$\frac{\bar{C}}{m}=\min \frac{C}{m} \quad \text { s.t. } \quad \sum _ {i=C+1}^{m}\left(\begin{array}{c}
        m  \\ 
        i
        \end{array}\right) p _ {0}^{i}\left(1-p _ {0}\right)^{m-i}<\alpha$$

        将$\frac{\bar{C}}{m}, \frac{C}{m}, p _ {0}$等价替换为$\bar{\epsilon}, \epsilon, \epsilon _ {0}$
        $$\bar{\epsilon}=\min \epsilon \quad \text { s.t. } \quad \sum _ {i=\epsilon \times m+1}^{m}\left(\begin{array}{c}
        m  \\ 
        i
        \end{array}\right) \epsilon _ {0}^{i}\left(1-\epsilon _ {0}\right)^{m-i}<\alpha$$

      - 若在$\alpha$的显著度下，$\epsilon \leq \epsilon _ {0}$不能被拒绝，那么就能以$1-\alpha$的置信度认为，学习器的泛化错误率不大于$\epsilon _ {0}$

    - **t检验**（大学概率学习内容）

      - 经过多次重复留出法/交叉验证法进行多次训练/测试，会得到多个测试错误率

      - 注意误分类样本数服从二项分布，大量样本之后，根据**中心极限定理**，所以大量二项分布（极限）服从正态

        <img src="/assets/MLpics/T13.png" style="zoom:50%;" />

      - 假设我们得到了k个测试错误率$\hat\epsilon _ {1}, \hat{\epsilon} _ {2}, \ldots, \hat{\epsilon} _ {k}$，则

        - $\mu=\frac{1}{k} \sum _ {i=1}^{k} \hat{\epsilon} _ {i}$
        - $\sigma^{2}=\frac{1}{k-1} \sum _ {i=1}^{k}\left(\hat{\epsilon} _ {i}-\mu\right)^{2}$
        - 则$\tau _ {t}=\frac{\sqrt{k}\left(\mu-\epsilon _ {0}\right)}{\sigma}$

      - 大学概率补充知识：

        <img src="/assets/MLpics/T6.png" style="zoom:50%;" />

        <img src="/assets/MLpics/T8.png" style="zoom:50%;" />

        - t假设检验<img src="/assets/MLpics/T7.png" style="zoom:50%;" />

        - 得到服从自由度为k-1的t分布

          <img src="/assets/MLpics/T9.png" style="zoom:50%;" />

        - 对假设$\mu=\epsilon$和显著度$\alpha$，我们可以计算出当测试错误率均值为$\epsilon _ {0}$时，在$1-\alpha$概率内能观测到的最大错误率

        - 假设检验（两侧）

        - $ \vert \mu-\epsilon _ {0} \vert $在$[t _ {-\alpha/2},t _ {\alpha/2}]$内，为接受域

- **交叉验证t检验**

  - 对两个学习器A和B，使用k折交叉验证法得到的测试错误率分别为$\epsilon _ {1}^{A},\epsilon _ {2}^{A}...\epsilon _ {k}^{A}$和$\epsilon _ {1}^{B},\epsilon _ {2}^{B}...\epsilon _ {k}^{B}$，其中$\epsilon _ {i}^{A}$和$\epsilon _ {i}^{B}$是在相同的第i折训练/测试集上得到的结果

  - 如果两个学习器的性能相同，则它们使用相同的训练/测试集得到的测试错误率应相同

  - 先对每对结果求差$\Delta i=\epsilon _ {i}^{A}-\epsilon _ {i}^{B}$

    - 若两个学习器性能相同，则差值均值应该为0

    - 应该根据差值$\Delta 1,\Delta 2,...\Delta k$来对学习器A与B性能相同这个假设做t检验

    - 计算差值的均值$\mu$和方差$\sigma^{2}$

    - $\tau _ {t}= \vert \frac{\sqrt{k}\mu}{\sigma} \vert $

    - 小于$t _ {\alpha/2}(k-1)$，为接受域，两个学习器的性能没有显著差别

    - 大学概率补充知识

      <img src="/assets/MLpics/T10.png" style="zoom:50%;" />

  - 假设检验的重要前提：测试错误率均为泛化错误率的独立采样

    - 但通常情况下由于样本有限，所以在使用交叉验证时，不同轮次的训练集会有一定程度的重叠
    - 过高估计假设成立的概率
    - 采用5x2交叉验证
      - 做5次2折交叉验证
      - 在每次2折交叉验证之前随机将数据打乱，使得5次交叉验证中的数据划分不重复
      - 对两个学习器A，B，第i次2折交叉验证将产生两对错误测试率，对其分别求差，得到第1折上的差值$\Delta _ {i}^{1}$和第2折上的差值$\Delta _ {i}^{2}$
      - 为缓解测试错误率的非独立性，仅计算第1次2折交叉验证的两个结果的平均值$\mu=0.5(\Delta _ {1}^{1}+\Delta _ {1}^{2})$
      - 对每次2折实验的结果都计算其方差$\sigma _ {i}^{2}=(\Delta _ {i}^{1}-\frac{\Delta _ {1}^{1}+\Delta _ {1}^{2}}{2})^{2}+(\Delta _ {i}^{2}-\frac{\Delta _ {1}^{1}+\Delta _ {1}^{2}}{2})^{2}$
      - $\tau _ {t}=\frac{\mu}{\sqrt{0.2}\sum _ {i=1}^{5}\sigma _ {i}^{2}}$服从自由度为5的t分布

- **McNemar检验**

  - **列联表（contingency table）**是观测数据按两个或更多属性（定性变量）分类时所列出的频数表。它是由两个以上的变量进行交叉分类的频数分布表。

  - <img src="/assets/MLpics/T11.png" style="zoom:50%;" />

  - 若假设为两学习器性能相同，则应有$e _ {01}=e _ {10}$（实则为概率$p _ {e _ {01}}=p _ {e _ {10}}$）

  - 根据ALLEN L. EDWARDS的论文

    “NOTE ON THE "CORRECTION FOR CONTINUITY" IN TESTING THE SIGNIFICANCE OF THE DIFFERENCE BETWEEN CORRELATED PROPORTIONS ”

    <img src="/assets/MLpics/T15.png" style="zoom:40%;" />

  - 通过卡方分布进行评估，可将上表的$e _ {01}$与$e _ {10}$两个频率中较小的一个加上0.5、较大的一个减去0.5来进行**连续性校正**。

  - 那么变量$ \vert e _ {01}-e _ {10} \vert $应该服从正态分布，均值为1，方差为$e _ {01}+e _ {10}$

  - 变量$\tau _ {\chi}^{2}=\frac{( \vert e _ {01}-e _ {10} \vert -1)^{2}}{e _ {01}+e _ {10}}$

  - 服从自由度为1的**$\chi^{2}$分布**

  - 大学概率补充：

    <img src="/assets/MLpics/T12.png" style="zoom:50%;" />

- **Friedman检验**与Nemenyi后续检验

  - 当有多个算法参与比较时

    - 在每个数据集上分别列出两两比较的结果

      - 假定我们用$D _ {1},D _ {2},D _ {3},D _ {4}$四个数据集对ABC进行比较

      - 使用留出法/交叉验证法得到每个算法在每个数据集上的测试结果

      - 在每个数据集上根据测试性能由好到坏排序，并赋予序值

      - 若算法测试性能相同，则平分序值

        <img src="/assets/MLpics/T14.png" style="zoom:50%;" />

    - 使用基于算法排序的Friedman检验

      - 判断这些算法是否性能都相同（根据平均序值）

      - 我们在N个数据集上比较k个算法

        - $r _ {i}$表示第i个算法的平均序值

        - 暂时不考虑平分序值的情况

        - $r _ {i}$服从正态分布

          - 均值：$sum=k(k+1)/2$ 所以$\mu=sum/k=(k+1)/2$
        
          - 方差  
            
            $$ \begin{aligned} \delta^2 &=[(1-(k+1)/2)^{2}+(2-(k+1)/2)^{2}+...+(k-(k+1)/2)^{2}]/k \\ &=[(2-k-1)^{2}/4+(4-k-1)^{2}/4+...+(2k-k-1)^{2}/4]/k  \\ &= (4-2\times2(k+1)+(k+1)^{2}+...+(4k^{2}-2k\times2(k+1)+(k+1)^{2})/4k  \\ &=[(4+16+...+4k^{2})-2(k+1)(2+4+...+2k)+k(k+1)^{2}]/4k  \\ &=[4(1+2^{2}+...+k^{2})-4(k+1)(1+2+...+k)+k(k+1)^{2}]/4k \end{aligned} $$
      
            - 根据平方和公式（注意该图中n对应上式中k）<img src="/assets/MLpics/T16.png" style="zoom:40%;" />
      $$ \begin{aligned} \delta^2 &=[4k(k+1)(2k+1)/6-4(k+1)(1+2+...+k)+k(k+1)^{2}]/4k  \\ &=(k+1)[8k^{2}/6+4k/6-(k^{2}+k)]/4  \\ &=(k+1)[k^{2}/3-k/3]/4k \\ &=(k^{2}-1)/12 \end{aligned} $$
      
    - 所以其均值和方差分别为$(k+1)/2$和$(k^{2}-1)/12$，则
      
      $$ \begin{aligned} \tau _ {\chi}^{2}&=\frac{k-1}{k}\frac{12N}{k^{2}-1}\sum _ {i=1}^{k}(r _ {i}-\frac{k+1}{2})^{2}  \\ &=\frac{12N}{k(k+1)}(\sum _ {i=1}^{k}r _ {i}^{2}-\sum _ {i=1}^{k}r _ {i}(k+1)+\frac{k(k+1)^{2}}{4}) \\ &=\frac{12N}{k(k+1)}(\sum _ {i=1}^{k}r _ {i}^{2}-\frac{k(k+1)^{2}}{4}) \end{aligned}$$
    
  - 在k和N都较大的情况下，其服从自由度为k-1的$\chi^{2}$分布
    
  - 现在通常使用**F检验**
        
      - $\tau _ {F}=\frac{(N-1)\tau _ {\chi^{2}}}{N(k-1)-\tau _ {\chi^{2}}}$
        
      - 补充知识，**F分布**
        
        <img src="/assets/MLpics/T17.png" style="zoom:50%;" />
        
      - $\tau _ {F}$服从自由度为$k-1$和$(k-1)(N-1)$的F分布
  
- Nemenyi后续检验算法
  
  - 若所有算法的性能相同的假设被拒绝，需要进行“后续检验”来进一步区分各算法
    
  - Nemenyi检验计算出平均序值差别的临界值域
    
  - $CD=q _ {\alpha}\sqrt{\frac{k(k+1)}{6N}}$
    
    <img src="/assets/MLpics/T18.png" style="zoom:50%;" />
    
  - 若两个算法的平均序值之差超出了临界值域CD，则拒绝两个算法性能相同的假设
  
- 举个例子
  
  <img src="/assets/MLpics/T14.png" style="zoom:50%;" />
  
  - 先计算出$\tau _ {F}=24.429$，它大于$\alpha=0.05$时F的检验临界值，因此它拒绝“所有算法性能相同”的假设
    
      - 使用Nemenyi后续检验，$k=3$时$q _ {0.05}=2.344$，临界值域$CD=1.657$，所以算法A与B的差距，以及算法B与C的差距均未超过临界值域，而算法A和C的差距超过临界值域，所以认为A与C的性能显著不同
      
      - Friedman检验图如下图所示
      
        <img src="/assets/MLpics/T19.png" style="zoom:50%;" />
      
        - 两个横线段（临界值域）有交叠，则说明这两个算法没有显著差别

## 偏差与方差

- 偏差-方差分解

  - 是解释学习算法泛化性能的一种重要工具

  - 对学习算法的期望泛化错误率进行拆解

    - 算法在不同训练集（可能来自同一分布）上学得的结果可能不同

    - 对测试样本$x$，令$y _ {D}$为$x$在数据集中的标记，$y$为$x$的真实标记，$f(x;D)$为训练集D上学得模型$f$在$x$上的预测输出

    - 以回归任务为例

      - 学习算法期望

      <img src="/assets/MLpics/T20.png" style="zoom:40%;" />

      - **偏差**
        - 期望预测与真实结果的偏离程度
        - 学习算法本身的拟合能力
      - **方差**
        - 同样大小的训练集变动所导致的
        - 学习性能的变化数据扰动所造成的影响

    - 噪声

      - 在当前任务上任何学习算法所能达到的期望泛化误差的下界
      - 计算数据集中标记和真实标记的差别，属于学习该问题本身存在的差距
      - 学习问题本身的难度
      - 假定噪声期望为0，也就是<img src="/assets/MLpics/T21.png" style="zoom:40%;" />

    - 通过推导，可对算法的期望泛化误差进行分解

      <img src="/assets/MLpics/T22.png" style="zoom:50%;" />

    - 公式推导详细说明
      - 3-4: 
        
        $$ \begin{aligned} \mathbb{E} _ {D}[2(f(x;D)-\mathop{f}\limits_{}^-(x))\mathop{f}\limits_{}^-(x)] &= \mathbb{E} _ {D}[2(f(x;D)\mathop{f}\limits_{}^-(x)-\mathop{f}\limits_{}^-(x)^2)] \\ &= 2\mathop{f}\limits_{}^-(x)\mathbb{E} _ {D}(f(x;D))-2\mathop{f}\limits _ {}^-(x)^2 \\ &= 2\mathop{f}\limits _ {}^-(x)^2-2\mathop{f}\limits _ {}^-(x)^2=0\end{aligned} $$
        
        - $ \mathop{f}\limits _ {}^-(x)$为常量，并且$\mathbb{E} _ {D}(f(x;D))=\mathop{f}\limits _ {}^-(x) $，所以
    
          $$ \begin{aligned} \mathbb{E} _ {D}[2(f(x;D)-\mathop{f}\limits_{}^-(x))y_{D}] &= \mathbb{E} _ {D}[2(f(x;D)\mathop{f}y _ {D}-\mathop{f}\limits _ {}^-(x)y_{D})] \\ &= 2\mathbb{E} _ {D}(y _ {D})\mathbb{E} _ {D}(f(x;D))-2\mathop{f}\limits_{}^-(x)\mathbb{E} _ {D}(y _ {D}) \\ &= 2\mathop{f}\limits_{}^-(x)\mathbb{E} _ {D}(y _ {D})-2\mathop{f}\limits _ {}^-(x)\mathbb{E} _ {D}(y _ {D})=0 \end{aligned} $$
      
      - 6-7: $ \mathbb{E} _ {D}(2(\mathop{f}\limits _ {}^-(x)-y)(y-y _ {D}))=2(\mathop{f}\limits _ {}^-(x)-y)\mathbb{E} _ {D}(y-y _ {D}) $
        
        - $\mathbb{E} _ {D}(y-y _ {D})=0$
      - 原式=0
    
  - 最终结果$E(f;D)=var(x)+bias^2(x)+\epsilon^2$
  
      - 泛化误差可以分解为偏差、方差与噪声之和
    - 泛化性能是由学习算法的能力、数据的充分性和学习任务本身的难度所决定的
  
  - 偏差-方差窘境（bias-variance dilemma)
  
    - <img src="/assets/MLpics/T23.png" style="zoom:50%;" />
  - 在训练不足时，学习器的拟合能力不强，偏差大，方差小，此时偏差主导泛化错误率
    - 随着训练逐渐加深，拟合能力逐渐增强，方差逐渐主导泛化错误率
    - 当训练充足时，学习器拟合能力超强，方差大，偏差小，训练数据的微小变化都会导致学习器发生显著变化
      - 导致过拟合：训练数据自身的非全局的特性被学习到了，并且学习器使用这些无用的“特性”进行后续预测

## 部分习题解释

- 2.1

  - 需要1000个样本，700个训练集，300个测试集
  - 500个正例和500个反例平均分配
  - 700训练集：350正/350反，300测试集：150正/150反
  - $C _ {500}^{350}C _ {500}^{350}$

- 2.2 

  - 10折交叉验证
    - 分成10个大小相似的互斥子集（5个一组）
    - 9个子集为训练集，1个测试集
    - 有10种选择求均值
    - 正反例数目相同
    - 预测判断为正反例的概率相同
    - 错误率50%
  - 留一法
    - 每一个都是一个子集
    - 99个子集为训练集，1个测试集
    - 有100种选择求均值
    - 可能结果：正/反
    - 正：预测错误，反：预测错误 错误率100%

- 2.3

  - $F 1=\frac{2 \times P \times R}{P+R}$
  - 事实上在P同的情况下，R越大$F1$越大，反之亦然
  - 而P越大，R越小
  - P-R图通常是非单调，不平滑的，在很多局部有上下波动
  - A和B的F1值相同和BEP值二者没有绝对相关。
  - BEP点前后很可能同时出现更大的P和更大的R，因此现实中BEP并不实用。

- 2.4

  - 公式
    - $\mathrm{TPR}=\frac{T P}{T P+F N}$
    - $\mathrm{FPR}=\frac{F P}{T N+F P}$
    - $P=\frac{T P}{T P+F P}$
    - $R=\frac{T P}{T P+F N}$
    - $TPR=R$    查全率=真正例率

- 2.5

  - 证明$\mathrm{AUC}\approx1-\ell _ {\text {rank}}$

  - ROC曲线如下（已经乘$m^+m^-$）

    <img src="/assets/MLpics/T4.png" style="zoom:50%;" />

  - 以上图为例，一共有4个正例4个反例$m^+=m^-=4$

  - 理论向证明

    - 暂时不考虑$f(x^{+})=f(x^{-})$，$AUC=\sum _ {i=1}^{m-1}\left(x _ {i+1}-x _ {i}\right)y _ {i}$
      - 乘$m^+m^-$（如上图），可发现$x _ {i+1}-x _ {i}=\{0,1\}$
      - 当$x _ {i+1}-x _ {i}=1$时，$(x _ {i+1},y _ {i+1})$是一个反例样本，排在它前面的有$y _ {i}$个正例
      - 也就是说$AUC=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}(\mathbb{I}(f(x^{+})>f(x^{-}))$
      - 此时$\ell _ {r a n k}=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)\right)$
      - $\mathrm{AUC}=1-\ell _ {\text {rank}}$
    - 考虑$f(x^{+})=f(x^{-})$
      - 正例预测值和反例预测值都相同，如图中的（x2，y3）和（x3，y4）
      - 在AUC面对这种情况时，会多余的加上一个三角形，需要减去
      - 对于$\ell _ {r a n k}$来说就是加上这个三角（1/2）

  - 数据向证明

    - $\ell _ {r a n k}=\frac{1}{m^{+} m^{-}} \sum _ {x^{+} \in D^{+}} \sum _ {x^{-} \in D^{-}}\left(\mathbb{I}\left(f\left(x^{+}\right)<f\left(x^{-}\right)\right)+\frac{1}{2} \mathbb{I}\left(f\left(x^{+}\right)=f\left(x^{-}\right)\right)\right)$
      - $f(x^{+})<f(x^{-})$
        - （0，y2）点前有1个反例样本
        - （x2，y3）点前有2.5个反例样本
        - （x4，y5）点前有3个反例样本
      - $f(x^{+})=f(x^{-})$
        - （x2，y3）和（x3，y4）预测值相等，记0.5
    - 由上式可得$\ell _ {r a n k}=$5.5/16
    - $\mathrm{AUC}=\frac{1}{2} \sum _ {i=1}^{m-1}\left(x _ {i+1}-x _ {i}\right) \cdot\left(y _ {i}+y _ {i+1}\right)$（总面积）
    - 由上式可得$AUC=(1+2+1/2+3+4)/16=10.5/16$
    - 所以$\mathrm{AUC}=1-\ell _ {\text {rank}}$

- 2.6

  - $\mathrm{TPR}=\frac{T P}{T P+F N}$，针对$D^+$情况

  - $\mathrm{FPR}=\frac{F P}{T N+F P}$，针对$D^-$情况

    所以

    $$ \begin{aligned}
    E(f ; D ; \cos t)&= \frac{1}{m}(\sum _ {\boldsymbol{x} _ {i} \in D^{+}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}) \times \operatorname{cost} _ {01}
    +\sum _ {\boldsymbol{x} _ {i} \in D^{-}} \mathbb{I}\left(f\left(\boldsymbol{x} _ {i}\right) \neq y _ {i}\right) \times \operatorname{cost} _ {10}\right)
     \\ &= \frac{1}{m}( \vert D^+ \vert (1-TPR) cost _ {01}+FPR \vert D^- \vert cost _ {10}) \\ \end{aligned} $$

- 2.10

  - 一个为卡方检验：主要针对具有相同频数的数据组的一致性（在2.34中用于比较不同算法之间的差异）
  - 一个为基于卡方检验的F检验：主要针对两组数值差异性（在2.35中基于卡方检验进行数据集之间的差异性）
  - F检验比起卡方检验更好的是考虑到了不同数据集所带来的影响（卡方检验直接xN）

# 线性模型

## 基本形式

- $f(x)=w _ {1}x _ 1+w _ 2x _ 2+...+w _ dx _ d+b$
- 向量形式$f(x)=w^Tx+b$
- 许多非线性模型可以在线性模型的基础上通过引入层级结构或高维影射得到
- 由于w直观表达了各属性在预测中的重要性，因此线性模型有很好的**可解释性**

## 线性回归

- 数据集$D={(x _ 1,y _ 1),(x _ 2,y _ 2)...,(x _ m,y _ m)}$

- $x _ i=(x _ i1;x _ i2;...;x _ id)$

- 线性回归试图学得一个线性模型以尽可能的预测实值并输出标记

- 最简单的情形：输入属性的数目只有一个，也就是说第二条的$x _ i=x _ {i1}$

  - $D=\{(x _ i,y _ i)\} _ {i=1}^m$

  - 对于离散属性

    - 若属性值之间存在序关系
      - 可通过连续化将其转化为序列值
      - 如高、矮可转化为$\{1.0,0.0\}$
    - 若属性值之间不存在序关系
      - 如果有k个属性值，则通常转化为k维向量
      - 如西瓜、南瓜可转化为$(1,0),(0,1)$

  - 线性回归试图学得$f(x _ i)=wx _ i+b,\text{使得}f\left(x _ {i}\right) \simeq y _ {i}$

    - 如何求得w和b

      - 在性能度量一节中，均方误差是回归任务最常用的，为凸函数（说明详见西瓜书P54注解）

      - 试图让均方误差最小化$$ \begin{aligned}
        \left(w^{ * }, b^{ * }\right) &=\underset{(w, b)}{\arg \min } \sum _ {i=1}^{m}\left(f\left(x _ {i}\right)-y _ {i}\right)^{2}  \\ 
        &=\underset{(w, b)}{\arg \min } \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}-b\right)^{2}
        \end{aligned} $$

      - 在2.3节中提到，均方误差的几何意义对应欧式距离，基于均方误差最小化来进行模型求解的方法为**“最小二乘法”**

      - 线性回归模型的最小二乘“参数估计”：求解$w,b$使得均方误差最小化的过程

      - 我们分别对$w,b$求导

        - $\frac{\partial E _ {(w, b)}}{\partial w}=2\left(w \sum _ {i=1}^{m} x _ {i}^{2}-\sum _ {i=1}^{m}\left(y _ {i}-b\right) x _ {i}\right)$

        - $\frac{\partial E _ {(w, b)}}{\partial b}=2\left(m b-\sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)\right)$

        - 令上两式为0得到$w,b$的最优解的闭式解

        - **闭式解**

          - 又名解析解，针对一些严格的公式，给出任意的自变量就可以求出其因变量,也就是问题的解, 他人可以利用这些公式计算各自的问题。
  - 除此之外，**数值解**是采用某种计算方法,如数值逼近,插值的方法 得到的解。别人只能利用数值计算的结果, 而不能随意给出自变量并求出计算值。
    
  - $b=\frac{1}{m} \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)$
    
  - $w=\frac{\sum _ {i=1}^{m} y _ {i}\left(x _ {i}-\bar{x}\right)}{\sum _ {i=1}^{m} x _ {i}^{2}-\frac{1}{m}\left(\sum _ {i=1}^{m} x _ {i}\right)^{2}}$
          
    - 求解上方求导式（注意带入b）：
      
      - $w \sum _ {i=1}^{m} x _ {i}^{2} =\sum _ {i=1}^{m} y _ {i} x _ {i}-\sum _ {i=1}^{m}b x _ {i}$
      
      - $b=\frac{1}{m} \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)$
      
        - $\frac{1}{m} \sum _ {i=1}^{m}y _ {i}=\bar{y}$
      
        - $\frac{1}{m} \sum _ {i=1}^{m}w x _ {i}=w\bar{x}$
      
        - $b=\bar{y}-w\bar{x}$
      
          $$ \begin{aligned} w \sum _ {i=1}^{m} x _ {i}^{2} &=\sum _ {i=1}^{m} y _ {i} x _ {i}-\sum _ {i=1}^{m}(\bar{y}-w \bar{x}) x _ {i}  \\ 
          w \sum _ {i=1}^{m} x _ {i}^{2} &=\sum _ {i=1}^{m} y _ {i} x _ {i}-\bar{y} \sum _ {i=1}^{m} x _ {i}+w \bar{x} \sum _ {i=1}^{m} x _ {i} \end{aligned}$$
      
        - 可得
      
          $$\begin{aligned}w\left(\sum _ {i=1}^{m} x _ {i}^{2}-\bar{x} \sum _ {i=1}^{m} x _ {i}\right) &=\sum _ {i=1}^{m} y _ {i} x _ {i}-\bar{y} \sum _ {i=1}^{m} x _ {i} \\ w=&\frac{\sum _ {i=1}^{m} y _ {i}\left(x _ {i}-\bar{x}\right)}{\sum _ {i=1}^{m} x _ {i}^{2}-\frac{1}{m}\left(\sum _ {i=1}^{m} x _ {i}\right)^{2}}\end{aligned}$$

- 更一般的情形是$f(x _ i)=wx _ i^T+b,\text{使得}f\left(x _ {i}\right) \simeq y _ {i}$，称之为多元线性回归

  - 同样适用最小二乘法进行估计

  - 把$w,b$放入向量形式

  - 把数据集D表示为一个$m\times(d+1)$大小的矩阵X

    - 其中每行对应一个示例

    - 该行前d个元素对应于示例的d个属性值，最后一个元素恒为1

      $$\mathbf{X}=\left(\begin{array}{ccccc}
      x _ {11} & x _ {12} & \ldots & x _ {1 d} & 1  \\ 
      x _ {21} & x _ {22} & \ldots & x _ {2 d} & 1  \\ 
      \vdots & \vdots & \ddots & \vdots & \vdots  \\ 
      x _ {m 1} & x _ {m 2} & \ldots & x _ {m d} & 1
      \end{array}\right)=\left(\begin{array}{cc}
      \boldsymbol{x} _ {1}^{\mathrm{T}} & 1  \\ 
      \boldsymbol{x} _ {2}^{\mathrm{T}} & 1  \\ 
      \vdots & \vdots  \\ 
      \boldsymbol{x} _ {m}^{\mathrm{T}} & 1
      \end{array}\right) $$

    - 把标记写成向量形式：$\hat{\boldsymbol{w}}^{*}=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$

    - 令$E _ {\hat{\boldsymbol{w}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})=\boldsymbol{y}^{\mathrm{T}} \boldsymbol{y}-\boldsymbol{y}^{\mathrm{T}} \mathbf{X} \hat{\boldsymbol{w}}-\hat{\boldsymbol{w}}^{\mathrm{T}} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}+\hat{\boldsymbol{w}}^{\mathrm{T}} \mathbf{X}^{\mathrm{T}} \mathbf{X} \hat{\boldsymbol{w}}$
    
      - 对$E _ {\hat{\boldsymbol{w}}}$求导
  - 矩阵微分公式$\frac{\partial \boldsymbol{a}^{\mathrm{T}} \boldsymbol{x}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{x}^{\mathrm{T}} \boldsymbol{a}}{\partial \boldsymbol{x}}=\boldsymbol{a}, \frac{\partial \boldsymbol{x}^{\mathrm{T}} \mathbf{A} \boldsymbol{x}}{\partial \boldsymbol{x}}=\left(\mathbf{A}+\mathbf{A}^{\mathrm{T}}\right) \boldsymbol{x}$
      - $\frac{\partial E _ {\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=0-X^Ty-X^Yy+(X^TX+X^TX)w \\  \frac{\partial E _ {\hat{\boldsymbol{w}}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y})$
      
    - 令上式为0得到$\hat{\boldsymbol{w}}$的最优解的闭式解

      - 当$\mathbf{X} ^T\mathbf{X} $为**满秩矩阵/正定矩阵**时

        - 补充：

          - 满秩矩阵：设A是n阶矩阵, 若r（A） = n, 则称A为满秩矩阵。但满秩不局限于n阶矩阵。

            - 在线性代数中，一个矩阵A的列秩是A的线性独立的纵列的极大数目。类似地，行秩是A的线性无关的横行的极大数目。

            - 如下图

              <img src="/assets/MLpics/T24.png" style="zoom:50%;" />

          - 正定矩阵

            - 设M是n阶方阵，如果对任何非零向量z，都有$z^TMz>0$，就称M为正定矩阵。

        - 令求导式为0得到

          - $\hat{\boldsymbol{w}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$
      - 令$\hat{\boldsymbol{x _ i}}=(x _ i,1)$，最终学得的多元线性回归模型为
        
          - $f(\hat{\boldsymbol{x _ i}})= \hat{\boldsymbol{x _ i}}^T\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$
    
  - 现实生活中$\mathbf{X} ^T\mathbf{X} $往往不是满秩矩阵，此时可以解出多个$\hat{\boldsymbol{w}}$，他们都能使均方误差最小化，选择哪一个解作为输出将由学习算法的归纳偏好决定
    
      - 常见做法为引入**正则化(regularization)**

- **广义线性模型**

  - 对数线性回归
    - 我们得到了线性回归模型之后，可否令模型预测值逼近$y$的衍生物
    - 譬如我们认为示例所对应的输出标记是在指数尺度上变化
      - $lny=w^Tx+b$
      - 虽然形式上仍然是线性回归，但实质上已经是在求取空间到输出空间的非线性函数映射
      - <img src="/assets/MLpics/T25.png" style="zoom:50%;" />
      - 这里对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用
  - 广义线性模型
    - 考虑单调可微函数$g(·)$（称之为联系函数）
    - $y=g^{-1}(w^Tx+b)$
    - 对数线性回归是广义线性模型在$g(·)=ln(·)$时的特例

## 对数几率回归

- 先决条件

  - 二分类任务
  - 输出标记为$y\in\{0,1\}$
  - 线性回归模型产生的预测值$z=w^Tx+b$是实际值
  - 我们需要将z转换为0/1值

- $y=\frac{1}{1+e^{-(w^Tx+b)}}$  公式推导

  - 单位阶跃函数（unit-step function）$$ y=\left\{\begin{array}{cl}
  0, & z<0  \\ 
    0.5, & z=0  \\ 
    1, & z>0    \end{array}\right. $$
  
  - 若预测值z大于0就判为正例，小于0则判为反例，预测值为临界值0则可以任意判别

    <img src="/assets/MLpics/T26.png" style="zoom:50%;" />

  - 从图中可以看出，单位阶跃函数不连续，因此不能直接用作广义线性模型中的$g^-(·)$

    - 找出一个替代函数，并且单调可微---对数几率函数

      - $y=\frac{1}{1+e^{-z}}$

      - 对数几率函数是一个**Sigmoid函数**，将z转化成一个接近0或1的y值

      - 将其作为$g^-(·)$得到

        $y=\frac{1}{1+e^{-(w^Tx+b)}}$

      - 上式可变化为$ln{\frac{y}{1-y}}=w^Tx+b$

      - 将y视为样本x作为正例的可能性，1-y为其反例可能性，$\frac{y}{1-y}$称之为**几率（odds）**，将其取对数称之为**对数几率（log odds/logit）**

  - 虽然$y=\frac{1}{1+e^{-(w^Tx+b)}}$叫对数几率回归，但实际上却是一种分类学习方法
  - 优点
      - 对分类可能性进行建模，无需假设数据分布，避免了假设分布不准确所带来的问题
      - 不是仅预测出“类别”，而是可得到近似概率预测，适合许多需要利用概率辅助决策的任务
      - 对率函数是任意阶可导的凸函数，现有的许多数值优化算法可以直接用于求取最优解
  
- 确定$w,b$

  - 若将y视作**类后验概率估计**p($y=1 \vert x$)，则

    $$\begin{aligned} ln{\frac{y}{1-y}}=&w^Tx+b  \\  ln\frac{p(y=1 \vert x)}{p(y=0 \vert x)}=&w^Tx+b  \\  p(y=1 \vert x)=&\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}  \\  p(y=0 \vert x)=&\frac{1}{1+e^{w^Tx+b}}\end{aligned}$$

    （最后两公式为3.23和3.24）

    - 通过极大似然法

      - 补充知识

        <img src="/assets/MLpics/T27.png" style="zoom:30%;" />

      - 给定数据集$\{(x _ i,y _ i)\}^m _ {i=1}$，对率回归模型最大化“对数似然”

        $\ell(\boldsymbol{w}, b)=\sum _ {i=1}^{m} \ln p\left(y _ {i} \mid \boldsymbol{x} _ {i} ; \boldsymbol{w}, b\right)$								(公式3.25)

      - 令每个样本属于其真实标记的概率越大越好

        - 令$\beta=(w;b),\hat{x}=(x;1)$，则$w^Tx+b=\beta^T\hat{x}$

        - $p _ 1(\hat{x};\beta)=p(y=1 \vert \hat{x};\beta)$

        - $p _ 0(\hat{x};\beta)=p(y=0 \vert \hat{x};\beta)=1-p(y=1 \vert \hat{x};\beta)$

        - 公式3.25的似然项为

          $$\begin{aligned}p(y _ i \vert x _ i;w,b)=&p(y=1 \vert \hat{x};\beta)+p(y=0 \vert \hat{x};\beta)  \\  p(y _ i \vert x _ i;w,b)=&y _ ip _ 1(\hat{x _ i};\beta)+(1-y _ i)p _ 0(\hat{x _ i};\beta)\end{aligned}$$

        - 将上式带入3.25
    
          - 注意
        
        $$\begin{aligned}p _ 1(\hat{x _ i};\beta)=&\frac{e^{\beta^T\hat{x _ i}}}{1+e^{\beta^T\hat{x _ i}}}  \\  p _ 0 (\hat{x _ i};\beta)=&\frac{1}{1+e^{\beta^T\hat{x _ i}}}\end{aligned}$$
    
  - 根据公式3.23和3.24：最大化似然函数等价于最小化似然函数的相反数（下式中倒数第二个推导出的公式为极大似然估计的似然函数）
              
      $$\begin{aligned}\ell(\beta)=& \sum _ {i=1}^mln(y _ ip _ i(\hat{x _ i};\beta)+(1-y _ i)p _ 0(\hat{x _ i};\beta))  \\  =&\sum _ {i=1}^mln(\frac{y _ ie^{\beta^T\hat{x _ i}}+1-y _ i}{1+e^{\beta^T\hat{x _ i}}})  \\  =&\sum _ {i=1}^{m}(ln(y _ ie^{\beta^T\hat{x _ i}}+1-y _ i)-ln(1+e^{\beta^T\hat{x _ i}}))  \\  \ell(\beta)=&\left\{\begin{array}{cl}
            \sum _ {i=1}^{m}(-ln(1+e^{\beta^T\hat{x _ i}})), & y _ i=0  \\ 
        \sum _ {i=1}^{m}(\beta^T\hat{x _ i}-ln(1+e^{\beta^T\hat{x _ i}})), & y _ i=1
            \end{array}\right.  \\  \ell(\beta)=&\sum _ {i=1}^{m}\left(y _ {i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}-\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}}\right)\right)  \\  \ell(\beta)=&\sum _ {i=1}^{m}\left(-y _ {i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}}\right)\right)\end{aligned} $$
            

  - 上式是关于$\beta$的高阶可导连续凸函数，根据凸优化，通过**梯度下降、牛顿法**都可求其最优解

      - $\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}}{\arg \min } \ell(\boldsymbol{\beta})$

      - 做求导


      ​        

## 线性判别分析

- 线性判别分析（Linear Discriminant Analysis，简称LDA，亦称Fisher判别分析）

- 思想

  - 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离
  - 在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别
  - LDA与Fisher判别分析稍有胡同，前者假设了各类样本的协方差矩阵相同并且满秩

- 二维示意图

  <img src="/assets/MLpics/T28.png" style="zoom:50%;" />

- 公式推导

  - 给定数据集$D=\{(x _ i,y _ i)\}^m _ {i=1}$

  - 令$X _ i,\mu _ i,\sum _ i$分别表示第$i\in\{0,1\}$类示例的集合、均值向量、协方差矩阵

  - 若将数据投影到直线$w$上，则两类样本的中心在直线上的投影分别为$w^T\mu _ 0$和$w^T\mu _ 1$

  - 若将所有样本点都投影到直线上，则两类样本的协方差分别为$w^T\sum _ 0w$和$w^T\sum _ 1w$

    - 知识补充：

      - **协方差**：协方差是对两个随机变量联合分布线性相关程度的一种度量。两个随机变量越线性相关，协方差越大，完全线性无关，协方差为零。

      <img src="/assets/MLpics/T29.png" style="zoom:30%;" />

      - **样本协方差**：对于多维随机变量$Q(x1,x2,x3,…,xn)$，样本集合为$$x _ {ij}=[x _ {1j},x _ {2j},…,x _ {nj}] (j=1,2,…,m) $$，m为样本数量，在$ a,b（a,b=1,2…n）$两个维度内:

        $\operatorname{cov}\left(\mathrm{x} _ {\mathrm{a}}, \mathrm{x} _ {\mathrm{b}}\right)=\frac{\sum _ {j=1}^{m}\left(x _ {a j}-\bar{x} _ {a}\right)\left(x _ {b j}-\bar{x} _ {b}\right)}{m-1}$

      - **协方差矩阵**：对于多维随机变量$Q(x1,x2,x3,…,xn)$我们需要对任意两个变量$(xi,xj)$求线性关系，即需要对任意两个变量求协方差矩阵$$ \operatorname{cov}\left(x _ {i}, x _ {j}\right)=\left[\begin{array}{ccccc}
        \operatorname{cov}\left(x _ {1}, x _ {1}\right) & \operatorname{cov}\left(x _ {1}, x _ {2}\right) & \operatorname{cov}\left(x _ {1}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {1}, x _ {n}\right)  \\ 
        \operatorname{cov}\left(x _ {2}, x _ {1}\right) & \operatorname{cov}\left(x _ {2}, x _ {2}\right) & \operatorname{cov}\left(x _ {2}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {2}, x _ {n}\right)  \\ 
        \operatorname{cov}\left(x _ {3}, x _ {1}\right) & \operatorname{cov}\left(x _ {3}, x _ {2}\right) & \operatorname{cov}\left(x _ {3}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {3}, x _ {n}\right)  \\ 
        \vdots & \vdots & \vdots & \ddots & \vdots  \\ 
        \operatorname{cov}\left(x _ {m}, x _ {1}\right) & \operatorname{cov}\left(x _ {m}, x _ {2}\right) & \operatorname{cov}\left(x _ {m}, x _ {3}\right) & \cdots & \operatorname{cov}\left(x _ {m}, x _ {n}\right)
        \end{array}\right] $$

  - 欲最大化

    - 欲使同类样例的投影点尽可能接近，可以让同类样例点的协方差$w^T\sum _ 0w+w^T\sum _ 1w$尽可能小

    - 欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$\|w^T\mu _ 0-w^T\mu _ 1\| _ 2^2$

    - 同时考虑二者，则可以得到欲最大化的目标
      
      $$ \begin{aligned}
      J &=\frac{\left\|\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu} _{0}-\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\mu} _{1}\right\|_{2}^{2}}{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma} _{0} \boldsymbol{w}+\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\Sigma} _{1} \boldsymbol{w}} \\
  &=\frac{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\mu} _{0}-\boldsymbol{\mu} _{1}\right)\left(\boldsymbol{\mu} _{0}-\boldsymbol{\mu} _{1}\right)^{\mathrm{T}} \boldsymbol{w}}{\boldsymbol{w}^{\mathrm{T}}\left(\boldsymbol{\Sigma} _{0}+\boldsymbol{\Sigma} _{1}\right) \boldsymbol{w}}
      \end{aligned} $$
    
    - 定义**类内散度矩阵**

      $$ S _ w &=\sum _ 0+\sum _ 1 \\ \begin{aligned} &=\sum _ {\boldsymbol{x} \in X _ {0}}\left(\boldsymbol{x}-\boldsymbol{\mu} _ {0}\right)\left(\boldsymbol{x}-\boldsymbol{\mu} _ {0}\right)^{\mathrm{T}}+\sum _ {\boldsymbol{x} \in X _ {1}}\left(\boldsymbol{x}-\boldsymbol{\mu} _ {1}\right)\left(\boldsymbol{x}-\boldsymbol{\mu} _ {1}\right)^{\mathrm{T}}\end{aligned}$$

    - 定义**类间散度矩阵**

      $S _ b=(\mu _ 0-\mu _ 1)(\mu _ 0-\mu _ 1)^T$

    - 则J可重写为

      $J=\frac{w^TS _ bw}{w^TS _ ww}$

    - LDA欲最大化的目标：$S _ b$和$S _ w$的**广义瑞利商（generalized Rayleigh quotient）**
  
  - 确定w
  
  - 由于J的解与w的长度无关，只与其方向有关，不失一般性，令$w^TS _ ww=1$，则J的重写式等价于
    $$\begin{array}{ll}
      \min  _ {\boldsymbol{w}} & -\boldsymbol{w}^{\mathrm{T}} \mathbf{S} _ {b} \boldsymbol{w}  \\ 
    \text { s.t. } & \boldsymbol{w}^{\mathrm{T}} \mathbf{S} _ {u} \boldsymbol{w}=1
      \end{array}$$

    - 由拉格朗日乘子法

      - 补充说明：拉格朗日乘数法是一种寻找变量受一个或多个条件所限制的多元函数的极值的方法。这种方法将一个有n 个变量与k 个约束条件的最优化问题转换为一个有n + k个变量的方程组的极值问题，其变量不受任何约束。这种方法引入了一种新的标量未知数，即拉格朗日乘数：约束方程的梯度的线性组合里每个向量的系数，从而找到能让设出的隐函数的微分为零的未知数的值。

        - 设给定二元函数z=ƒ(x,y)和附加条件φ(x,y)=0，为寻找z=ƒ(x,y)在附加条件下的极值点，先做拉格朗日函数 

          <img src="/assets/MLpics/T30.png" style="zoom:50%;" /> 

          (其中λ为参数)

      - 推导：

        $L(w,\lambda)=-w^TS _ bw+\lambda(w^TS _ ww-1)$

        对$w$求偏导

        $\frac{\partial L(w,\lambda)}{\partial w}=-(S _ b+S _ b^T)w+\lambda (S _ w+S _ w^T)w$

        因为$S _ b=S _ b^T,S _ w=S _ w^T$

        $\frac{\partial L(w,\lambda)}{\partial w}=-2S _ bw+2\lambda S _ ww=0$

        $S _ bw=\lambda S _ ww$

        由于拉格朗乘子具体取值多少无所谓，我们像求解的只有$w$，所以我们可以任意设定$\lambda$来配合我们求解$w$

        如果我们令$\lambda=(\mu _ 0-\mu _ 1)^Tw$，那么$S _ bw=\lambda(\mu _ 0-\mu _ 1)$

        则$w=S _ w^{-1}(\mu _ 0-\mu _ 1)$
  
      - 考虑到数值解的稳定性，实际上是对$S _ w$做奇异值分解
  
        - 补充知识：**奇异值分解**
          - 假设M是一个m×n阶矩阵，其中的元素全部属于域 K，也就是实数域或复数域。如此则存在一个分解使得$M=U\sum V^T$
        - 其中U是m×m阶酉矩阵（$U^TU=UU^T=I _ A/U^T=U^{-1}$）；$\sum$是半正定m×n阶对角矩阵；$V^ *$，(V的共轭转置)，是n×n阶酉矩阵。这样的分解就称作M的奇异值分解。Σ对角线上的元素Σi，其中Σi即为M的奇异值。
        - $S _ w=U\sum V^T$
      - $\sum$是一个实对角矩阵，其对角线上的元素是$S _ w$的奇异值
      
    - $S _ w^{-1}=V\sum^{-1}U^T$
  
- LDA贝叶斯决策理论阐释
  
  - 可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类
  
- LDA推广到多分类任务重
  
    - 假定存在N个类，且第i类示例数为$m _ i$
    
  - **全局散度矩阵**
  
    - $S _ t=S _ b+S _ w  =\sum^{m} _ {i=1}(x _ i-\mu)(x _ i-\mu)^T$
  
    - 其中$\mu$是所有示例的均值向量，将类内散度矩阵$S _ w$重定义为每个类别的散度矩阵之和$$ \begin{aligned}S _ w=\sum^N _ {i=1}S _ {w _ i}\end{aligned}$$
  
      - 所以说$$ \begin{aligned}S _ {w _ i}=\sum _ {\boldsymbol{x} \in X _ {i}}\left(\boldsymbol{x}-\boldsymbol{\mu} _ {i}\right)\left(\boldsymbol{x}-\boldsymbol{\mu} _ {i}\right)^T\end{aligned}$$
  
      $$ \begin{aligned}S _ b=&S _ t-S _ w  \\  =&\sum^{m} _ {i=1}(x _ i-\mu)(x _ i-\mu)^T-\sum^N _ {i=1}\sum _ {x \in X _ {i}}(x-\mu _ {i})(x-\mu _ {i})^T               \\ =&\sum^N _ {i=1}(\sum _ {x \in X _ {i}}((x-\mu)(x-\mu)^T-(x-\mu _ {i})(x-\mu _ {i})^T))  \\ =&\sum^N _ {i=1}(\sum _ {x \in X _ {i}}((x-\mu)(x^T-\mu^T)-(x-\mu _ {i})(x^T-\mu _ {i}^T)))                           \\ =& \sum^N _ {i=1}(\sum _ {x \in X _ {i}}(-\mu x^T-x\mu^T +\mu\mu^T+\mu _ i^Tx+\mu _ ix^T-\mu _ i\mu _ i^T))                          \\ =& \sum^N _ {i=1}(-m _ i\mu \mu _ i^T-m _ i\mu _ i\mu^T +m _ i\mu\mu^T+m _ i\mu _ i^T\mu _ i)      \\ =& \sum^N _ {i=1}m _ i(\mu _ i\mu _ i^T-\mu \mu _ i^T-\mu _ i\mu^T +\mu\mu^T) \\ =&\sum^N _ {i=1}m _ i(\mu _ i-\mu)(\mu _ i-\mu)^T\end{aligned}$$
  
    - 多分类LDA可以有多种实现方法
    
      - 采用优化目标$$\max  _ {\mathbf{W}} \frac{\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {b} \mathbf{W}\right)}{\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {w} \mathbf{W}\right)}$$
    
        - $\mathbf{W}=(w _ 1,w _ 2...w _ i,...w _ {N-1}) \in \mathbb{R}^{d \times(N-1)}$
        
        - $tr(·)$表示矩阵的迹，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和。
          
          $$ \left\{ \begin{aligned}
          \operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {b} \mathbf{W}\right) &=\sum_{i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {b} \boldsymbol{w} _ {i}  \\ 
          \operatorname{tr}\left(\mathbf{W}^{\mathrm{T}} \mathbf{S} _ {w} \mathbf{W}\right) &=\sum _ {i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {w} \boldsymbol{w} _{i}
          \end{aligned} \right. $$
          
        - 可以变形为$\max  _ {\mathbf{W}} \frac{\sum _ {i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {b} \boldsymbol{w} _ {i}}{\sum _ {i=1}^{N-1} \boldsymbol{w} _ {i}^{\mathrm{T}} \mathbf{S} _ {w} \boldsymbol{w} _ {i}}$
        
        - 为$J=\frac{w^TS _ bw}{w^TS _ ww}$的推广式
        
        - 求解：$S _ bW=\lambda S _ w W$
  
- 若将W视为一个投影矩阵
      
  - 则多分类$LDA$将样本投影到N-1维空间，N-1通常远小于数据原有的属性数
        - 可以通过这个投影来减小样本点的维数，且投影过程中使用了类别信息
            - LDA通常也被视为一种经典的监督降维技术

## 多分类学习

- 有些二分类学习方法可直接推广到多分类

- 二分类学习器

  - 考虑N个类别$C _ 1,C _ 2,...,C _ N$
  - 多分类学习器的基本思路是**拆解法**
    - 将多分类任务拆为若干个二分类任务求解
    - 先对问题进行拆分
    - 为拆出的每个二分类任务训练一个分类器
    - 测试时：对这些分类器的预测结果进行集成
    - 重点为**拆分和集成**

- 拆分策略

  - **“一对一”（OvO）**

    - 给定数据集$D=\{(x _ 1,y _ 1),(x _ 2,y _ 2),...,(x _ m,y _ m)\},y _ i \in \{C _ 1,C _ 2,...,C _ N\}$
    - 将这N个类别两两配对，从而产生N(N-1)/2个二分类任务
    - 例如OvO为区别$C _ i,C _ j$训练一个分类器
      - 该分类器把D中的$C _ i$类样例作为正例
      - 把D中的$C _ j$类样例作为反例
    - 测试阶段，新样本将同时提交给所有分类器，所以获得N(N-1)/2个分类结果
    - 被预测得最多的类别作为最终分类结果

  - **“一对其余”（OvR）**

    - 只训练N个分类器

    - 训练

      - 每次将一个类的样例作为正例
      - 所有其他类的样例作为反例

    - 测试时若仅有一个分类器预测为正类，对应的类别标记为最终分类结果

      <img src="/assets/MLpics/T31.png" style="zoom:33%;" />

    - 比起OvO的优缺点

      - 优点：存储开销和测试时间开销更小
      - 缺点：OvR的每个分类器均适用全部训练样例，而OvO仅需要用到两个类的样例
        - 在类别很多时，OvO的训练时间开销通常比OvR更小

  - **“多对多”（MvM）**

    - 每次将若干个类作为正类，若干个其他类作为反类

    - OvO和OvR是MvM的特例

    - MvM的正反类构造需要有特殊设计

      - 最常用的MvM技术**纠错输出码 （Error Correcting Output Codes，简称ECOC）**

        - 编码

          - 对N个类做M次划分，每次划分将一部分类别划为正类，一部分类划为反类，从而形成一个二分类训练集
          - 一共产生M个训练集，可以训练出M个分类器

        - 解码

          - M个分类器分别对测试样本进行预测，这些预测标记组成一个编码
          - 将这个预测编码与每个类别各自的编码进行比较
          - 最终预测结果：距离最小的类别

        - 类别划分：**”编码矩阵（coding matrix）**

          - 二元码

            - 将每个类别分别指定为正类和反类
            - 如下图，分类器$f _ 2$将$C _ 1$类和$C _ 3$类的样例作为正例，$C _ 2$类和$C _ 4$类的样例作为反例

          - 三元码

            - 在正、反类之外，还可以指定“停用类”
            - 如下图，分类器$f _ 6$将$C _ 1$类和$C _ 3$类的样例作为正例，$C _ 2$类的样例作为反例

            <img src="/assets/MLpics/T32.png" style="zoom:50%;" />

          - 在解码阶段

            - 各分类器的预测结果联合起来形成了测试示例的编码
            - 将距离最小的编码所对应的类别作为预测结果，如上图二元码结果为$C _ 3$

        - 修正

          - 在测试阶段，EOOC编码对分类器的错误有一定的容忍和修正能力
          - 如上图a中，如果$f _ 2$出错导致错误编码（-1，-1，+1，-1，+1），但结果仍然正确
          - 同一个学习任务，ECOC编码越长
            - 纠错能力越强
            - 所需训练的分类器越多，计算存储开销增大
            - 对有限类别数，可能的组合数目是有限的，码长超过一定范围后就失去了意义
          - 同等长度的编码
            - 任意两个类别之间的编码距离越远，纠错能力越强
            - NP难问题
              - 码长较小时可以根据上一条的原则计算出理论最优编码
              - 码长稍大就难以有效确定最优编码（NP难问题）
              - 但通常我们不需要获得理论最优编码

## 类别不平衡问题

- 引入

  - 前面的分类学习方法共同的基本假设：
    - 不同类别的训练样例数目相当
    - 如果不同类别的训练样例数目差别很大，则会造成困扰
      - 如998个反例，正例只有2个，学习方法之返回一个永远将新样本预测为返例的学习器就能达到99.8%的精度

- 类别不平衡

  - 分类任务中不同类别的训练样例数目差别很大的情况，本节假定**正类样例较少，反类样例较多**
  - 发生情况：
    - 通过拆分法解决多分类问题时，即使原始问题不同类别的样例数目相当
      - 在使用OvR、MvM策略后
        - 虽然每个类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵消
        - 但也仍然可能出现类别不平衡

- **再缩放（rescaling）**

  - 类别不平衡学习的一个基本策略

  - 线性分类器的角度

    - 用预测出的y值与一个阈值进行比较
    - 当训练集中正、反例数目相同
      - 例如阈值为0.5，y>0.5时判为正例，否则为反例
      - y表达了正例的可能性，$\frac{y}{1-y}$表达了正例可能性与反例可能性之比值
      - 阈值0.5表明分类器认为真实正、反例可能性相同
      - 若$\frac{y}{1-y}>1$时，预测为正例
    - 当训练集中正、反例数目不同，$m^+、m^-$表示正、反例数目
      - 观测几率是$\frac{m^+}{m^-}$
      - 假设训练集是真实样本总体的**无偏采样**（真实样本总体的类别比例在训练集中得以保持），观测几率=真实几率
      - 若$\frac{y}{1-y}>\frac{m^+}{m^-}$，预测为正例
        - 将上式转换$\frac{y'}{1-y'}=\frac{y}{1-y}\times \frac{m^-}{m^+}$ => $\frac{y'}{1-y'}>1$
        - 这就是再缩放

  - 如何做

    - 训练集是真实样本总体的无偏采样这个假设往往**不成立**

    - 做法1：直接对训练集的反类样例做**”欠采样“（undersampling）**（又称下采样）

      - 去除一些反例使得正、反例数目接近
      - 可能会丢弃一些重要信息
      - **EasyEnsemble**利用集成学习，将反例划分为若干个集合供不同学习器使用
        - 全局来看没有丢失重要信息

    - 做法2：对训练集里的正类样例做**"过采样"（oversampling）**（又称“上采样”）

      - 增加一些正例使得正、反例数目接近，然后再进行学习
      - 不能简单的对初始正例样本进行重复采样，否则会导致过拟合
      - **SMOTE**通过对训练集里的正例进行插值来产生额外的正例

    - 做法3：直接基于原始训练集进行学习

      - **“阈值移动”**：在进行预测时，将$\frac{y'}{1-y'}=\frac{y}{1-y}\times \frac{m^-}{m^+}$嵌入到其决策过程中

    - 比较

      | 方法     | 优点                   | 缺点                   |
      | -------- | ---------------------- | ---------------------- |
      | 欠采样法 | 时间开销小（丢弃反例） | 丢失重要信息           |
      | 过采样法 |                        | 时间开销大（增加正例） |

## 习题

- 3.1 试析在什么情形下式（3.2）中不必考虑偏置项b

  - 式3.2：$f(x)=w^Tx+b$
  - 根据$b=\frac{1}{m} \sum _ {i=1}^{m}\left(y _ {i}-w x _ {i}\right)$，所有标志值之和等于所有加权属性值之和时，偏置项b等于0
  - 根据$y_i'=y_i-y_0=w(x_i-x_0)$当训练的样本集为原样本集中每个样本与任意一个样本之差时，不用考虑b

- 3.2 试证明，对于参数w，对率回归的目标函数（3.18）是非凸的，但其对数似然函数（3.27）是凸的

  - 凸函数就是一个定义在某个向量空间的凸子集C（区间）上的实值函数

    ![](/assets/MLpics/T33.png)

    - 定义

      <img src="/assets/MLpics/T34.png" style="zoom:70%;" />

      - 如果一个多元函数是凸函数，他的Hessian矩阵是半正定的

        - 多元函数的Hessian矩阵就类似一元函数的二阶导。
        - 多元函数Hessian矩阵半正定就相当于一元函数二阶导非负

      - **Hessian（海森）矩阵**

        - $H_{ij}=\frac{\partial^2f(x)}{\partial x_i \partial y_j}$

        ![](/assets/MLpics/T35.png)

      - 半正定：对任意不为0的是实列向量X，都有$X^TAX>=0$

      - 泰勒展开

        <img src="/assets/MLpics/T36.png" style="zoom:80%;" />

        ![](/assets/MLpics/T37.png)

      - Hessian矩阵和凸函数的关系

        - 直接在x0点处二阶展开形式$$\begin{equation}
          f(\mathbf{x})=f\left(\mathbf{x} _{0}\right)+\nabla f\left(\mathbf{x} _{0}\right)\left(\mathbf{x}-\mathbf{x} _{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x} _{0}\right)^{T} \mathbf{H}\left(\mathbf{x}-\mathbf{x} _{0}\right)
          \end{equation}$$
        - H为海森矩阵，$H=\nabla^2 f(x_0)$，也就是把梯度向量推广为二阶形式
          - 因为梯度向量本身也是雅可比行列式$\frac{\partial\left(u_{1}, u_{2}, \cdots, u_{n}\right)}{\partial\left(x_{1}, x_{2}, \cdots, x_{n}\right)}$的特例
        - 通过凸函数图像$f(x) \ge f(x_0)+\nabla f(x_0)(x-x_0)$对于任意的x和x0都成立
        - $\frac{1}{2}(x-x_0)^TH(x-x_0) \ge0$必须对于任意的x和x0都成立（凸函数的二阶条件$\nabla^2 f(x) \ge0$)
          - 也就是说对于任意$\Delta x	$，$\Delta x^TH\Delta x \ge0$恒成立
          - 正是H半正定的充要条件

  - 式3.18： $y=\frac{1}{1+e^{-(w^Tx+b)}}$      $y \in (0,1)$

    - 则对上式一阶求导，可得：

      $$\begin{aligned}\frac{dy}{dw}&=-(1+e^{-(w^Tx+b)})^{-2}\times e^{-(w^Tx+b)}(-x) \\&=\frac{xe^{-(w^Tx+b)}}{(1+e^{-(w^Tx+b)})^2} \\ &=\frac{x(1+e^{-(w^Tx+b)}-1)}{(1+e^{-(w^Tx+b)})^2} \\&= \frac{x}{1+e^{-(w^Tx+b)}}-\frac{x}{(1+e^{-(w^Tx+b)})^2} \\&=x(y-y^2)\end{aligned}$$

    - 对上式二阶求导，可得：
      
      $$\begin{aligned}\frac{d}{dw}(\frac{dy}{dw})&=x[y'-2yy']\\ &=x^Tx[y-y^2-2y^2+2y^3]\\ &=x^Tx[y-3y^2+2y^3]\end{aligned}$$
      
      - $x^Tx$半正定，$x^Tx \ge0$
      - 如果$y-y^2>0$则$y(1-y)<0$，$y>1$
      - 如果$y-3y^2+2y^3>0$则$y(1-2y)(1-y)>0$，则$(1-2y)>0$，$2y<1$，矛盾，所以非凸

  - 式3.27：$\ell(\beta)=\sum _ {i=1}^{m}(-y _ {i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}+\ln (1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}} _ {i}}))$

    - 则：$\frac{d\ell(\beta)}{d \beta}=\sum _ {i=1}^{m}(-y _ {i}  \hat{x} _ {i}+\frac{e^{\beta^T \hat{x} _ i} \hat{x} _ i}{(1+e^{\beta^T \hat{x} _ i}))}$
    - 二阶求导：$\frac{d^2\ell(\beta)}{(d \beta)^2}=\sum _ {i=1}^{m}\frac{\hat{x} _ i^2e^{\beta^T \hat{x} _ i}}{(1+e^{\beta^T \hat{x} _ i})^2}$
      - 上式必大于等于0，所以为凸函数

- 3.3 编程实现对率回归，并给出西瓜数据集$3.0\alpha$的结果

  ```python
  #本部分为训练部分，使用误差反传
  #看起来更接近二次函数，so添加二次项
  def fit_double(X,y,eta=0.01,n_iters=500000,eps=1e-8):
      # 注意是wx+b，要多一行
      beta=np.ones((len(X),1))
  
      # 按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。
      data=np.c_[beta,X]
      
      # 计算密度的平方
      x2=np.square(data[:,[1]])
      # 添加密度平方项
      data=np.c_[data,x2]
      
      weights=np.ones((4,1))
      i_iters=0
      
      while i_iters<n_iters :
          y_sig=sigmoid(data.dot(weight))
          m=y_sig-y  #计算误差值
          weights=weights-data.transpose().dot(m)*eta   #误差反传更新参数
          i_iters+=1
          
      #打印最后的误差值
      print(np.abs(m).sum())
      
      return weights,data
  ```

  结果如下

  <img src="/assets/MLpics/T38.png" style="zoom:50%;" />

- 3.4 选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对率回归的错误率

  - 留一法

    ```python
    #留一法
    def leave_one():
        X,y=ReadExcel('./data.xlsx')
        total=X.shape[0]
        sum=0
        for k in range(total):
            test_index=k #测试集下标
            
            test_X=X[k]
            test_y=y[k]       
            
            train_X=np.delete(X,test_index,axis=0)
            train_y=np.delete(y,test_index,axis=0)
            
            weights,data=fit(train_X,train_y)
            
            y_pre=pre(test_X,weights,1)#代表使用留一法
            sum+=accuracy(y_pre,test_y,1)
            
        print('''LeaveOneOut's Accuracy: ''', sum / total)
    ```

  - 10折交叉验证法

  ```python
  # 10折交叉验证
  def cross_val():
      X,y=ReadExcel('./data.xlsx')
      total=X.shape[0]
      sum=0
      num_split=int(total/10)
      # 把样本分成10等分，依次抽取一个做测试集
      for k in range(10):
          test_index=range(k*num_split,(k+1)*num_split) #测试集下标
          
          test_X=X[test_index]
          test_y=y[test_index]
          
          train_X=np.delete(X,test_index,axis=0)
          train_y=np.delete(y,test_index,axis=0)
          
          weights,data=fit(train_X,train_y)
          
          y_pre=pre(test_X,weights,0)#代表使用非留一法
          sum+=accuracy(y_pre,test_y,0)
          
      print('''10-foldCrossValidation's Accuracy: ''', sum / total)
  ```

  留一法准确率高，运算时间长，准确率在Iris数据集上差距不大

- 3.5

  ```python
  def LDA(X0,X1):
      # X0 8行2列，X1 9行2列
      # mean0 1行2列
      mean0=np.mean(X0,axis=0,keepdims=True)# 仍然保持为矩阵而非vector
      mean1=np.mean(X1,axis=0,keepdims=True)
  
      # 由于最终结果是2x2的矩阵，所以不能按照公式写法
      Sw=(X0-mean0).T.dot(X0-mean0)+(X1-mean1).T.dot(X1-mean1)
  
      Sw=np.array(Sw,dtype='float')# 为了求逆
      omega=np.linalg.inv(Sw).dot((mean0-mean1).T)
  
      return omega
  ```

  结果如下图：

  <img src="/assets/MLpics/T42.png" style="zoom:50%;" />

- 3.6 线性判别分析仅在线性可分数据上能获得理想结果，试设计一个改进方法，使其能较好地用于非线性可分数据

  使用Kernal function

- 3.9 使用OvR和MvM将多分类任务分解为二分类任务求解时，试述为何无需专门针对类别不平衡性进行处理

  - 因为对于OvR和MvM来说，对于每个类进行了相同的处理，拆解出的二分类任务中类别不平衡的影响会相互抵消
  - 以ECOC编码为例，每个生成的二分类器会将所有样本分成较为均衡的二类，使类别不平衡的影响减小。
  - 但是拆解后仍然可能出现明显的类别不平衡现象，比如出现了一个非常大的类和一群小类。

# 决策树

## 基本流程

- 基于树结构进行决策

  <img src="/assets/MLpics/T43.png" style="zoom:50%;" />

- 树结构

  - 结点：包含的样本集合根据属性测试的结果被划分到子节点中
    - 叶结点：决策结果
    - 其余结点：对应一个属性测试
    - 根结点：包含样本全集
  - 路径
    - 从根结点到每个叶结点的路径对应了一个判定测试序列

- 目的

  - 产生一棵泛化能力强，即处理未见示例能力强的决策树

- 流程：**分而治之（divide-and-cunquer）**

  ​	<img src="/assets/MLpics/T44.png" style="zoom:40%;" />

  - 递归返回的情况
    - 当前结点包含的样本全部属于同一类别，无需划分
    - 当前属性集为空/所有样本在所有样本在所有属性上取值相同，无法划分
      - 把当前结点标记为叶结点
      - 并将其类别设定为该结点所含样本最多的类别
      - 利用当前结点的后验分布
    - 当前结点包含的样本集合为空，不能划分
      - 把当前结点标记为叶结点
      - 将其类别设定为其父结点所含样本最多的类别
      - 把父结点的样本分布作为当前结点的先验分布

## 划分选择

- 决策树的关键是上图（4.2）中的第8行-----如何选择最优划分属性

  - 随着划分的不断进行，希望决策树的分支结点所包含的样本尽可能属于同一类别
  - 结点的“纯度”越来越高

- **信息增益**

  - **信息熵**：度量样本集合纯度最常用的一种指标

  - 信息熵定义为

    - 假定当前样本集合D中第k类样本所占的比例为$p_k（k=1,2,...,\vert y \vert）$

      $Ent(D)=-\sum_{k=1}^{\vert y \vert} p_klog_2p_k$

    - $Ent(D)$的值越小，D的纯度越高

    - 约定若$p=0,plog_2p=0$

    - 最小值为0，最大值为$log_2 \vert y \vert$

  - 信息增益

    - 假定离散属性a有V个可能的取值$a^1,a^2,...,a^V$

    - 若使用a来对样本集D进行划分，则会产生V个分支结点

    - 第V个分支结点包含了D中所有在属性a熵取值为$a^v$的样本，也就是$D^v$

    - 计算出$D^v$的信息熵

    - 由于不同的分支结点所包含的样本数不同，给分支结点赋予权重$\vert D^v \vert / \vert D \vert$

    - 样本数越多的分支结点影响越大

    - 属性a对样本集D进行划分所获得的信息增益

      $Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}Ent(D^v)$

    - 信息增益越大，使用属性a进行划分所获得的“纯度提升”越大

      - 信息增益越大，$Ent(D)$越小，$\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}Ent(D^v)$越大
      - 总纯度越低，v划分纯度越高

    - 我们可以用信息增益来进行决策树的划分属性选择

      - 图4.2第八行选择$a_*=arg_{a \in A}maxFain(D,a)$
      - 例如ID3决策树学习算法

    - 案例：

      <img src="/assets/MLpics/T45.png" style="zoom:50%;" />

      - 包含17个训练样例，$\vert y \vert =2$

      - $p_1=\frac{8}{17},p_2=\frac{9}{17}$

        $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_k log_2 p_k \\ =&-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})\\ &=0.998 \end{aligned}$$

      - 计算出当前属性集合中每个属性的信息增益

        - {色泽，根蒂，敲声，纹理，脐部，触感}

        - 如：色泽取值：{青绿，乌黑，浅白}

          - D1(色泽=青绿)

            - 样例：{1,4,6,10,13,17}

            - 其中正例3，反例3

            - $p_1=\frac{3}{6},p_2=\frac{3}{6}$

              $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_klog_2p_k \\ =&-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})\\ &=1.000 \end{aligned}$$

          - D2(色泽=乌黑)

            - 样例：{2,3,7,8,9,15}

            - 其中正例4，反例2

            - $p_1=\frac{4}{6},p_2=\frac{2}{6}$

              $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_klog_2p_k \\ =&-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})\\ &=0.918 \end{aligned}$$

          - D3(色泽=浅白)

            - 样例：{4,11,12,14,16}

            - 其中正例1，反例4

            - $p_1=\frac{1}{5},p_2=\frac{4}{5}$

              $$\begin{aligned} Ent(D) =& -\sum_{k=1}^{\vert y \vert} p_klog_2p_k \\ =&-(\frac{4}{5}log_2\frac{4}{5}+\frac{1}{5}log_2\frac{1}{5})\\ &=0.722 \end{aligned}$$

        - 当前的信息增益

          $$\begin{aligned}Gain(D,a)&=Ent(D)-\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}Ent(D^v) \\ &=0.998-(\frac{6}{17} \times 1+\frac{6}{17} \times 0.918+\frac{5}{17} \times 0.722)  \\ &= 0.109\end{aligned}$$

          <img src="/assets/MLpics/T46.png" style="zoom:50%;" />

        - 属性“纹理”的信息增益最大，被选为划分属性

          <img src="/assets/MLpics/T47.png" style="zoom:50%;" />

        - 然后分别计算这三个类别的剩下六个属性的信息增益做进一步划分

          <img src="/assets/MLpics/T48.png" style="zoom:50%;" />

          <img src="/assets/MLpics/T49.png" style="zoom:50%;" />

- **增益率**

  - 如果我们把编号也作为一个候选划分属性，则计算其信息增益为0.998

    - 因为编号产生了17个分支，每个分支仅包含一个样本
    - 这些分支结点的纯度已经达到最大
    - 决策树不具有泛化能力，无法对新样本进行有效预测

  - 信息增益准则对可取值数目较多的属性有所偏好

    - **C4.5**决策树不直接使用信息增益，而是使用增益率来选择最优划分属性

  - 增益率定义为

    $Gain\_ {ratio}(D,a)=\frac{Gain(D,a)}{IV(a)}$

    - 属性a的**固有值（intrinsic value）**

      $IV(a)=-\sum_{v=1}^{V}\frac{\vert D^v \vert}{\vert D \vert}log_2\frac{\vert D^v \vert}{\vert D \vert}$

      - V越大，IV(a)的值越大

  - 增益率准则对可取值数目较少的属性有所偏好

  - C4.5并不是直接选择增益率最大的候选划分属性

    - 使用了**启发式**：从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的

- 基尼指数（Gini index）

  - 数据集D的纯度可用基尼值来度量

    $$\begin{aligned}Gini(D) &=\sum^{\vert y \vert} _{k=1}\sum_{k'\not=k}p_kp_{k'} \\ &= 1-\sum^{\vert y \vert} _{k=1}p_k^2\end{aligned}$$

  - Gini(D)反映了从数据集中随意抽取两个样本，其类别标记不一致的概率

  - Gini(D)越小，数据集D的纯度越高

  - $Gini \_ index(D,a)=\sum_{v=1}^V\frac{\vert D^v \vert}{\vert D \vert}Gini(D^v)$

  - **CART**决策树

    - 不会严格按照上面公式来选择最优划分属性

    - CART决策树是一颗二叉树

    - 构造算法

      - 对每个属性a的可能取值v，将D分为$a=v,a\not=v$两部分

        $Gini \_index(D,a)=\frac{\vert D^{a=v} \vert}{\vert D \vert}Gini(D^{a=v})+\frac{\vert D^{a\not=v} \vert}{\vert D \vert}Gini(D^{a\not=v})$

      - 选择基尼指数最小的属性作为最优划分属性（其取值为最优划分点）

      - 重复以上两步

    - 案例

      - 色泽取值：{青绿，乌黑，浅白}

        - D1(色泽=青绿)
          - 样例：{1,4,6,10,13,17}
          - 其中正例3，反例3
          - $p_1=\frac{3}{6},p_2=\frac{3}{6}$
        - D2(色泽 !=青绿)
          - 样例：{2,3,5,7,8,9,11,12,14,15,16}
          - 其中正例5，反例6
          - $p_1=\frac{5}{11},p_2=\frac{6}{11}$
        - $Gini \_index(D,a)=\frac{6}{17}(1-(\frac{3}{6})^2-(\frac{3}{6})^2)+\frac{11}{17}(1-(\frac{5}{11})^2-(\frac{6}{11})^2)=0.497$

        <img src="/assets/MLpics/T50.png" style="zoom:50%;" />

  - 在候选集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性

    - $a_*=arg_{a \in A}min Gini\_index(D,a)$

## 剪枝处理

- 决策树学习算法对付“过拟合”的主要手段

- 决策树学习中可能会造成分支过多的问题，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合

- 剪枝策略

  - **预剪枝（prepruning）**

    - 案例：根据信息增益准则

      - 选择“脐部”来进行划分，产生三个分支

        <img src="/assets/MLpics/T51.png" style="zoom:50%;" />

      - 对划分前后的泛化性能进行估计

        - 划分之前，所有样例集中在根结点，若不进行划分，则该结点将被标记为叶结点，其类别标记为训练样例数最多的类别
          - 若我们将这个叶结点标记为“好瓜”
          - 则编号为{4,5,8}的样例被分类正确，另外4个样例分类错误
          - 验证集精度为3/7 x 100%=42.9%

      - 图4.6中的结点2、3、4

        - 分别包含编号为{1,2,3,14}、{6,7,15,17}、{10,16}的训练样例
        - 这三个结点分别被标记为“好瓜”，“好瓜”，“坏瓜”
        - 此时验证集中编号为{4,5,8,11,12}的样例被分类正确
        - 验证集精度为5/7 x 100%=71.4%

      - 结点2进行划分

        - 基于信息增益准则挑选出“色泽”
        - 编号为{5}的验证集样本分类由正确转为错误，精度下降
        - 禁止结点2被划分

      - 结点3进行划分

        - “根蒂”
        - 验证集精度不变，不能提升
        - 禁止结点3被划分

      - 结点4所含训练样例已属于同一类，不再进行划分

      - 结果：仅有一层划分的决策树，别名**“决策树桩”（decision stump）**

    - 优缺点

      - 优点：使得决策树的很多分支都没有“展开”
        - 降低了过拟合的风险
        - 显著减少了训练时间和测试时间开销
      - 缺点：
        - 在当前划分基础上的后续划分却有可能导致性能显著提高
        - 预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险

  - **后剪枝（post-pruning）**

    - 先从训练集生成一棵完整决策树，如上图4.5

      <img src="/assets/MLpics/T52.png" style="zoom:50%;" />

    - 首先考察结点6

      - 将其领衔的分支剪除，相当于把6替换为叶结点
      - 替换后的叶结点包含编号为{7,15}的训练样本
      - 该类别标记为“好瓜”
      - 决策树的验证集精度提高至57.1%
      - 决定剪枝

    - 结点5

      - 其领衔的子树替换为叶结点
      - 替换后的叶结点包含编号为{6,7,15}的训练样例
      - 叶结点类别标记为“好瓜”
      - 决策树的验证集精度仍为57.1%
      - 不剪枝（但其实根据奥卡姆剃刀准则，应该剪枝，但本书因为绘图不便没有剪枝）

    - 结点2

      - 其领衔的子树替换为叶节点
      - 替换后的叶结点包含编号为{1,2,3,14}的训练样例
      - 叶结点标记为“好瓜”
      - 决策树的验证集精度提高至71.4%
      - 决定剪枝

    - 结点3和1

      - 其领衔的子树替换为叶节点
      - 精度分别为71.4%和42.9%
      - 并未提高，得到保留

    - 结果

      - 验证集精度为71.4%

      <img src="/assets/MLpics/T53.png" style="zoom:50%;" />

    - 优缺点

      - 优点：
        - 欠拟合风险很小
        - 泛化性能优于预剪枝决策树
      - 缺点：
        - 在生成完全决策树之后进行
        - 要自底向上地对树中的所有非叶结点逐一进行考察
        - 训练时间开销比未剪枝决策树和预剪枝决策树都要大得多

## 连续与缺失值

- 连续值处理

  - **二分法**

    - C4.5决策树算法中采用的机制

    - 给定样本集D和连续属性a

    - 假定a在D上出现了n个不同的取值，将它们从小到大进行排序，记为$\{ a^1,a^2,...,a^n \}$

    - 基于划分点t可将D分为子集$D_t^-,D_t^+$

      - $D_t^-$包含那些在属性a上取值不大于t的样本
      - $D_t^+$包含那些在属性a上取值大于t的样本

    - 对于相邻属性取值$a^i,a^{i+1}$来说，t在区间$[a^i,a^{i+1})$中取任意值所产生的划分结果相同

    - 对于连续属性a，我们可考察一个包含n-1个元素的候选划分点集合

      $T_a= \{ \frac{1^i+a^{i+1}}{2} \vert 1 \le i \le n-1\}$

      - 把区间中位点作为候选划分点

      - 像离散属性值一样来考察这些划分点，选取最优

      - 例如针对信息增益公式改造

        $$\begin{aligned} Gain(D,a) &=max_{t \in T_a} Gain(D,a,t) \\ &= max_{t \in T_a} Ent(D)-\sum_{\lambda \in \{ -,+\}}\frac{\vert D^\lambda_t \vert}{\vert D \vert}Ent(D^\lambda_t)\end{aligned}$$

        - 其中Gain(D,a,t)是样本集D基于划分点t二分后的信息增益
        - $\lambda = -$时，$D^\lambda_t=D^{a \le t} _t$
        - $\lambda = +$时，$D^\lambda_t=D^{a > t} _t$

    - 案例：

      - 在西瓜数据集2.0加上连续属性“密度”和“含糖率”，得到西瓜数据3.0

        <img src="/assets/MLpics/T54.png" style="zoom:50%;" />

        - 密度

          - 划分点集合包含16个候选值
          - T密度={0.244,0.294,0.351,0.381,0.42,0.459,0.518,0.574,0.6,0.621,0.636,0.648,0.661,0.681,0.708,0.746}
          - 信息增益为0.262，对应划分点0.381

        - 含糖率

          - 划分点集合包含16个候选值
          - T密度={0.049,0.074,0.095,0.101.0.126,0.155,0.179,0.204,0.213,0.226,0.250,0.265,0.292,0.344,0.373,0.418}
          - 信息增益为0.349，对应划分点0.126

          <img src="/assets/MLpics/T55.png" style="zoom:50%;" />

  - 不同于离散属性，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性

    - 例如在父结点上使用了密度<=0.381，不会禁止在子结点上使用密度<=0.294

- 缺失值处理

  - 如果直接放弃不完整样本

    <img src="/assets/MLpics/T56.png" style="zoom:50%;" />

    - 仅有四个样本可以使用

  - 如何在属性值缺失的情况下进行划分属性选择

    - 给定训练集D和属性a

    - $\tilde{D}$表示D在属性a上没有缺失值的样本子集，仅根据其判断属性a的优劣

    - 假定属性a有V个可取值$\{ a^1,a^2,...,a^n \}$

    - $\tilde{D}^v$表示$\tilde{D}$中在属性a上取值为$a^v$的样本子集

    - $\tilde{D} _k$表示$\tilde{D}$中属于第k类$(k=1,2,....,\vert y \vert)$的样本子集

    - $\tilde{D}=\bigcup_{k=1}^{\vert y \vert} \tilde{D} _{k}$

    - $\tilde{D}=\bigcup_{v=1}^V \tilde{D}^v$

    - 假设我们为每个样本x赋予一个权重$w_x$

    - $\rho=\frac{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in D} w_{\boldsymbol{x}}}$

        - 对属性a，其表示无缺失值样本所占的比例
          - $\tilde{p} _ k=\frac{\sum_{\boldsymbol{x} \in \tilde{D} _ k} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}$       $(1 \le k \le \vert y \vert)$

        - 对属性a，表示无缺失值样本中第k类所占的比例
            - $\sum_{k=1}^{\vert y \vert} \tilde{p} _k=1$
            - $\tilde{r} _ v=\frac{\sum _ {\boldsymbol{x} \in \tilde{D}^v} w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x} \in \tilde{D}} w_{\boldsymbol{x}}}$          $(1 \le v \le V)$
        - 对属性a，表示无缺失值样本中在属性a上取值$a^v$的样本所占的比例
          - $\sum_{v=1}^V \tilde{r} _v=1$

    - 4.2（信息增益）公式推广

      $$\begin{aligned} Gain(D,a)&=\rho \times Gain(\tilde{D},a) \\ &=\rho \times (Ent(\tilde{D}-\sum_{v=1}^V \tilde{r} _v Ent(\tilde{D}^v)))\end{aligned}$$

    - $Ent(\tilde{D}) = -\sum_{k=1}^{\vert y \vert} \tilde{p} _k log_2 \tilde{p} _k$

  - 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分

    - 若样本x在划分属性a上的取值已知，则将x划入与其取值对应的子结点，且样本权值在子结点中保持为$w_x$
    - 若样本x在划分属性a上的取值未知，则将x同时划入所有子结点，样本权值在与属性值$a^v$对应的子结点中调整为$\tilde{r} _v·w_x$

  - 案例：（以表4.4为例）

    - 开始时，各样例的权值均为1

    - 色泽

      - 该属性上无缺失值的样例子集$\tilde{D}$包含编号为{2,3,4,5,7,8,9,10,11,12,14,15,16,17}

      - $\tilde{D}$的信息熵为

        $$\begin{aligned}Ent(\tilde{D})&=-\sum_{k=1}^2 \tilde{p} _k log_2 \tilde{p} _k \\ &=-(\frac{6}{14}log_2 \frac{6}{14}+ \frac{8}{14}log_2\frac{8}{14}) =0.985\end{aligned}$$

      - 令$\tilde{D}^1,\tilde{D}^2,\tilde{D}^3$分别表示在青绿，乌黑和浅白的样本子集

        - $Ent(\tilde{D}^1)=-(\frac{2}{4}log_2 \frac{2}{4}+\frac{2}{4}log_2 \frac{2}{4})=1.000$
        - $Ent(\tilde{D}^2)=-(\frac{4}{6}log_2 \frac{4}{6}+\frac{2}{6}log_2 \frac{2}{6})=0.918$
        - $Ent(\tilde{D}^3)=-(\frac{0}{4}log_2 \frac{0}{4}+\frac{4}{4}log_2 \frac{4}{4})=0.000$

      - 样本子集$\tilde{D}$上色泽的信息增益为：

        $$\begin{aligned}Gain(\tilde{D},色泽) &= Ent(\tilde{D})-\sum_{v=1}^3 \tilde{r} _v Ent(\tilde{D}^v) \\ &= 0.985-(\frac{4}{14} \times 1.000 + \frac{6}{14} \times 0.918 +\frac{4}{14} \times 0.000) \\ &=0.306\end{aligned}$$

      - 样本集D上色泽的信息增益为：

        $Gain(D,色泽)=\rho \times Gain(\tilde{D},色泽)=\frac{14}{17} \times 0.306 =0.252$

    - <img src="/assets/MLpics/T57.png" style="zoom:50%;" />

    - 纹理信息增益最大，被用于对根结点进行划分

      - 使编号为{1,2,3,4,5,6,15}的样本进入“纹理=清晰”分支

      - {7,9,13,14,17}进入“纹理=稍糊”分支

      - {11,12,16}进入“纹理=模糊”分支

      - 样本在各子结点中的权重保持为1

      - 8样本在“纹理”上出现了缺失值，所以进入三个分支中，权重在三个结点中调整为7/15，5/15，3/15

      - 编号为10的样本与上类似

      - 结果：

        <img src="/assets/MLpics/T58.png" style="zoom:50%;" />

## 多变量决策树

- **轴平行（axis-parallel）**

  - 若我们把每个属性视为坐标空间中的一个坐标轴

  - d个属性描述的样本就对应了d维空间的一个数据点

  - 对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界

  - 轴平行是指它的分类边界由若干个与坐标轴平行的分段组成

  - 如图（只看密度和含糖率）

    <img src="/assets/MLpics/T54.png" style="zoom:50%;" />

  - 生成的决策树和分类边界

    <img src="/assets/MLpics/T59.png" style="zoom:50%;" />

  - 分类边界的每一段都与坐标轴平行，这样的分类边界使学习结构有较好的可解释性

    - 每一段划分都直接对应了某个属性取值

    - 但真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似

    - 如下图，决策树更加复杂，需要更多的属性测试，预测时间开销变大

      <img src="/assets/MLpics/T60.png" style="zoom:50%;" />

- **多变量决策树（multivariate decision tree）**

  - 亦称斜决策树（oblique decision tree）

  - 使用**斜划分**边界，如上图，决策树模型将大为简化

  - 非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试

  - 每个非叶结点是一个形如$\sum_{i=1}^d w_i a_i =t$的线性分类器

    - $w_i和t$可在该结点所含的样本集和属性集上学得

  - 与传统的“单变量决策树（univariate decision tree）”不同，在多变量决策树的学习过程中

    - 不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器

    - 针对西瓜数据3.0

      <img src="/assets/MLpics/T61.png" style="zoom:50%;" />

## 习题

- 4.1 试证明对于不含冲突数据（即特征向量完全相同但标记不同）的训练集，必存在与训练集一致（即训练误差为0）的决策树

  - 不含冲突数据
  - 则相同属性特征的样本进入同一叶子结点
  - 必存在与训练集一致的决策树

- 4.2 试析使用“最小训练误差”作为决策树划分选择准则的缺陷

  - 过拟合

- 4.3 试变成实现基于信息熵进行划分选择的决策树算法，并为表4.3中数据生成一棵决策树

  <img src="/assets/MLpics/T54.png" style="zoom:50%;" />

  ```python
  # 根据书中伪代码写function即可
  def treeGenerate(D,A,root,lastNode,lastA):
      flag, category = same_category(D)
      if flag:
          if category==1:
              lastNode[lastA] = '好瓜'  
          else:
              lastNode[lastA] = '坏瓜'
          return
      
      if empty_attribute(A) or sameOnA(D,A):
          lastNode[lastA]=findMost(D)
          return
      
      best_a=findBestA(D,A)
  
      root[best_a]={}
      for av in pd.unique(data.loc[:,best_a]):
          Dv = getDv_a(D,best_a,av)
          if Dv.shape[0]==0:
              root[best_a][av]=findMost(D)
          else:
              A_ = A.drop(best_a)
              root[best_a][av] = {}
              lastA = av
  
              treeGenerate(Dv,A_,root[best_a][av],root[best_a],lastA)
  ```

  ​	结果如下：

  ​	<img src="/assets/MLpics/T62.png" style="zoom:50%;" />

- 4.4 试实现基于基尼指数进行划分选择的决策树算法，为表4.2中数据生成预剪枝、后剪枝决策树，并与未剪枝决策树进行比较

  - 未剪枝

    ```python
    def treeGenerate(D,A,root,lastNode,lastA):
        flag, category = same_category(D)
        if flag:
            if category==1:
                lastNode[lastA] = '好瓜'  
            else:
                lastNode[lastA] = '坏瓜'
            return
        
        if empty_attribute(A) or sameOnA(D,A):
            lastNode[lastA]=findMost(D)
            return
        
        best_a=findBestA(D,A)
    
        root[best_a]={}
        for av in pd.unique(data.loc[:,best_a]):
            Dv = getDv_a(D,best_a,av)
            if Dv.shape[0]==0:
                root[best_a][av]=findMost(D)
            else:
                A_ = A.drop(best_a)
                root[best_a][av] = {}
                lastA = av
                
                treeGenerate(Dv,A_,root[best_a][av],root[best_a],lastA)
    ```

  ​	

  - 预剪枝

    ```python
    # 预剪枝
    def prePruning(D,Dtest,A,root,lastNode,lastA):
        # 如果基尼指数为0，即D中样本全属于同一类别，返回
        flag, category = same_category(D)
        if flag:
            if category==1:
                lastNode[lastA] = '好瓜'  
            else:
                lastNode[lastA] = '坏瓜'
            return
        
        if empty_attribute(A) or sameOnA(D,A):
            lastNode[lastA]=findMost(D)
            return
        best_a=findBestA(D,A)
        
        accCnt=calAccNum(D,Dtest)
        
        # 如果不划分的正确率更高，则不划分
        if cmpAcc(D,Dtest,accCnt,best_a,1):
            root[best_a]={}
        else:
            lastNode[lastA]=findMost(D)
            return
        
        for av in pd.unique(data.loc[:,best_a]):
    
            Dv = getDv_a(D,best_a,av)
            
            Dv_test=getDv_a(Dtest,best_a,av)
            
            if Dv.shape[0]==0:
                root[best_a][av]=findMost(D)
            else:
                A_ = A.drop(best_a)
                root[best_a][av] = {}
                lastA = av
                
                prePruning(Dv,Dv_test,A_,root[best_a][av],root[best_a],lastA)
    ```

    

  - 后剪枝

    ```python
    # 后剪枝
    def postPruning(D,Dtest,A,root,lastNode,lastA):
        
        flag, category = same_category(D)
        if flag:
            if category==1:
                lastNode[lastA] = '好瓜'  
            else:
                lastNode[lastA] = '坏瓜'
            return
        
        if empty_attribute(A) or sameOnA(D,A):
            lastNode[lastA]=findMost(D)
            return
        
        best_a=findBestA(D,A)
        if lastA!=None:
            lastlastA=lastA
        else:
            lastlastA=None
            
        root[best_a]={}
        for av in pd.unique(data.loc[:,best_a]):
            Dv = getDv_a(D,best_a,av)
            Dv_test=getDv_a(Dtest,best_a,av)
            if Dv.shape[0]==0:
                root[best_a][av]=findMost(D)
            else:
                A_ = A.drop(best_a)
                root[best_a][av] = {}
                lastA = av
    
                postPruning(Dv,Dv_test,A_,root[best_a][av],root[best_a],lastA)
        
    
        # 针对叶子结点开始剪枝
        
        accCnt=calAccNum(D,Dtest)# 计算剪枝的正确率
    
        # 如果不划分的正确率更高，则不划分
        if cmpAcc(D,Dtest,accCnt,best_a,0)==False:
            #lastNode[lastA]=findMost(D)
            lastNode[lastlastA]=findMost(D)
    
            return
    ```

    

  - 结果

    <img src="/assets/MLpics/T63.png" style="zoom:70%;" />

- 4.5 试编程实现基于对率回归进行划分选择的决策树算法，并为表4.3中数据生成一棵决策树

  ```python
  # 对率回归核心算法
  def sigmoid(Z):
      return 1.0/(1+np.exp(-Z))
  
  def gradDescent(data,label,eta=0.1,n_iters=500):
      m,n=data.shape
      label=label.reshape(-1,1)
      
      beta=np.ones((n,1))
      
      for i in range(n_iters):
          y_sig=sigmoid(data.dot(beta))
          m=y_sig-label  #计算误差值
          beta=beta-data.transpose().dot(m)*eta   #误差反传更新参数
          
      return beta
  ```

  ```python
  # 决策树生成算法
  def treeGenerate(D,root,lastNode,lastBeta):
      flag, category = same_category(D)
      if flag:
          if category==1:
              lastNode[lastBeta] = '好瓜'  
          else:
              lastNode[lastBeta] = '坏瓜'
          return
      
      if len(D[0])==1:
          lastNode[lastBeta]=findMost(D)
          return
      
      bestBeta=gradDescent(D[:, :-1], D[:, -1])
  
  
      nodeTxt=""
  
      for i in range(len(bestBeta)):
          if i==0:
              continue
          else:
              nodeTxt+="w"+str(i)+" "+str(bestBeta[i][0])+' \n '
              
      nodeTxt+="<=" + str(-bestBeta[0][0])
      
      root[nodeTxt]={nodeTxt:{}}
      #print(root[nodeTxt],'\n')
      
      Dv_b1,Dv_b2=getDv_b(D,bestBeta)
      class1="是"
      class2="否"
      # 根据beta进行数据集分割
      root[nodeTxt][class1] = {}
      root[nodeTxt][class2] = {}
      treeGenerate(Dv_b1,root[nodeTxt][class1],root[nodeTxt],class1)
      
      treeGenerate(Dv_b2,root[nodeTxt][class2],root[nodeTxt],class2)
  ```

- 4.7 图4.2是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致栈溢出。试使用“队列”数据结构，以参数MaxDepth控制树的最大深度，写出与图4.2等价，但不使用递归的决策树生成算法

  - 如题意，使用队列保存结点
  - 初始化一个队列，并将头结点放入队列中。
  - 用一个while循环，当队列为空时停止。
  - 让一个节点出队列作为当前节点
    - 如果当前节点中的数据都为一类，则把该节点设置为叶子节点。
    - 如果数据集只剩下类，也把当前节点设置为叶子节点，findMaxLabel
  - 如果当前节点的深度小于MaxSize，则继续划分。否则得到叶子节点。



# 神经网络

## 神经元模型

- **神经网络（neural networks）**是具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应

- 成分

  - **神经元（neuron）**：最基本的，相当于定义中的“简单单元”

- **M-P神经元模型**：

  ​	<img src="/assets/MLpics/T64.png" style="zoom:50%;" />

  - 神经元接收到来自n个其他神经元传递过来的输入信号
  - 这些输入信号通过带权重的连接进行传递
  - 神经元接收到的总输入值将与神经元的阈值进行比较
  - 通过**激活函数（activation function）**处理以产生神经元的输出

- 激活函数

  - 理想：阶跃函数

    - 1对应神经元兴奋
    - 0对应神经元抑制
    - 但具有不连续，不光滑等不佳性质

  - 实际：Sigmoid（别名挤压函数（squashing function））

    <img src="/assets/MLpics/T65.png" style="zoom:50%;" />

## 感知机与多层网络

- **感知机（Perceptron）**

  - 两层神经元组成

    <img src="/assets/MLpics/T66.png" style="zoom:50%;" />

  - 输入层接收外界输入信号传递给输出层

  - 输出层是M-P神经元，亦称**阈值逻辑单元（threshold logic unit）**

    - 感知机只有输出层神经元进行激活函数处理
    - 只有一层**功能神经元（functional neuron）**

  - $y=f(\sum_i w_i x_i-\theta)=f(w^Tx-\theta)$（假设f为阶跃函数）

  - 可进行逻辑与、或、非运算

    - 均属于**线性可分（linearly separable）**问题

    - 与运算（x1^x2）

      - 令$w_1=w_2=1,\theta=2,y=f(1* x _ 1+1* x _ 2-2)$
      - 仅$x_1=x_2=1,y=1$

    - 或运算（x1 v x2）

      - 令$w_1=w_2=1,\theta=0.5,y=f(1* x _1+1 * x _ 2-0.5)$
      - 仅$x_1=1 \  or \  x_2=1,y=1$

    - 非运算<img src="/assets/MLpics/T67.png" style="zoom:50%;" align="left" />

      - 令$w_1=-0.6,w_2=0,\theta=-0.5,y=f(-0.6*x_1+0.5)$
      - 仅$x_1=1,y=0$
      - $x_1=0,y=1$

    - 若两类模式是线性可分的，即存在一个线性超平面能将它们分开

      - n维空间的超平面方程为$w_1x_1+...w_nx_n=w^Tx+b=0$
      - $w^Tx-\theta$可以看作是n维空间中的一个超平面，将n维空间划分为
        - $w^Tx-\theta \ge 0$ 样本模型输出值为1
        - $w^Tx-\theta <0$ 样本模型输出值为0

      <img src="/assets/MLpics/T68.png" style="zoom:50%;" />

    - 感知机的学习过程一定会收敛（converge）而求得适当的权向量w

    - 否则学习过程会发生振荡（fluctuation），w难以稳定下来，如上图中的异或问题

  - 学习

    - 给定训练数据集，权重$w_i(i=1,2,...,n)$以及阈值$\theta$可以通过学习得到
      - 阈值作为一个固定输入为-1.0的**哑结点（dummy node）**所对应的权重$w_{n+1}$
      - 权值和阈值的学习统一为权重的学习
    - 损失函数
      - 易得$(y-\hat{y})(w^T x-\theta) \ge 0$恒成立
      - $L(w,\theta)=\sum_{x \in M}(y-\hat{y})(w^T x-\theta)$
        - M为误分类样本集合
        - 该公式连续可导
          - 没有误分类点，损失函数值为0
          - 误分类点越少，则误分类点离超平面越近，损失函数值越小
      - 求参数
        - 极小化$min_{w,\theta}L(w,\theta)=min_{w,\theta}\sum_{x_i \in M}(\hat{y_i}-{y_i})(w^T x_i-\theta)$
        - 把阈值看为哑结点，简化$w^Tx_i-\theta=w^Tx_i$
          - $min_{w}L(w,\theta)=min_{w}\sum_{x_i \in M}(\hat{y_i}-y_i)w^T x_i$
        - 假设M固定，求梯度
          - $\Delta_wL(w)=\sum_{x_i \in M}(\hat{y_i}-y_i)x_i$
      - 对训练样例（x，y），若当前感知机的输出为$\hat{y}$
        - $w_i \leftarrow w_i+\Delta w_i$
        - 参数更新公式：$\Delta w_i=\eta(y-\hat{y})x_i$
    - **学习率（learning rate）**：$\eta \in (0,1)$
    - 权重调整
      - 若$\hat{y}=y$，感知机不发生变化
      - 根据错误的程度进行权重调整

  - 多层功能神经元

    - 解决非线性可分问题

      <img src="/assets/MLpics/T69.png" style="zoom:50%;" />

    - **隐（含）层（hidden layer）**：也是拥有激活函数的功能神经元

    - **多层前馈神经网络（multi-layer feedforward neural networks）**

      - A别称两层网络（本书称为单隐层网络）

        <img src="/assets/MLpics/T70.png" style="zoom:50%;" />

        - 输入层：接收外界输入
        - 隐藏层，输出层（功能神经元）：对信号进行加工
        - 输出层：输出结果

      - 学习过程就是根据训练数据来调整神经元之间的**“连接权”（connection weight）**以及每个功能神经元的阈值

## 误差逆传播算法

- 误差逆传播（error BackPropagation，简称BP）

  - 训练多层网络的学习算法
  - 虽然可用于其他类型神经网络训练，但通常指的是多层前馈

- 算法思想

  - 给定训练集$D=\{ (x_1,y_1),...(x_m,y_m)\},x_i \in R^d，y_i \in R^l$

    - 输入示例由d个属性描述，输出l维实值向量

  - 下图有d个输入神经元，l个输出，q个隐藏层神经元

    <img src="/assets/MLpics/T71.png" style="zoom:50%;" />

    - $\theta_j$：输出层第j个神经元的阈值
    - $\gamma_h$：隐藏层第h个神经元的阈值
    - $v_{ih}$：输入层第i个神经元与隐藏层第h个神经元之间的连接权
    - $w_{hj}$：隐藏层第h个神经元与输出层第j个神经元之间的连接权
    - $b_h$：隐藏层第h个神经元的输出
    - 使用sigmoid
    - 第h个隐藏层神经元的输入和第j个输出神经元的输入见上图

  - 对训练样例$(x_k,y_k)$，假定$\hat{y_k}=(\hat{y_1}^k,\hat{y_2}^k...\hat{y_l}^k)$

    - $\hat{y_j}^k=f(\beta_j-\theta_j)$

    - 则网络在样例上的均方误差为

      $E_k=\frac{1}{2}\sum_{j=1}^l(\hat{y_j}^k-y_j^k)^2$

  - 上图的网络中有 (d+l+1)q+l个参数需要确定

    - 输入层到隐藏层：dq
    - 隐藏层：q
    - 隐藏层到输出层：ql
    - 输出层：l

  - BP算法公式推导

    - 迭代学习

    - 基于梯度下降，以目标的负梯度方向对参数进行调整

    - 在每一轮中采用广义感知机对参数进行更新估计：$v \leftarrow v+\Delta v$

    - 以上图中$w_{hj}$为例

      - $\Delta w_{hj}=- \eta \frac{\delta E_k}{\delta w_{hj}}$

      - 先影响到第j个输出层神经元的输入值$\beta_j$，再影响到其输出值$\hat{y_j}^k$

        - $\frac{\delta E_k}{\delta w_{hj}}=\frac{\delta E_k}{\delta \hat{y_j}^k}·\frac{\delta  \hat{y_j}^k}{\delta \beta_j}·\frac{\delta \beta_j}{\delta w_{hj}}$

        - $\frac{\delta \beta_j}{\delta w_{hj}}=b_h$

        - sigmoid函数的性质$f'(x)=f(x)(1-f(x))$

          $$\begin{aligned}g_j &=-\frac{\delta E_k}{\delta \hat{y_j}^k}·\frac{\delta  \hat{y_j}^k}{\delta \beta_j} \\ &=-(\hat{y_j}^k-y_j^k)f'(\beta_j-\theta_j) \\ &=\hat{y_j}^k(1-\hat{y_j}^k)(y_j^k-\hat{y_j}^k) \end{aligned}$$

        - $\Delta w_{hj}=- \eta g_j b_h$

        - 同理可得（不懂推导的童鞋可以参考南瓜书）

          - $\Delta \theta_j=-\eta g_j$

            - 个人的简单理解：和上方$w_{hj}$同公式，但$\theta$和输入值无关，所以不需要$b_h$

          - $\Delta v_{ih}=\eta e_h x_i$

            - 非常类似于$w_{hj}$的推导，只是这次为求输入层和隐藏层的连接权

          - $\Delta \gamma_h=-\eta e_h$

            - 同$\theta$

          - 注意$e_h$

            $$\begin{aligned} e_h &=-\frac{\delta E_k}{\delta b_h}·\frac{\delta b_h}{\delta \alpha_h} \\ &=-\sum_{j=1}^l \frac{\delta E_k}{\delta \beta_j}·\frac{\delta \beta_j}{\delta b_h} f'(\alpha_h-\gamma_h) \\ &=\sum_{j=1}^l w_{hj}g_j f'(\alpha_h-\gamma_h) \\ &=b_h(1-b_h)\sum_{j=1}^l w_{hj}g_j\end{aligned}$$

          - 注意$\eta$

            - 学习率控制者每一轮迭代中的更新步长，
              - 太大容易振荡
              - 太小收敛速度会过慢
            - 精细调节
              - $w_{hj},\theta$和$v_{ih},\gamma_h$使用的$\eta$可以不同

  - BP算法的工作流程

    - BP算法的目标：最小化训练集D上的累积误差$E=\frac{1}{m}\sum_{k=1}^m E_k$
    - 标准BP算法：每次仅针对一个训练样例更新连接权和阈值
      - 也就是说下图的更新规则是基于单个$E_k$推导得到
      - 参数更新频繁
      - 对不同样例可能会出现抵消现象
      - 为了达到同样的累积误差最小点，需要更多次的迭代
      - 对应**随机梯度下降（stochastic gradient descent，简称SGD）**

    <img src="/assets/MLpics/T72.png" style="zoom:50%;" />

    结果示例：

    <img src="/assets/MLpics/T73.png" style="zoom:50%;" />

  - **累积误差逆传播（accumulated error backpropagation）**

    - 基于累积误差最小化的更新规则
    - 在读取整个训练集D一遍后才对参数进行更新
    - 更新频率更低
    - 但下降到一定程度之后，进一步下降会非常缓慢，这时标准BP会更快获得较好的解（在D非常大时更明显）
    - 对应**标准梯度下降**

- **试错法（trial-by-error）**

  - **只需要一个包含足够多神经元的隐藏层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数**
  - 如何设置隐藏层神经元的个数

- 过拟合缓解

  - **早停（early stopping）**
    - 将数据氛围训练集和验证集
    - 训练集：计算梯度、更新连接权和阈值
    - 验证集：估计误差
    - 训练集误差降低但验证集误差升高，则停止训练，返回连接权和阈值
  - **正则化（regularization）**
    - 在误差目标函数中增加一个用于描述网络复杂度的部分
    - 例如连接权与阈值的平方和
    - $E=\lambda \frac{1}{m}\sum_{k=1}^mE_k+(1-\lambda)\sum_i w_i^2$
      - $\lambda \in (0,1)$
        - 使用交叉验证进行估计
        - 用于对经验误差与网络复杂度这两项进行折中

## 全局最小与局部极小

- E表示训练集上的误差，训练过程为在参数空间中，寻找一组最优参数使得E最小

  - 若存在$w ^ *,\theta ^ *,\epsilon>0$使得

    $\forall (w;\theta) \in \{ (w;\theta) \vert \ \vert\vert (w;\theta)-(w^ *;\theta^ *) \vert\vert \le \epsilon \}$

- **局部极小（local minimum）**
  - 对上式都有$E(w;\theta) \ge E(w^ *;\theta^ *)$成立，$(w^ *;\theta^ *)$为局部最小解
  - 其邻域点的误差函数值均不小于该点的函数值
  - 对应的$E(w^ *;\theta^ *)$为局部极小值
  - 梯度为0，误差函数值小于邻点
  - 可能存在多个
- **全局最小（global minimum）**
  - 对参数空间的任意$(w;\theta)$都有$E(w;\theta) \ge E(w^ *;\theta^ *),(w^ *;\theta^ *)$为全局最小解
  - 所有点的误差函数值均不小于该点的函数值
  - 对应的$E(w^ *;\theta^ *)$为全局最小值
  - 只能存在一个
  - 一定是局部极小

<img src="/assets/MLpics/T74.png" style="zoom:50%;" />

- 基于梯度的搜索
  - 最广泛的参数寻优
  - 思路
    - 从初始解出发，迭代寻找最优参数
    - 每次迭代中
      - 先计算误差函数在当前点的梯度
      - 根据梯度决定搜索方向
        - 负梯度方向是函数值下降最快的方向
      - 若当前点的梯度为0，则达到局部最小，参数的迭代在这里停止
    - 找到全局最小（大多启发式，理论上缺乏保障）
      - 只有一个局部极小=全局最小
      - 多个局部极小，则试图“跳出”局部极小
        - 方法1：以多组不同参数初始化多个神经网络，取误差最小的解最为最终参数（需要陷入不同的局部极小）
        - 方法2：**模拟退火（simulated annealing）**，每一步都以一定的概率接受比当前解更差的结果
          - 在每步迭代的过程中，接受“次优解”的概率要随着时间的推移而逐渐降低
        - 方法3：随机梯度下降，在计算梯度时加入了随机因素，陷入局部极小点时，它计算出的梯度仍可能不为0
        - 方法4：**遗传算法（genetic algorithms）**

## 其他神经网络

- **RBF（Radial Basis Function，径向基函数）**网络

  - 单隐层前馈神经网络

  - 使用径向基函数作为隐层神经元激活函数

  - 输出层：对隐层神经元的线性组合

  - 假定输入d维向量x

    $\phi(x)=\sum_{i=1}^q w_i \rho(x,c_i)$

    - q为隐层神经元个数

    - $c_i,w_i$分别为第i个隐层神经元对应的中心和权重

    - $\rho(x,c_i)$为径向基函数（某种沿径向对称的标量函数）

      - 样本x到数据中心$c_i$之间欧式距离的单调函数

      - 常用高斯径向基函数

        $\rho(x,c_i)=e^{-\beta_i \vert\vert x-c_i \vert\vert^2}$

  - 步骤

    - 确定神经元中心$c_i$
      - 方式：随机采样，聚类
    - 利用BP算法等来确定参数$w_i,\beta_i$

- **ART（Adaptive Resonance Theory，自适应谐振理论）网络**

  - **竞争型学习（competitve learning）**
    - 无监督学习策略
    - **胜者通吃（winner-take-all）**原则
      - 网络的输出神经元相互竞争
      - 每个时刻仅有一个竞争获胜的神经元被激活，其它的被抑制
    - ART是其中的重要代表
  - 组成
    - 比较层：接收输入样本，并将其传递给识别层神经元
    - 识别层：每个神经元对应一个模式类，神经元数目可在训练过程中动态增长增加新的模式类
      - 模式类可以认为是某个类比的子类
    - 识别阈值
    - 重置模块
  - 过程
    - 识别层神经元收到比较层的输入信号
    - 神经元之间相互竞争产生获胜神经元
      - 计算输入向量与每个识别层神经元所对应的 模式类的 代表向量之间的距离
      - 距离最小者胜
    - 获胜神经元向其他识别层神经元发送信号，抑制激活
    - 输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值
      - 当前输入样本被归为该代表向量所属类别
      - 网络连接权更新
      - 以后再接收到相似输入样本时该模式类会计算出更大的相似度，从而该获胜神经元更大可能获胜
    - 相似度不大于识别阈值
      - 重置模块会在识别层增设一个新的神经元
      - 代表向量为当前输入向量
  - 识别阈值对性能影响
    - 识别阈值较高：输入样本将会分成比较多的、精细的模式类
    - 识别阈值较低：产生比较少、比较粗略的模式类
  - **可塑性-稳定性窘境（stability-plasticity dilemma）**
    - 可塑性：神经网络要有学习新知识的能力
    - 稳定性：神经网络在学习新知识时要保持对旧知识的记忆
    - ART的优点：可以进行**增量学习（incremental learning）**或**在线学习（online learning）**

- **SOM（Self-Organizing （Feature） Map，自组织（特征）映射）**网络

  - 属于竞争学习型的无监督神经网络

  - 功能

    - 将高维输入数据映射到低维空间（通常为二维）

    - 同时保持输入数据在高维空间的拓扑结构

    - 也就是将高维空间中相似的样本点映射到网络输出层中的邻近神经元，从而保持拓扑结构

      <img src="/assets/MLpics/T75.png" style="zoom:50%;" />

  - 过程

    - 接收到一个训练样本
    - 每个输出层神经元会计算该样本与自身携带的权向量之间的距离
    - **最佳匹配单元（best matching unit）**：距离最近的神经元成为竞争获胜者
    - 最佳匹配单元及其邻近神经元的权向量将被调整，使得这些权向量与当前输入样本的距离缩小
    - 不断迭代直至收敛

- **级联相关（Cascade-Correlation）**网络

  - 属于**结构自适应（亦称 构造性（constructive））**神经网络
    
  - 不同于一般的网络结构事先固定，将网络结构也当作学习目标之一
    
  - 训练

    ​	<img src="/assets/MLpics/T76.png" style="zoom:50%;" />

    - 级联：建立层次连接的层级结构
      - 开始时，网络只有输入层和输出层，处于最小拓扑结构
      - 新的隐层神经元逐渐加入
        - 输入端连接权值是冻结固定的
    - 相关：最大化新神经元的输出与网络误差之间的相关性来训练参数

  - 优劣势

    - 无需设置网络层数，隐层神经元网络，训练速度较快
    - 在数据较小时容易陷入过拟合

- Elman网络

  - 属于**递归神经网络（recurrent neural networks）**
    - 允许网络中出现环形结构
    - 可以让一些神经元的输出反馈回来作为输入信号
    - 因此网络在t时刻的输出状态不仅与t时刻的输入状态和t-1时刻的网络状态有关，能够处理与时间有关的动态变化
  - 与前馈区别：隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起作为隐层神经元在下一时刻的输入
  - 隐层神经元：Sigmoid激活函数
  - 网络训练：BP算法

- Boltzmann机（亦称“平衡态（equilibrium）或平稳分布（stationary distribution））

  - **基于能量的模型（energy-based model）**

    - 为网络状态定义一个能量，其最小化时网络达到理想状态，网络训练就是在最小化这个能量函数

  - 神经元

    - 分为两层
      - 显层：表示数据的输入与输出
      - 隐层：数据的内在表达
    - 类型：布尔型的
      - 只能取0和1两种状态
        - 0：抑制
        - 1：激活

  - 能量定义

    - 令向量$s \in \{ 0,1  \}^n$表示n个神经元的状态

    - $w_{ij}$表示神经元i与j之间的连接权

    - $\theta_i$表示神经元i的阈值

    - 状态s对应的Boltzmann机能量的定义为

      $E(s)=-\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{ij} s_i s_j - \sum_{i=1}^n \theta_i s_i$

    - 推导

      - 能量值越大，当前状态越不稳定（物理学上），能量值最小系统处于稳定态

      - Boltzmann机本质上引入了隐变量的无向图模型，能量为

        $E_{graph}=E_{edges}+E_{nodes}$

      - 分别代表图、边和结点的能量

      - 边能量：两连接结点的值及其权重的乘积$E_{edge_{ij}}=-w_{ij}s_is_j$

      - 结点能量：结点的值及其阈值的乘积$E_{nodes_i}=-\theta_is_i$

      - $E_{edges}=\sum_{i=1}^{n-1}\sum_{j=i+1}^n E_{edge_{ij}}=-\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{ij}s_is_j$

      - $E_{nodes}=\sum_{p=1}^n E_{node_i}=-\sum_{p=1}^n \theta_i s_i$

      - $E_{graph}=-\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{ij}s_is_j-\sum_{p=1}^n \theta_i s_i$

  - Boltzmann分布

    - 若网络中的神经元以任意不依赖于输入值的顺序进行更新，最终网络将达到Boltzmann分布

    - 状态s出现的概率将仅由其能量与所有可能状态向量的能量确定

    - 推导

      - 注意相关知识在14.2节

      - 无向图网络联合概率分布

        - k为无向图中的极大团个数
        - $c_i$为极大团的节点集合
        - $x_{c_i}$为极大团所对应的节点变量
        - $\Phi_i$为势函数
        - Z为规范化因子
        - $P(s)=\frac{1}{Z}\prod_{i=1}^k \Phi_i(s_{c_i})$

      -  Boltzmann机的极大团只有一个（为全连接网络），结点集合为$c=\{ s_1,s_2,...,s_n \}$

        - 联合概率分布为

          $P(s)=\frac{1}{Z} \Phi(s_c)$

      - 势函数$\Phi(s_c)$一般定义为指数型函数，所以其一般形式为

        $\Phi(s_c)=e^{-E(s_c)}$

      - 其中$s_c=s$

        - 状态集合T中某个状态s的概率定义：状态s的联合概率分布与所有可能的状态的联合概率分布的比值

        - 则状态s下的联合概率分布为

          $P(s)=\frac{1}{Z} e^{-E(s)}$

        - $P(s)=\frac{e^{-E(s)}}{\sum_{t \in T} e^{-E(t)}}$

      - $P(s)=\frac{e^{-E(s)}}{\sum_t e^{-E(t)}}$            （公式5.21）

  - 训练过程

    - 将每个样本视为一个状态向量，使其出现的概率尽可能大

  - 分类

    ​		<img src="/assets/MLpics/T77.png" style="zoom:50%;" />

    - 标准的Boltzmann机：是一个全连接图，训练网络的复杂度很高，难以解决现实任务
    - **受限Boltzmann机（Restricted Boltzmann Machine，简称RBM）**：仅保留显层与隐层之间的连接，从而将其结构由完全图转为二部图
      - 使用**对比散度（Contrastive Divergence，简称CD）**算法
      - 假定网络中有d个显层神经元和q个隐层神经元
      - 令v和h分别表示显层与隐层的状态向量，则由于同一层内不存在连接
      - $P(v \vert h) = \prod_{i=1}^d P(v_i \vert h)$
      - $P(h \vert v) = \prod_{j=1}^q P(h_j \vert v)$
      - CD算法对每个训练样本v
        - 先根据上式计算出隐层神经元状态的概率分布
        - 根据这个概率分布采样得到h
        - 类似地根据上上式从h产生$v'$，再从$v'$产生$h'$
        - 连接权的更新公式：$\Delta w=\eta(vh^T-v'h'^T)$（推导见本章附录1）

## 深度学习

- 诞生原因（复杂模型的缺陷和当下为何可用）

  - 计算能力大幅提高，缓解训练低效性
  - 训练数据的大幅增加可降低过拟合风险

- 提高容量

  - 容量越大，能完成更复杂的学习任务
  - 增加隐层神经元的数目
  - 增加隐层的数目（更有效）
    - 增加拥有激活函数的神经元数目
    - 增加激活函数嵌套的层数
    - 多隐层（三个以上隐层）神经网络，难以用经典算法（如标准BP）进行训练，因为会**发散（diverge）**而不能收敛到稳定态

- **无监督逐层训练（unsupervised layer-wise training）**

  - 多隐层网络训练的有效手段

  - 思想

    - 预训练+微调：可视为将大量参数分组，对每组先找到局部较优，然后基于这些结果进行全局寻优

      - 目的
        - 利用模型大量参数所提供的自由度时间
        - 节省训练开销
      - **预训练（pre-training）**
        - 每次训练一层隐结点
        - 训练时将上一层隐结点的输出作为输入
        - 本层隐结点的输出作为下一层隐结点的输入
      - **微调（finetuning）**

    - **权共享（weight sharing）**

      - 让一组神经元使用相同的连接权

      - 用于CNN

        - **特征映射（feature map）**：一个由多个神经元构成的平面

        - **RELU**：将sigmoid激活函数替换为修正线性函数
            $$
            f(x)=\left\{\begin{array}{ll}
            0, & \text { if } x<0 \\
            x, & \text { otherwise }
            \end{array}\right.
            $$

        - **采样层（亦称汇合层）（pooling）**：基于局部相关性原理进行亚采样，减少数据量，保留有用信息

        - 可用BP训练

- 理解

  - **特征学习（feature learing）或表示学习（representation learning）**
  - 对输入信号进行逐层加工，把初始输入转化为和输出目标联系更密切的表示，让最后一层输出映射成为可能
    - 逐渐将初始的低层特征转化为高层特征表示后
    - 用简单模型即可完成复杂的分类等学习任务

- **特征工程（feature engineering）**

  - 用于描述样本的特征
  - 现在一般为人工设计
  - 特征学习通过ML技术产生好特征

## 附录1

- 受限Boltzmann机连接权更新公式推导

- 本推导来自南瓜书

  <img src="/assets/MLpics/T78.png" style="zoom:50%;" />

  <img src="/assets/MLpics/T79.png" style="zoom:50%;" />

  <img src="/assets/MLpics/T80.png" style="zoom:50%;" />

  <img src="/assets/MLpics/T81.png" style="zoom:50%;" />


## 习题

- 5.1 试述将线性函数$f(x)=w^Tx$用作神经元激活函数的缺陷

  - 神经网络中必须要有非线性的激活函数，如果全部用线性函数做激活函数，无论就是线性回归而非神经网络。

- 5.2 试述使用图5.2(b)激活函数的神经元与对率回归的联系

  <img src="/assets/MLpics/T65.png" style="zoom:50%;" />

  - 相同点：两者都是将连续值映射到{0,1}上
  - 不同点：激活函数不一定要使用sigmoid，只要是非线性的可导函数都可以使用。

- 5.3 对于图5.7中的$v_{ih}$，试推导出BP算法的更新公式

  见附录1

- 5.4 试述式（5.6）中学习率的取值对神经网络训练的影响

  - 式5.6 $\Delta w_{hj}=- \eta \frac{\delta E_k}{\delta w_{hj}}$
  - 学习率过大，可能会出现扰动现象，在最小值附近来回波动
  - 学习率过小，会导致下降过慢，迭代次数很多

- 5.5 试编程实现标准BP算法和累积BP算法，在西瓜数据集3.0上分别用这两个算法训练一个单隐层网络，并进行比较

  - ```python
    # 标准BP算法
    # hideNum为隐层神经元个数
    def BP(X,Y,hideNum=5,eta=0.01,epoch=1000000):
    
        # 权值及偏置初始化
        V = np.random.rand(X.shape[1],hideNum)
        V_b = np.random.rand(1,hideNum)
        W = np.random.rand(hideNum,Y.shape[1])
        W_b = np.random.rand(1,Y.shape[1])
    
        trainNum=0
        
        while trainNum<epoch:
            # 标准BP每次处理一个样本
            for k in range(X.shape[0]):
                B_h=sigmoid(X[k,:].dot(V)-V_b) # 输入层-隐层 注意是减去阈值
                Y_=sigmoid(B_h.dot(W)-W_b)     # 隐层-输出层 注意是减去阈值
                loss=sum((Y[k]-Y_)**2)*0.5      # 算均方误差
                
                # 计算梯度并更新参数
                g=Y_*(1-Y_)*(Y[k]-Y_)
                e=B_h*(1-B_h)*g.dot(W.T)
                
                # 参数更新
                W+=eta*B_h.T.dot(g)
                W_b-=eta*g
                V+=eta*X[k].reshape(1,X[k].size).T.dot(e)
                V_b-=eta*e
                trainNum+=1
                
        print("标准BP")
        print("总训练次数：",trainNum)
        print("最终损失：",loss)
    ```

  - ```python
    # 累积BP算法
    def BPAcc(X,Y,hideNum=5,eta=0.01,epoch=1000000):
        
        # 权值及偏置初始化
        V = np.random.rand(X.shape[1],hideNum)
        V_b = np.random.rand(1,hideNum)
        W = np.random.rand(hideNum,Y.shape[1])
        W_b = np.random.rand(1,Y.shape[1])
        
        trainNum=0
        
        while trainNum<epoch:
            # 累积BP直接处理所有样本
            B_h=sigmoid(X.dot(V)-V_b)   # 输入层-隐层 注意是减去阈值
            Y_=sigmoid(B_h.dot(W)-W_b)  # 隐层-输出层 注意是减去阈值
            loss=0.5*sum((Y-Y_)**2)/X.shape[0]     # 算均方误差
            
            # 计算梯度并更新参数
            g=Y_*(1-Y_)*(Y-Y_)
            e=B_h*(1-B_h)*g.dot(W.T)
                
            # 参数更新
            W+=eta*B_h.T.dot(g)
            W_b-=eta*g.sum(axis=0)
            V+=eta*X.T.dot(e)
            V_b-=eta*e.sum(axis=0)
            trainNum+=1
            
        print("累积BP")
        print("总训练次数：",trainNum)
        print("最终损失：",loss)
    ```

  - 结果

    <img src="/assets/MLpics/T82.png" style="zoom:50%;" />

    - 累积BP性能更好

- 5.6 试设计一个BP改进算法，能通过动态调整学习率显著提升收敛速度，编程实现该算法，并选择两个UCI数据集与标准BP算法进行实验比较

  - BP优化算法，其中自适应调节学习率相关领域现在比较火热
  - 推荐blog[深度学习 --- BP算法详解（BP算法的优化）](https://blog.csdn.net/weixin_42398658/article/details/83958133)

- 5.7 根据式（5.18）和（5.19），试构造一个能解决疑惑问题的单层RBF神经网络

  - ```python
    class RBF():
        # 权值及偏置初始化
        # 注意是单层RBF
        def __init__(self):
            self.hideNum=4
            self.epoch=10000
        
            self.w = np.random.rand(self.hideNum,1)
            self.beta = np.random.rand(self.hideNum,1)
            self.c=np.random.rand(self.hideNum,2)   #中心
            
        def forward(self,X):
            self.X=X
            self.dist=np.sum((X-self.c)**2,axis=1,keepdims=True)
            # 高斯径向基
            self.rho=np.exp(-self.beta*self.dist)# 注意径向基为激活函数，相当于BP的sigmoid
            self.y=self.w.T.dot(self.rho)
            # w第一位代表w_b,所以y第一位代表预测值
            return self.y[0, 0]
            
            
        # 梯度下降
        # 通过y回退
        def grad(self,y):
            grad_y=self.y-y
            grad_w=grad_y*self.rho
            grad_rho=grad_y*self.w
            grad_beta=-grad_rho*self.rho*self.dist
            grad_c=grad_rho*self.rho*2*self.beta*(self.X-self.c)
            self.grads = [grad_w, grad_beta, grad_c]
            
        # 参数更新
        def update(self,eta=0.01):
            self.w-=eta*self.grads[0]
            self.beta-=eta*self.grads[1]
            self.c-=eta*self.grads[2]
        
        def loss(self,X,y):
            y_=self.forward(X)
            loss=0.5*(y_-y)**2
            return loss
        
        def train(self,X,y):
            losses=[]
            for e in range(self.epoch):
                loss=0
                for i in range(len(X)):
                    self.forward(X[i])
                    self.grad(y[i])
                    self.update()
                    loss+=self.loss(X[i],y[i])
                    
                losses.append(loss)
            return losses
    ```

  - 结果

    <img src="/assets/MLpics/T83.png" style="zoom:50%;" />

# 支持向量机

- **支持向量机（Support Vector Machine，简称SVM）**

## 间隔与支持向量

- 分类学习：找到一个划分超平面，如何找到最合适的一个？

  - 如下图所示，直观看需要找到正中间最粗的那条超平面

    - 该划分超平面对训练样本局部扰动的“容忍性”最好
    - 容忍性：例如由于训练集的局限性或噪声，训练集外的样本可能比图中的训练样本更接近两个类的分隔界，而中间的超平面受影响最小
      - 也就是其分类结果是最鲁棒的，对未见示例的泛化能力最强

    <img src="/assets/MLpics/T84.png" style="zoom:50%;" />

- 划分超平面线性方程$w^Tx+b=0$

  - 其中$w=(w_ 1;w_ 2;...;w_ d)$为法向量，决定了超平面的方向

  - b为位移，决定了超平面与原点之间的距离

  - 样本空间中任意点x到超平面（w,b）的距离可写为$r=\frac{\vert w^Tx+b \vert}{\vert \vert w \vert \vert}$

  - 假设超平面（w,b）能将训练样本正确分类，即对于$(x_ i,y_ i) \in D,若 y_ i=+1，则w^Tx_ i+b <0$

  - 式（6.3）：

    $$ y=\left\{\begin{array}{cl}w^Tx_ i+b \ge +1, & y_ i=+1 \\ w^Tx_ i+b \le -1 & y_ i=-1  \end{array}\right. $$

  - 如下图所示

    - 距离最近的训练样本使上式等号成立，即为**支持向量（supprot vector）**
    - **间隔（margin）**：两个异类支持向量到超平面的距离为$\gamma=\frac{2}{\vert \vert w \vert \vert}$
      - 间隔与w有关也与b有关，因为b通过约束隐式地影响着w的取值，进而对间隔产生影响

    <img src="/assets/MLpics/T85.png" style="zoom:50%;" />

    - **最大间隔（maximum margin）**：找到满足6.3的w和b使得$\gamma$最大

      - （6.5）

        $max_ {w,b} \frac{2}{\vert \vert w \vert \vert}$

        $s.t. \  y_ i(w^Tx_ i+b) \ge 1  \ \ i=1,2,...m$

      - 最大化$\vert \vert w \vert \vert ^{-1}$，等价于最小化$\vert \vert w \vert \vert ^2$

      - 重写得到SVM的基本型（6.6）

        $max_ {w,b} \frac{\vert \vert w \vert \vert ^2}{2}$

        $s.t. \  y_ i(w^Tx_ i+b) \ge 1  \ \ i=1,2,...m$

## 对偶问题

- 求解6.6得到大间隔划分超平面对应的模型$f(x)=w^Tx+b$

- 方法1：

  - 式子6.6本身是一个**凸二次规划（convex quadratic programming）**，可以直接用优化计算包求解

- 方法2:

  - 使用拉格朗日乘子法得到**对偶问题（dual problem）**

  - 对式6.6的每条约束添加拉格朗日乘子$\alpha_ i \ge 0$，则拉格朗日函数可写为

    （6.8）$L(w,b,\alpha)=\frac{1}{2} \vert \vert w \vert \vert ^2+\sum_ {i=1}^m \alpha_ i(1-y_ i(w^Tx_ i+b))$

  - 其中$\alpha=(\alpha_ 1;\alpha_ 2;...;\alpha_ m)$

  - 令$L(w,b,\alpha)$对w和b的偏导为0可得

    （6.9）$w=\sum_ {i=1}^m \alpha_ iy_ ix_ i$

    （6.10）$0=\sum_ {i=1}^m \alpha_ iy_ i$        （对w求导再对b求导得到）

  - 把6.9代入6.8，可以将w和b消除，考虑6.10的约束，就得到对偶问题（6.11）

    $max_ {\alpha} \sum_ {i=1}^m \alpha_ i-\frac{1}{2}\sum_ {i=1}^m \sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j x_ i^T x_ j$

    $s.t. \ \  \sum_ {i=1}^m \alpha_ i y_ i=0$

    $\alpha_ i \ge 0,i=1,2,...,m$

  - 解出$\alpha$（拉格朗日乘子），$\alpha_ i$对应$(x_ i,y_ i)$

    - 求出w和b（6.12）

      $f(x)=w^Tx+b=\sum_ {i=1}^m \alpha_ i y_ i x_ i^Tx+b$

  - 6.6中有不等式约束，因此上述过程需要满足**KKT（Karush-Kuhn-Tucker）**

    - 证明见附录B.1

    $$ \left\{\begin{array}{cl}\alpha_ i \ge 0; \\ 1-y_ if(x_ i) \le 0; \\ \alpha_ i(y_ i f(x_ i)-1)=0  \end{array}\right. $$

    - 若$\alpha_ i=0$，则该样本不会在6.12中出现，不会对f(x)有任何影响
    - 若$\alpha_ i>0$，则必有$y_ i f(x_ i)=1$，对应样本点位于最大间隔边界上
    - SVM性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关

  - 求解6.11:

    - 二次规划问题（见附录B.2）

    - 但该问题的规模正比于训练样本数，会造成很大开销

    - **SMO（Sequential Minimal Optimization）**算法

      - 先固定$\alpha_ i$之外的所有参数，然后求$\alpha_ i$上的极值

      - 由于存在约束$\sum_ {i=1}^m \alpha_ i y_ i=0$

      - 若固定$\alpha_ i$之外的其他变量，则其可由其他变量导出

      - SMO每次选择$\alpha_ i,\alpha_ j$，并固定其他参数，进行初始化

      - SMO不断进行以下步骤进行收敛

        - 选取一对需要更新的变量$\alpha_ i,\alpha_ j$
        - 固定$\alpha_ i,\alpha_ j$以外的参数，求解6.11获得更新后的$\alpha_ i,\alpha_ j$

      - SMO先选取违背KKT条件程度最大的变量

        - 只需选取的$\alpha_ i,\alpha_ j$中有一个不满足KKT，目标函数就会在迭代后减小
        - KKT条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大

      - 再选取一个使目标函数值见效最快的变量

        - 减幅的复杂度过高，因此SMO采用了启发式
          - 选取的两变量所对应样本之间的间隔最大
          - 这样的两个变量会有很大的差别，与对两个相似的变量进行更新对比，对它们进行更新会带给目标函数值更大的变化

      - SMO高效的原因

        - 在固定其他参数后，仅优化两个参数的过程能做到非常高效

        - 仅考虑$\alpha_ i,\alpha_ j$，6.11中的约束可以重写为

          $\alpha_ iy_ i+\alpha_ jy+j=c，\alpha_ i \ge 0，\alpha_ j \ge 0$

        - 其中$c=-\sum_ {k \not=i,j}\alpha_ ky_ k$是使得$\sum_ {i=1}^m \alpha_ i y_ i=0$成立的常数

        - 用$\alpha_ iy_ i+\alpha_ jy_ j=c$消去6.11中的变量$\alpha_ j$，得到一个关于$\alpha_ i$的单变量二次规划问题，仅有约束$\alpha_ i \ge 0$

        - 这样的二次规划问题具有闭式解，于是不必调用数值优化算法即可高效计算出更新后的$\alpha_ i,\alpha_ j$

      - 确定偏移项b，对任意$(x_ s,y_ s)$都有$y_ sf(x_ s)=1$

        - （6.17）：$y_ s(\sum_ {t \in S} \alpha_ i y_ i x_ i^T x_ s+b)=1$
        - $S=\{i \vert \alpha_ i >0,i=1,2,...,m\}$为所有支持向量的下标集
        - 理论上，可选取任意支持向量并通过求解6.17获得b
        - 现实上，使用更鲁棒的做法
          - 使用所有支持向量求解的平均值
          - $b=\frac{1}{\vert S \vert} \sum_ {s \in S}(y_ s-\sum_ {i \in S} \alpha_ i y_ i x_ i^T x_ s)$

## 核函数

- 现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面

  - 例如异或问题

  <img src="/assets/MLpics/T86.png" style="zoom:50%;" />

- 如何解决

  - 将样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间内线性可分，如上图，将二维空间映射到三维
  - 如果原始空间是有限维（属性数有限），一定存在一个高维特征空间使样本可分

- 公式推导

  - $\phi(x)$表示将x映射后的特征向量

  - 超平面所对应的模型$f(x)=w^T\phi(x)+b$

  - w和b是模型参数，类似式6.6，有

    $min_ {w,b}=\frac{1}{2} \vert \vert w \vert \vert ^2$

    $s.t. \ \ y_ i(w^T\phi(x_ i)+b) \ge 1,\ i=1,2,...,m$

  - 对偶问题是（6.21）

    $max_ {\alpha} \sum_ {i=1}^m \alpha_ i-\frac{1}{2}\sum_ {i=1}^m \sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j \phi(x_ i)^T \phi(x_ j)$

    $s.t. \ \  \sum_ {i=1}^m \alpha_ i y_ i=0$

    $\alpha_ i \ge 0,i=1,2,...,m$

  - 求解6.21

    - $\phi(x_ i)^T\phi(x_ j)$是样本$x_ i,x_ j$映射到特征空间之后的内积

    - 为避免由于特征空间维数过高导致的计算困难，设计函数

      $\kappa(x_ i,x_ j) = \ <\phi(x_ i),\phi(x_ j)> \ =\phi(x_ i)^T\phi(x_ j)$

    - 即$x_ i,x_ j$在特征空间的内积等于它们在原始样本空间中通过函数$k(·,·)$计算的结果

    - 因此6.21可以重写为

      $max_ {\alpha} \sum^m_ {i=1}\alpha_ i-\frac{1}{2}\sum_ {i=1}^m\sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j k(x_ i,x_ j)$

      $s.t. \ \ \sum_ {i=1}^m \alpha_ i y_ i=0$

      $\alpha_ i \ge 0,i=1,2,...,m$

    - 求解得到式6.24

      $\begin{aligned} f(x)&=w^T\phi(x)+b \\ &= \sum_ {i=1}^m \alpha_ i y_ i \phi(x_ i)^T \phi(x)+b \\ &=\sum_ {i=1}^m \alpha_ i y_ i k(x,x_ i)+b\end{aligned}$

    - 这里的函数$\kappa(·,·)$就是**“核函数（kernel function）”**

    - 6.24显示出模型最优解可通过训练样本的核函数展开，这一展式亦称**“支持向量展式”（support vector expansion）**

- **核函数**

  - 当$\chi$为输入空间，$\kappa(·,·)$是定义在$\chi \times \chi$上的对称函数

  - 则$\kappa$是核函数当且仅当对于任意数据$D= \{x_ 1,x_ 2,...,x_ m\}$

  - **“核矩阵”（kernel matrix）K**总是半正定的

    $$\mathbf{K}=\left[\begin{array}{ccccc}\kappa\left(\boldsymbol{x}_ {1}, \boldsymbol{x}_ {1}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {1}, \boldsymbol{x}_ {j}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {1}, \boldsymbol{x}_ {m}\right) \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ \kappa\left(\boldsymbol{x}_ {i}, \boldsymbol{x}_ {1}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {i}, \boldsymbol{x}_ {j}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {i}, \boldsymbol{x}_ {m}\right) \\ \vdots & \ddots & \vdots & \ddots & \vdots \\ \kappa\left(\boldsymbol{x}_ {m}, \boldsymbol{x}_ {1}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {m}, \boldsymbol{x}_ {j}\right) & \cdots & \kappa\left(\boldsymbol{x}_ {m}, \boldsymbol{x}_ {m}\right)\end{array}\right]$$

  - 只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用

  - 对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$

  - 换言之，任何一个核函数都隐式定义了一个**“再生核希尔伯特空间”（Reproducing Kernel Hilbert Space，简称RKHS）**的特征空间

  - 我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要

- 核函数选择

  - SVM的最大变数

  - 常用核函数

    <img src="/assets/MLpics/T87.png" style="zoom:50%;" />

  - 函数组合

    - 若$\kappa_ 1，\kappa_ 2$为核函数，则对于任意正数$\gamma_ 1，\gamma_ 2$，其线性组合$\gamma_ 1 \kappa_ 1+\gamma_ 2 \kappa_ 2$也是核函数
    - 若$\kappa_ 1，\kappa_ 2$为核函数，则核函数的直积$\kappa_ 1 \otimes \kappa_ (x,z)=\kappa_ 1(x,z)\kappa_ 2(x,z)$也是核函数
    - 若$\kappa_ 1$为核函数，则对于任意函数$g(x)$，$\kappa(x,z)=g(x)\kappa_ 1(x,z)g(z)$也是核函数

## 软间隔与正则化

- 现实中很难确定合适的核函数使得训练样本在特征空间中线性可分

- 即使找到也很难判断是否是由于过拟合造成的

- **硬间隔（hard margin）**：要求所有样本均满足约束（6.3），前面提到的SVM都是如此

- **软间隔（soft margin）**：允许SVM在一些样本上出错

  - 式6.28：$y_ i(w^Tx_ i+b) \ge 1$

  <img src="/assets/MLpics/T88.png" style="zoom:50%;" />

- **软间隔支持向量机**

  - 最大化间隔的同时，不满足约束的样本应该尽可能少，因此优化目标为

  - 式（6.29）：$min_ {w,b} \frac{1}{2} \vert \vert w \vert \vert ^2+C\sum_ {i=1}^m l_ {0/1}(y_ i(w^Tx_ i+b)-1)$ 

  - 其中$C >0$是一个常数$l_ {0/1}$是0/1损失函数

    $$ l_ {0/1}(z)=\left\{\begin{array}{cl}1, &if \  z<0; \\ 0, &otherwise  \end{array}\right. $$

  - 当C为无穷大时，6.29迫使所有样本均满足约束（6.28），于是式6.29等价于6.6

  - 当C取有限值时，6.29允许一些样本不满足约束

  - **替代损失（surrogate loss）**

    - 由于$l_ {0/1}$非凸、非连续，数学性质不好，使得式6.29不易直接求解

    - 需要用其他一些函数替代$l_ {0/1}$

    - 常用的替代损失函数

      - hinge损失：$l_ {hinge}(z)=max(0,1-z)$
      - 指数损失（exponential loss）：$l_ {exp}(z)=exp(-z)$
      - 对率损失（logistic loss）：$l_ {log}(z)=log(1+exp(-z))$
        - 对率损失函数通常表示为$l_ {log}(·)$而非$ln(·)$

      <img src="/assets/MLpics/T89.png" style="zoom:50%;" />

    - 若采用hinge损失，则6.29变成

      ​    $min_ {w,b} \frac{1}{2} \vert \vert w \vert \vert ^2+C\sum_ {i=1}^m max(0,1-y_ i(w^Tx_ i+b))$ 

      - $max(0,1-y_ i(w^Tx_ i+b))=\xi_ i$

      - 引入**松弛变量（slack variables）**$\xi_ i \ge 0$，可将上式重写

        - 当$1-y_ i(w^Tx_ i+b)>0$，$1-y_ i(w^Tx_ i+b)=\xi_ i$
        - 当$1-y_ i(w^Tx_ i+b) \le 0$，$\xi_ i=0$

      - 得到软间隔SVM（6.35）

        $min_ {w,b,\xi_ i} \frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m \xi_ i$

        $s.t. \ \ y_ i(w^Tx_ i+b) \ge 1- \xi_ i$

        $\xi_ i \ge 0,i=1,2,...,m$

      - 每一个样本对应松弛变量，表征该样本不满足约束6.28的程度

      - 仍是一个二次规划问题，通过拉格朗日乘子法

        $$\begin{aligned}L(w,b,\alpha,\xi,\mu) =&\frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m \xi_ i \\ &+\sum_ {i=1}^m \alpha_ i(1-\xi_ i-y_ i(w^Tx_ i+b))-\sum_ {i=1}^m \mu_ i \xi_ i \end{aligned}$$

        - $\alpha_ i \ge 0,\mu_ i \ge 0$是拉格朗日乘子

        - 令$L(w,b,\alpha,\xi,\mu)$对$w,b,\xi_ i$的偏导为0可得

          式6.37：$w=\sum_ {i=1}^m \alpha_ i y_ i x_ i$

          式6.38：$0=\sum_ {i=1}^m \alpha_ i y_ i$

          式6.39：$C=\alpha_ i+\mu_ i$

        - 将式6.37-6.39代入式6.36即可得到式6.35的对偶问题（6.40）

          $max_ {\alpha} \sum_ {i=1}^m \alpha_ i -\frac{1}{2}\sum_ {i=1}^m \sum_ {j=1}^m \alpha_ i \alpha_ j y_ i y_ j x_ i^T x_ j$

          $s.t. \ \ \sum_ {i=1}^m \alpha_ i y_ i=0$

          $0 \le \alpha_ i \le C,i=1,2,...,m$

        - 将上式与软间隔下的对偶问题6.11只有对偶变量的约束不同，所以采用同样的算法求解，因此KKT

          $$ \left\{\begin{array}{cl}\alpha_ i \ge 0, & \mu_ i \ge 0, \\ 1-\xi_ i-y_ if(x_ i) \le 0; \\ \alpha_ i(y_ i f(x_ i)-1+\xi_ i)=0 \\ \xi_ i \ge 0, &\mu_ i \xi_ i=0 \end{array}\right. $$

          - 对任意训练样本$(x_ i,y_ i)$,总有$\alpha_ i=0$或$y_ if(x_ i)=1-\xi_ i$
          - 若$\alpha_ i=0$，则该样本不会对$f(x)$有任何影响
          - 若$\alpha_ i>0$则必有$y_ i f(x_ i)=1-\xi_ i$，即该样本是支持向量
          - 由6.39可知，
            - 若$\alpha_ i<C$，则$\mu_ i > 0$，进而$\xi_ i = 0$，则该样本恰在最大间隔边界上
            - 若$\alpha_ i=C$，则$\mu_ i = 0$，此时
              - 若$\xi_ i \le 1$则该样本落在最大间隔内部
              - 若$\xi_ i > 1$则该样本被错误分类

      - 软间隔SVM的最终模型仅与支持向量有关

        - hinge函数有一处“平坦”的0区域，使得解具有稀疏性

    - 使用对率损失函数，几乎就得到了对率回归模型

      - 和对率回归的异同
        - SVM与对率回归的优化目标相近，一般性能相当
        - 对率回归的优势在于其输出具有自然的概率意义；SVM输出不具有概率意义，欲得到概率输出需进行特殊处理
        - 对率回归能直接用于多分类任务；SVM为此则需要进行推广
      - 光滑的单调递减函数，不能导出类似支持向量的概念，依赖于更多的训练样本，预测开销大

      

- **正则化（regularization）**

  - 可理解为对不希望得到的结果进行惩罚，从而使得优化过程趋向于希望目标
    - 从贝叶斯估计角度看，正则化项是提供了模型的先验概率
  - 损失函数的共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项用于表示训练集上的误差，可以写为（6.42）$min_ {f} \Omega(f)+C\sum_ {i=1}^m l(f(x_ i),y_ i)$
    - $\Omega(f)$称为**结构风险（structural risk）**，描述模型f的某些性质
    - $\sum_ {i=1}^m l(f(x_ i),y_ i)$称为**经验风险（empirical risk）**，描述模型与训练数据的契合程度
    - C用于对二者进行折中
    - 某种意义上6.42可以称为正则化问题
      - $\Omega(f)$称为**正则化项**
      - C称为**正则化常数**
        - $L_ p$范数是常用的正则化项
          - $L_ 2（\vert \vert w \vert \vert_ 2）$范数倾向于w的分量取值尽量均衡（非0分量个数尽量稠密
          - $L_ 0$范数$（\vert \vert w \vert \vert_ 0）$和$L_ 1（\vert \vert w \vert \vert_ 1）$$范数则倾向于w的分量尽量稀疏，即非0分量个数尽量少

## 支持向量回归

- 与传统模型不同，**支持向量回归（Support Vector Regression，简称SVR）**假设能容忍$f(x),y$之间最多有$\epsilon$的偏差

  - 如下图所示，以$f(x)$为中心，构建了一个宽度为$2\epsilon$的间隔带，若落入间隔带，则认为预测正确

  <img src="/assets/MLpics/T90.png" style="zoom:50%;" />

- SVR问题形式化（6.43）

  $min_ {w,b} \frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m l_ \epsilon(f(x_ i)-y_ i)$    

  - C为正则化常数

  - $l_ \epsilon$是**$\epsilon$-不敏感损失（$\epsilon$-insensitive loss）**函数

    $$l_ \epsilon(z)=\left\{\begin{array}{cl}0, & if \vert z \vert \le \epsilon; \\ \vert z \vert - \epsilon & otherwise.  \end{array}\right. $$

    <img src="/assets/MLpics/T91.png" style="zoom:50%;" />

  - 引入松弛变量$\xi_ i，\hat{\xi_ i}$，可以将6.43重写为

    $min_ {w,b,\xi_ i,\hat{\xi_ i}} \frac{1}{2} \vert \vert w \vert \vert ^2+C \sum_ {i=1}^m (\xi_ i+\hat{\xi_ i})$

    $s.t. \ \ f(x_ i)-y_ i \le \epsilon+\xi_ i$

    $y_ i-f(x_ i) \le \epsilon+\hat{\xi_ i}$

    $\xi_ i \ge 0,\hat{\xi_ i} \ge 0,i=1,2,...,m$    

  - 引入拉格朗日乘子$\mu_ i \ge 0,\hat{\mu_ i} \ge 9,\alpha_ i \ge 0,\hat{\alpha_ i} \ge 0$，得到式（6.46）

    $L(\boldsymbol{w}, b, \boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}}, \boldsymbol{\mu}, \hat{\boldsymbol{\mu}})$
  
    $=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_ {i=1}^{m}\left(\xi_ {i}+\hat{\xi}_ {i}\right)-\sum_ {i=1}^{m} \mu_ {i} \xi_ {i}-\sum_ {i=1}^{m} \hat{\mu}_ {i} \hat{\xi}_ {i}$
  $+\sum_ {i=1}^{m} \alpha_ {i}\left(f\left(\boldsymbol{x}_ {i}\right)-y_ {i}-\epsilon-\xi_ {i}\right)+\sum_ {i=1}^{m} \hat{\alpha}_ {i}\left(y_ {i}-f\left(\boldsymbol{x}_ {i}\right)-\epsilon-\hat{\xi}_ {i}\right)$
  
- 将式$f(x)=w^Tx+b$代入，对w，b，$\xi_ i，\hat{\xi_ i}$偏导为0可得
  
  $$\begin{aligned} \boldsymbol{w} &=\sum_ {i=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right) \boldsymbol{x}_ {i} \\ 0 &=\sum_ {i=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right) \\ C &=\alpha_ {i}+\mu_ {i} \\ C &=\hat{\alpha}_ {i}+\hat{\mu}_ {i} \end{aligned}$$
  
- 上面几式代入6.46，得到SVR的对偶问题（6.51）
  
  $$\begin{aligned} \max _ {\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}} & \sum_ {i=1}^{m} y_ {i}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right)-\epsilon\left(\hat{\alpha}_ {i}+\alpha_ {i}\right) \\ &-\frac{1}{2} \sum_ {i=1}^{m} \sum_ {j=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right)\left(\hat{\alpha}_ {j}-\alpha_ {j}\right) \boldsymbol{x}_ {i}^{\mathrm{T}} \boldsymbol{x}_ {j} \\ \text { s.t. } & \sum_ {i=1}^{m}\left(\hat{\alpha}_ {i}-\alpha_ {i}\right)=0 \\ & 0 \leqslant \alpha_ {i}, \hat{\alpha}_ {i} \leqslant C \end{aligned}$$
  
- 上述过程需要满足KKT条件
  
    $$\left\{\begin{array}{l}
    \alpha_ {i}\left(f\left(\boldsymbol{x}_ {i}\right)-y_ {i}-\epsilon-\xi_ {i}\right)=0 \\
    \hat{\alpha}_ {i}\left(y_ {i}-f\left(\boldsymbol{x}_ {i}\right)-\epsilon-\hat{\xi}_ {i}\right)=0 \\
    \alpha_ {i} \hat{\alpha}_ {i}=0, \xi_ {i} \hat{\xi}_ {i}=0 \\
    \left(C-\alpha_ {i}\right) \xi_ {i}=0,\\ \left(C-\hat{\alpha}_ {i}\right) \hat{\xi}_ {i}=0
  \end{array}\right.$$
  
    - 当且仅当$f(\boldsymbol{x}_ {i})-y_ {i}-\epsilon-\xi_ {i}=0$（不落入间隔带·中）时$\alpha_ i$和$\hat{\alpha_ i}$能取非0值
  - $f(\boldsymbol{x}_ {i})-y_ {i}-\epsilon-\xi_ {i}=0$和$f(\boldsymbol{x}_ {i})-y_ {i}-\epsilon-\hat{\xi_ {i}=0}$不能同时成立，$\alpha_ i,\hat{\alpha_ i}$至少有一个为0
  
- 将$w =\sum_ {i=1}^{m}(\hat{\alpha}_ {i}-\alpha_ {i}) x_ {i}$代入$f(x)=w^Tx+b$，则SVR的解形如
  
  $f(x)=\sum_ {i=1}^m(\hat{\alpha_ i}-\alpha_ i)x_ i^Tx+b$
  
  - 能使上式中的$(\hat{\alpha_ i}-\alpha_ i) \not=0$的样本即为SVR的支持向量，必落在间隔带之外
  
  - 间隔带中的样本均满足$\alpha_ i$和$\hat{\alpha_ i}$均为0
  
    - 其解仍然具有稀疏性
  
  - 由KKT条件，若$0 < \alpha_ i <C$必有$\xi_ i=0$，进而
  
    $b=y_ i+\epsilon-\sum_ {i=1}^m(\hat{\alpha_ i}-\alpha_ i)x_ i^Tx$
  
      - 在求解6.51得到$\alpha_ i$后，理论上可以任意选取满足$0 < \alpha_ i <C$的样本求得b
    - 只见中采用更鲁棒的方法，选取多个满足条件的样本求b后求平均值
  
- 若考虑特征映射形式（6.19），则
  
  $w =\sum_ {i=1}^{m}(\hat{\alpha}_ {i}-\alpha_ {i}) \phi(x_ {i})$
  
  - SVR可以表示为
  
    $f(x)=\sum_ {i=1}^{m}(\hat{\alpha}_ {i}-\alpha_ {i}) \kappa(x,x_ i)+b$
  
      $\kappa(x,x_ i)=\phi(x_ i)^T\phi(x_ j)$为核函数

## 核方法

- **表示定理（representer theorem）**

  - 令$\Bbb{H}$为核函数$\kappa$对应的再生核希尔伯特空间，$\vert \vert h \vert \vert_ {\Bbb{H}}$表示$\Bbb{H}$空间中关于h的范数，对于任意单调递增函数$ \Omega : [0, \infty ] \mapsto \Bbb{R}$和任意非负损失函数$l:\Bbb{R}^m \mapsto  [0, \infty ]$，优化问题（式6.57）

    $\min _ {h \in \mathbb{H}} F(h)=\Omega\left(\|h\|_ {\mathbb{H}}\right)+\ell\left(h\left(\boldsymbol{x}_ {1}\right), h\left(\boldsymbol{x}_ {2}\right), \ldots, h\left(\boldsymbol{x}_ {m}\right)\right)$

    的解可总写为

    $h^{*}(\boldsymbol{x})=\sum_ {i=1}^{m} \alpha_ {i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_ {i}\right)$

  - 表示定理对$\Omega$要求单调递增，不需要是凸函数

    - 对于一般的损失函数和正则化项，优化问题6.57的最优解$h^*(x)$都可以表示为核函数$\kappa(x,x_ i)$的线性组合

- **核方法（kernel methods）**

  - 基于核函数的学习方法

- **线性判别分析（Kernelized Linear Discriminant Analysis，简称KLDA）**

  - 通过**核化**（引入核函数）来为线性学习器拓展为非线性

  - 过程

    - 假设通过$\phi:\chi \mapsto \Bbb{F}$将样本映射到一个特征空间$\Bbb{F}$，执行线性判别分析，得到

      $h(x)=w^T \phi(x)$

    - KLDA的学习目标

      $max_ w J(w)=\frac{w^T S_ b^ \phi w}{w^T S_ w^ \phi w}$

      - $S_ b ^\phi,S_ w^\phi$分别为训练样本在特征空间中的类间散度矩阵和类内散度矩阵

    - $X_ i$表示第$i \in \{0,1\}$在类样本的集合，其样本数为$m_ i$，总样本数为$m=m_ 0+m_ 1$，第i类样本在特征空间的均值为

      $\mu_ i ^ \phi=\frac{1}{m_ i}\sum_ {x \in X_ i} \phi(x)$

    - 两个散度矩阵分别为

      $\mathbf{S}_ {b}^{\phi} =\left(\boldsymbol{\mu}_ {1}^{\phi}-\boldsymbol{\mu}_ {0}^{\phi}\right)\left(\boldsymbol{\mu}_ {1}^{\phi}-\boldsymbol{\mu}_ {0}^{\phi}\right)^{\mathrm{T}}  $

      $\mathbf{S}_ {w}^{\phi} =\sum_ {i=0}^{1} \sum_ {\boldsymbol{x} \in X_ {i}}\left(\phi(\boldsymbol{x})-\boldsymbol{\mu}_ {i}^{\phi}\right)\left(\phi(\boldsymbol{x})-\boldsymbol{\mu}_ {i}^{\phi}\right)^{\mathrm{T}} $

    - 一般难以知道映射的具体形式，使用核函数来隐式表达映射和特征空间

    - $J(w)$作为6.57的损失函数可得到式（6.64）

      $h(\boldsymbol{x})=\sum_ {i=1}^{m} \alpha_ {i} \kappa\left(\boldsymbol{x}, \boldsymbol{x}_ {i}\right)$

      $w=\sum_ {i=1}^m \alpha_ i \phi(x_ i)$

    - 令$K \in \Bbb{R}^{m \times m}$为核函数对应的核矩阵，$(K)_ {ij}=\kappa(x_ i,x_ j)$

    - $1_ i \in \{1,0\}^{m \times 1}$为第i类样本的指示向量

      - $1_ i$的第j个分量为1当且仅当$x_ j \in X_ i$，否则为0

        $\hat{\mu_ 0}=\frac{1}{m_ 0}K1_ 0$

        $\hat{\mu_ 1}=\frac{1}{m_ 1}K1_ 1$

        $M=(\hat{\mu_ 0}-\hat{\mu_ 1})(\hat{\mu_ 0}-\hat{\mu_ 1})^T$

        $N=KK^T-\sum_ {i=0}^1 m_ i\hat{\mu_ i}\hat{\mu_ i}^T$

      - 于是KLDA的学习目标等价为

        $max_ \alpha J(\alpha)=\frac{\alpha^T M \alpha}{\alpha^T N \alpha}$

      - 求得$\alpha$之后可以由6.64得到投影函数$h(x)$

## 习题

- 6.1：试证明样本空间中任意点x到超平面（w,b）的距离为$r=\frac{\vert w^Tx+b \vert}{\vert \vert w \vert \vert}$

  <img src="/assets/MLpics/T92.png" style="zoom:50%;" />

  - 如上图所示，$x=x_0+r$

    $r=r_0 \frac{w}{\vert \vert w \vert \vert}$

    $x=x_0+r_0 \frac{w}{\vert \vert w \vert \vert}$

    $w^Tx_0+b=0，x_0=x-r_0 \frac{w}{\vert \vert w \vert \vert}$

    $w^Tx-w^Tr_0 \frac{w}{\vert \vert w \vert \vert}+b=0$

    $r_0=\frac{\vert w^Tx+b \vert}{\vert \vert w \vert \vert}$

- 6.2：试使用LIBSVM，在西瓜数据集3.0上分别用线性核和高斯核训练一个SVM，并比较其支持向量的差别

  ```python
  def SVM():
      X,y=ReadExcel('./alpha_data.xlsx')
      X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)
      y_train=y_train.T.tolist()[0]
      y_test=y_test.T.tolist()[0]
      prob=svm_problem(y_train,X_train)
      # t=2高斯核，t=0线性核
      param=svm_parameter('-t 2 -c 35')
      model=svm_train(prob,param)
      p_label,p_acc,p_val=svm_predict(y_test,X_test,model)
      print(p_acc)
      return p_label
  ```

  - 高斯核和线性核训练出来的支持向量一致，同时线性核以及高斯核训练的SVM准确率最高均在0.67。

- 6.3 选择两个UCI数据集，分别用线性核核高斯核训练一个SVM，并与BP神经网络和C4.5决策树进行实验比较

  ```python
  # SVC
  #线性核，核函数系数，软间隔分类器
  svc=SVC(kernel='linear',gamma="auto",C=1)
  svc.fit(X_train,y_train)
  print("The accuracy of svc on training set under linear",format(svc.score(X_train,y_train)))
  print("The accuracy of svc on test set under linear",format(svc.score(X_test,y_test)))
  
  #高斯核
  svc=SVC(kernel='rbf',gamma="auto",C=1)
  svc.fit(X_train,y_train)
  print("The accuracy of svc on training set under rbf",format(svc.score(X_train,y_train)))
  print("The accuracy of svc on test set under rbf",format(svc.score(X_test,y_test)))
  
  # BP神经网络
  mlp=MLPClassifier(random_state=0,max_iter=1000,alpha=1).fit(X_train,y_train)
  print("The accuracy of BP neural network on training set",format(mlp.score(X_train,y_train)))
  print("The accuracy of BP neural network on test set",format(mlp.score(X_test,y_test)))
  
  # C4.5决策树
  tree=DecisionTreeClassifier(random_state=0,max_depth=4).fit(X_train,y_train)
  print("The accuracy of C4.5 tree on training set",format(tree.score(X_train,y_train)))
  print("The accuracy of C4.5 tree on test set",format(tree.score(X_test,y_test)))
  ```

  结果

  ```python
  The accuracy of svc on training set under linear 0.9910714285714286
  The accuracy of svc on test set under linear 1.0
  The accuracy of svc on training set under rbf 0.9642857142857143
  The accuracy of svc on test set under rbf 1.0
  
  The accuracy of BP neural network on training set 0.9732142857142857
  The accuracy of BP neural network on test set 1.0
  
  The accuracy of C4.5 tree on training set 1.0
  The accuracy of C4.5 tree on test set 0.9736842105263158
  ```

  - 对比，在训练集上准确率相差不大，而在测试集上决策树表现相对不算优异

- 6.4：试讨论线性判别分析与线性核支持向量机在何种条件下等价

  - 线性判别分析能够解决 n 分类问题
  -  而 SVM只能解决二分类问题，如果要解决 n 分类问题要通过 OvR来迂回解决.
  - 线性判别分析能将数据以同类样例间低方差和不同样例中心之间大间隔来投射到一条直线上, 但是如果样本线性不可分, 那么线性判别分析就不能有效进行, 支持向量机也是
  - 综上, 等价的条件是
    - 数据有且仅有 2 种, 也就是说问题是二分类问题
    - 数据是线性可分的

- 6.5：试述高斯核SVM与RBF神经网络之间的联系

  - 若将隐层神经元数设置为训练样本数，且每个训练样本对应一个神经元中心，则以高斯径向基函数为激活函数的RBF网络恰与高斯核SVM的预测函数相同。
  - RBF的径向基函数和高斯核SVM均采用高斯核
  - 神经网络是最小化累计误差，将参数 w 作为惩罚项；而SVM相反，主要是最小化参数，将误差作为惩罚项。

- 6.6：试析SVM对噪声敏感的原因

  - SVM的基本形态是一个硬间隔分类器，它要求所有样本都满足硬间隔约束（函数间隔要大于1）
  - 若数据集中存在噪声点并且满足线性可分，那么SVM为了将噪声点划分为正确，远离该点的类靠近，使得划分超平面的几何间距变小，泛化性能差
  - 若数据集由于噪声点无法满足线性可分，就需要使用核方法，会得到更加复杂的模型，泛化能力差（过拟合）
  - 因此需要软间隔SVM来解决这些问题

- 6.7：试给出式（6.52）的完整KKT条件

  <img src="/assets/MLpics/T93.png" style="zoom:50%;" />

- 6.8：以西瓜数据集3.0的“密度”为输入，“含糖率”为输出，试使用LIBSVM训练一个SVR\

  ```python
  def SVR():
      X,y=ReadExcel('./alpha_data.xlsx')
      X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=0)
      y_train=y_train.tolist()
      # t=2高斯核，t=0线性核
      # s=3 epsilon-SVR
      model=svm_train(y_train,X_train,'-t 0 -s 3')
      p_label,p_acc,p_val=svm_predict(y_test,X_test,model)
      print(p_acc)
  ```

  结果

  ```python
  Mean squared error = 0.00833156 (regression)
  Squared correlation coefficient = 0.00832679 (regression)
  (0.0, 0.008331563045225086, 0.008326794756839036)
  ```

  

- 6.9：试使用核技巧推广对率回归，产生“核对率回归”

  <img src="/assets/MLpics/T94.png" style="zoom:50%;" />

  

  
